<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>技忆</title>
  <subtitle>Phoebe&#39;s little progress</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://www.phoebepan.cn/"/>
  <updated>2017-07-16T04:40:44.000Z</updated>
  <id>http://www.phoebepan.cn/</id>
  
  <author>
    <name>Phoebe</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>python中time模块</title>
    <link href="http://www.phoebepan.cn/2017/07/16/time/"/>
    <id>http://www.phoebepan.cn/2017/07/16/time/</id>
    <published>2017-07-16T07:30:16.000Z</published>
    <updated>2017-07-16T04:40:44.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote class="blockquote-center"><h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p>在日常数据处理中，常常需要与时间打交道，python中与时间处理有关的模块有：<code>time</code>，<code>datetime</code>，<code>calendar</code>。本文主要介绍<strong>time</strong>模块。<br><img src="/images/time_convert.png" alt="time"></p>
</blockquote>
<a id="more"></a>
<p>Python中，表示时间的方式有：</p>
<ul>
<li>时间戳：通常来说，表示的是从<strong>1970年1月1日00:00:00</strong>开始按秒计算的偏移量；</li>
<li>格式化的时间字符串；</li>
<li>元组(struct_time)。</li>
</ul>
<p>time模块常用的几个函数：</p>
<h3 id="time-localtime"><a href="#time-localtime" class="headerlink" title="time.localtime()"></a>time.localtime()</h3><p>将一个时间戳转换成当前时区的struct_time。</p>
<h3 id="time-time"><a href="#time-time" class="headerlink" title="time.time()"></a>time.time()</h3><p>返回当前时间的时间戳。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt;time.time()</div><div class="line"><span class="number">1500176454.689554</span></div></pre></td></tr></table></figure></p>
<h3 id="time-mktime"><a href="#time-mktime" class="headerlink" title="time.mktime()"></a>time.mktime()</h3><p>将一个struct_time转化为时间戳<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt;time.mktime(time.localtime())</div><div class="line"><span class="number">1500176622.0</span></div></pre></td></tr></table></figure></p>
<h3 id="time-sleep"><a href="#time-sleep" class="headerlink" title="time.sleep()"></a>time.sleep()</h3><p>线程推迟运行，单位为秒</p>
<h3 id="time-strftime"><a href="#time-strftime" class="headerlink" title="time.strftime()"></a>time.strftime()</h3><p>把一个代表时间的元组转化为格式化的时间字符串。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt;time.strftime(<span class="string">'%Y-%m-%d %X'</span>,time.localtime())</div><div class="line"><span class="string">'2017-07-16 11:58:07'</span></div></pre></td></tr></table></figure></p>
<h3 id="time-strptime"><a href="#time-strptime" class="headerlink" title="time.strptime()"></a>time.strptime()</h3><p>格式化时间字符串转化成struct_time。与strftime()操作互逆。<br><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt;time.strptime('2017-07-16 11:58:07','%Y-%m-%d %X'')</div><div class="line">time.struct_time(tm_year=2017, tm_mon=7, tm_mday=16, tm_hour=11, tm_min=58, tm_sec=7, tm_wday=6, tm_yday=197, tm_isdst=-1)</div></pre></td></tr></table></figure></p>
<p>了解更多，请参考<a href="https://docs.python.org/3/library/time.html" target="_blank" rel="external">time模块的官方文档</a>。</p>
]]></content>
    
    <summary type="html">
    
      &lt;blockquote class=&quot;blockquote-center&quot;&gt;&lt;h3 id=&quot;简介&quot;&gt;&lt;a href=&quot;#简介&quot; class=&quot;headerlink&quot; title=&quot;简介&quot;&gt;&lt;/a&gt;简介&lt;/h3&gt;&lt;p&gt;在日常数据处理中，常常需要与时间打交道，python中与时间处理有关的模块有：&lt;code&gt;time&lt;/code&gt;，&lt;code&gt;datetime&lt;/code&gt;，&lt;code&gt;calendar&lt;/code&gt;。本文主要介绍&lt;strong&gt;time&lt;/strong&gt;模块。&lt;br&gt;&lt;img src=&quot;/images/time_convert.png&quot; alt=&quot;time&quot;&gt;&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="Python" scheme="http://www.phoebepan.cn/categories/Python/"/>
    
    
      <category term="Python" scheme="http://www.phoebepan.cn/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>调超参神器——GridSearchCV</title>
    <link href="http://www.phoebepan.cn/2017/06/16/GridSearchCV/"/>
    <id>http://www.phoebepan.cn/2017/06/16/GridSearchCV/</id>
    <published>2017-06-16T07:30:16.000Z</published>
    <updated>2017-07-19T02:41:47.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote class="blockquote-center"><h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p>所谓超参，就是机器学习算法中，不能通过自身学习设定的参数，如SVM的惩罚因子C，核函数kernel，gamma参数等，参数间的组合很是繁琐，人工调节这些超参数时间成本太高，易出错。本文主要介绍sklearn模块的调参神器<code>GridSearchCV</code>模块，它能够在指定范围内自动搜索具有不同超参数的不同模型组合，寻找最佳参数，大大提高调参效率。</p>
</blockquote>
<a id="more"></a>
<h3 id="缘起"><a href="#缘起" class="headerlink" title="缘起"></a>缘起</h3><p>这两天，闲来参加下Ctrip的一个数据竞赛，model选择的是XGboost，好用是自然，但是参数有很多，最迫切需要一个自动调节参数工具，于是接触到GridSearchCV模块。</p>
<h3 id="官方手册"><a href="#官方手册" class="headerlink" title="官方手册"></a>官方手册</h3><p><a href="http://scikit-learn.org/stable/modules/generated/sklearn.grid_search.GridSearchCV.html#sklearn.grid_search.GridSearchCV" target="_blank" rel="external">手册链接</a></p>
<h3 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h3><p>自己训练的代码如下(XGboost+5-fold Cross Validation)，清晰易懂，无须解释。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> xgboost <span class="keyword">as</span> xgb</div><div class="line"><span class="keyword">from</span> sklearn.grid_search <span class="keyword">import</span> GridSearchCV</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">xgbmodel_train</span><span class="params">(train)</span>:</span></div><div class="line">    xgb_model = xgb.XGBClassifier()</div><div class="line">    train_feature, train_label = train.drop(<span class="string">'orderlabel'</span>, axis=<span class="number">1</span>), train[<span class="string">'orderlabel'</span>]</div><div class="line"></div><div class="line">    parameters = &#123;<span class="string">'nthread'</span>: [<span class="number">4</span>],</div><div class="line">                  <span class="string">'objective'</span>: [<span class="string">'binary:logistic'</span>],</div><div class="line">                  <span class="string">'learning_rate'</span>: [<span class="number">0.05</span>,<span class="number">0.06</span>,<span class="number">0.1</span>],</div><div class="line">                  <span class="string">'max_depth'</span>: [<span class="number">5</span>, <span class="number">6</span>],</div><div class="line">                  <span class="string">'min_child_weight'</span>: [<span class="number">1</span>, <span class="number">3</span>],</div><div class="line">                  <span class="string">'silent'</span>: [<span class="number">1</span>],</div><div class="line">                  <span class="string">'gamma'</span>: [<span class="number">0</span>, <span class="number">0.1</span>],</div><div class="line">                  <span class="string">'subsample'</span>: [<span class="number">0.6</span>, <span class="number">0.7</span>, <span class="number">0.8</span>],</div><div class="line">                  <span class="string">'colsample_bytree'</span>: [<span class="number">0.7</span>, <span class="number">0.5</span>, <span class="number">0.6</span>],</div><div class="line">                  <span class="string">'n_estimators'</span>: [<span class="number">5</span>],</div><div class="line">                  <span class="string">'missing'</span>: [<span class="number">-999</span>],</div><div class="line">                  <span class="string">'seed'</span>: [<span class="number">12455</span>]&#125;</div><div class="line"></div><div class="line">    clf = GridSearchCV(xgb_model, parameters, n_jobs=<span class="number">1</span>,</div><div class="line">                       cv=StratifiedKFold(train[<span class="string">'orderlabel'</span>], n_folds=<span class="number">5</span>, shuffle=<span class="keyword">True</span>),</div><div class="line">                       scoring=<span class="string">'roc_auc'</span>,</div><div class="line">                       verbose=<span class="number">2</span>, refit=<span class="keyword">True</span>)</div><div class="line"></div><div class="line"></div><div class="line">    clf.fit(train_feature, train_label)</div><div class="line">   </div><div class="line">    best_parameters, score, _ = max(clf.grid_scores_, key=<span class="keyword">lambda</span> x: x[<span class="number">1</span>])</div><div class="line">    print(<span class="string">'AUC score:'</span>, score)</div><div class="line">    <span class="keyword">for</span> param_name <span class="keyword">in</span> sorted(best_parameters.keys()):</div><div class="line">        print(<span class="string">'%s: %r'</span> % (param_name, best_parameters[param_name]))</div></pre></td></tr></table></figure></p>
<h3 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h3><p><img src="/images/process.png" alt="process"><br><img src="/images/best_score.png" alt="best para"></p>
]]></content>
    
    <summary type="html">
    
      &lt;blockquote class=&quot;blockquote-center&quot;&gt;&lt;h3 id=&quot;简介&quot;&gt;&lt;a href=&quot;#简介&quot; class=&quot;headerlink&quot; title=&quot;简介&quot;&gt;&lt;/a&gt;简介&lt;/h3&gt;&lt;p&gt;所谓超参，就是机器学习算法中，不能通过自身学习设定的参数，如SVM的惩罚因子C，核函数kernel，gamma参数等，参数间的组合很是繁琐，人工调节这些超参数时间成本太高，易出错。本文主要介绍sklearn模块的调参神器&lt;code&gt;GridSearchCV&lt;/code&gt;模块，它能够在指定范围内自动搜索具有不同超参数的不同模型组合，寻找最佳参数，大大提高调参效率。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="http://www.phoebepan.cn/categories/Machine-Learning/"/>
    
    
      <category term="ML" scheme="http://www.phoebepan.cn/tags/ML/"/>
    
      <category term="Tune" scheme="http://www.phoebepan.cn/tags/Tune/"/>
    
  </entry>
  
  <entry>
    <title>XGBoost调参指南</title>
    <link href="http://www.phoebepan.cn/2017/06/14/xgb_paras/"/>
    <id>http://www.phoebepan.cn/2017/06/14/xgb_paras/</id>
    <published>2017-06-14T07:30:16.000Z</published>
    <updated>2017-07-21T05:17:53.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote class="blockquote-center"><h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p>在预测分析建模时，如果结果不理想，那么不妨试试XGBoost，可以说，XGBoost已经成为许多数据科学家的秘密武器，，本文翻译自<a href="https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/" target="_blank" rel="external">Complete Guide to Parameter Tuning in XGBoost (with codes in Python)</a>，它详细介绍了XGBoost中参数的含义以及通过实例说明调参的技艺。在此，记下自己的学习笔记。</p>
</blockquote>
<a id="more"></a>
<h3 id="XGBoost"><a href="#XGBoost" class="headerlink" title="XGBoost"></a>XGBoost</h3><p>XGBoost是Gradient Boosting算法的一个优化版本，由于XGBoost算法的内部复杂性，涉及很多超参数，相比于建立一个XGBoost model，提高该model的性能需要花费很大的精力，但这又是必须做的。</p>
<h4 id="优势"><a href="#优势" class="headerlink" title="优势"></a>优势</h4><ul>
<li><strong>正则化</strong>，在GBM基础之上，增加了正则化，有效降低过拟合风险；</li>
<li><strong>并行处理</strong>，Boosting算法是顺序处理的，比起GBM，它快的惊人，另外，XGBoost也支持Hadoop实现；</li>
<li><strong>高度灵活性</strong>，允许自定义优化目标和评价标准；</li>
<li><strong>缺失值处理</strong>，内置处理缺失值的规则，提供一个特殊值作为参数传进去；</li>
<li><strong>剪枝</strong>，GBM实则是一贪心算法，遇到负损失，就停止分裂，XGBoost根据指定最大深度(max_depth)，回过头来剪枝。如果某节点之后没有正值，它会去除这一分裂；</li>
<li><strong>内置交叉验证</strong>，允许在每一轮boosting迭代中使用交叉验证，方便获得最优迭代次数；</li>
<li><strong>上一轮结果基础上继续训练</strong></li>
</ul>
<h3 id="参数"><a href="#参数" class="headerlink" title="参数"></a>参数</h3><p>XGBoost的作者把所有参数分成3类：通用参数、Booster参数、学习目标参数。</p>
<h4 id="通用参数"><a href="#通用参数" class="headerlink" title="通用参数"></a>通用参数</h4><p>控制XGBoost的宏观功能。有：</p>
<ul>
<li><code>booster</code>，选择每次迭代的模型，默认gbtree；</li>
<li><code>silent</code>，默认为0，设为1，则不会输出任何信息；</li>
<li><code>nthread</code>，最大可能的线程数，用来进行多线程控制。</li>
</ul>
<h4 id="Booster参数"><a href="#Booster参数" class="headerlink" title="Booster参数"></a>Booster参数</h4><p>这里只介绍tree booster参数，有：</p>
<ul>
<li><code>eta</code>，学习率，每一步减少权重，提高模型鲁棒性。通常设置在0.01-0.2范围内；</li>
<li><code>min_child_weight</code>，最小叶子节点样本权重和，避免过拟合，值过高会导致欠拟合，可以通过CV调节该参数；</li>
<li><code>max_depth</code>，树最大深度，避免过拟合，值越大，模型可学到更具体更局部样本，CV调节该参数，通常设在3-10区间内；</li>
<li><code>max_leaf_nodes</code>，树上叶子节点最大数量；</li>
<li><code>gamma</code>，默认为0，节点分裂时，只有分裂后损失函数值下降了，才会分裂该节点，该参数指定节点分裂损失函数最小下降值，值越大，越保守；</li>
<li><code>subsample</code>，控制每棵树随机采样比例，值越小越保守，避免过拟合，值过小，可能欠拟合，通常取值范围，0.5-1；</li>
<li><code>colsample_bytree</code>，默认1，对列数采样比例；</li>
<li><code>lambda</code>，默认1，L2正则化的权重，控制正则化部分，减少过拟合；</li>
<li><code>scale_pos_weight</code>，默认1，在类别十分不均衡时，该参数设置成正值，可以是model更快收敛。</li>
</ul>
<h4 id="学习目标参数"><a href="#学习目标参数" class="headerlink" title="学习目标参数"></a>学习目标参数</h4><p>控制理想的优化目标，和每一步结果的度量，有：</p>
<ul>
<li><code>objective</code>，最小化的损失函数，常用值有binary:logistic（二分类的逻辑回归） 、multi:softmax （softmax多分类器，返回预测的类别）、multi:softprob（softmax多分类器，返回属于各个类别的概率）</li>
<li><code>eval_metric</code>，对于回归默认rmse，分类默认error，常用值，rmse，mae，logloss，error，merror，mlogloss，auc</li>
<li><code>seed</code>，随机数种子，复现随机数据结果。</li>
</ul>
<p>最后，还有两个重要参数<code>num_boosting_rounds</code>，<code>early_stopping_rounds</code>，可以控制迭代次数。<br>更多参数参考：<br><a href="http://xgboost.readthedocs.io/en/latest/model.html" target="_blank" rel="external">XGBoost Guide – Introduction to Boosted Trees</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;blockquote class=&quot;blockquote-center&quot;&gt;&lt;h3 id=&quot;简介&quot;&gt;&lt;a href=&quot;#简介&quot; class=&quot;headerlink&quot; title=&quot;简介&quot;&gt;&lt;/a&gt;简介&lt;/h3&gt;&lt;p&gt;在预测分析建模时，如果结果不理想，那么不妨试试XGBoost，可以说，XGBoost已经成为许多数据科学家的秘密武器，，本文翻译自&lt;a href=&quot;https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/&quot;&gt;Complete Guide to Parameter Tuning in XGBoost (with codes in Python)&lt;/a&gt;，它详细介绍了XGBoost中参数的含义以及通过实例说明调参的技艺。在此，记下自己的学习笔记。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="http://www.phoebepan.cn/categories/Machine-Learning/"/>
    
    
      <category term="ML" scheme="http://www.phoebepan.cn/tags/ML/"/>
    
      <category term="Tune" scheme="http://www.phoebepan.cn/tags/Tune/"/>
    
      <category term="Python" scheme="http://www.phoebepan.cn/tags/Python/"/>
    
      <category term="XGBoost" scheme="http://www.phoebepan.cn/tags/XGBoost/"/>
    
  </entry>
  
  <entry>
    <title>数据可视化——Seaborn</title>
    <link href="http://www.phoebepan.cn/2017/06/06/learn_seaborn/"/>
    <id>http://www.phoebepan.cn/2017/06/06/learn_seaborn/</id>
    <published>2017-06-06T07:30:16.000Z</published>
    <updated>2017-06-28T15:14:42.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote class="blockquote-center"><h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p>EDA过程中，想要更了解你的数据，选择一个合适的可视化工具，可以说会让你的工作事半功倍。<br>本文主要介绍一个以matplotlib作为底层，更易上手的作图库<code>seaborn</code>。</p>
</blockquote>
<a id="more"></a>
<h3 id="Seaborn"><a href="#Seaborn" class="headerlink" title="Seaborn"></a>Seaborn</h3><p>基于matplotlib的可视化库，旨在使默认的数据可视化更加悦目，简化复杂图表创建，可以与pandas很好的集成。</p>
<h3 id="简易用法"><a href="#简易用法" class="headerlink" title="简易用法"></a>简易用法</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</div><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns <span class="comment">#一旦导入了seaborn，matplotlib的默认作图风格就会被覆盖成seaborn的格式</span></div><div class="line">%matplotlib inline </div><div class="line"><span class="comment">#在jupyter notebook里作图，需要用到这个命令</span></div></pre></td></tr></table></figure>
<h4 id="读取原始数据（这是一份红酒成分与口感评分数据）"><a href="#读取原始数据（这是一份红酒成分与口感评分数据）" class="headerlink" title="读取原始数据（这是一份红酒成分与口感评分数据）"></a>读取原始数据（这是一份红酒成分与口感评分数据）</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">winedata=pd.read_csv(<span class="string">'winequality-red.csv'</span>)</div><div class="line">winedata.head()</div></pre></td></tr></table></figure>
<p><img src="/images/winedata.png" alt="png"></p>
<h4 id="直方图——seaborn-distplot"><a href="#直方图——seaborn-distplot" class="headerlink" title="直方图——seaborn.distplot()"></a><strong>直方图</strong>——seaborn.distplot()</h4><p>如对上面的quality列做直方图，保留概率密度曲线<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">sns.distplot(winedata[<span class="string">'quality'</span>])   <span class="comment"># 不需要概率密度曲线直接将 kde=False 即可</span></div><div class="line">sns.set_style(<span class="string">'dark'</span>)    <span class="comment">#设置背景色</span></div><div class="line">sns.utils.axlabel(<span class="string">'Quality'</span>, <span class="string">'Frequency'</span>) <span class="comment">#设置X,Y坐标名</span></div><div class="line">sns.plt.show()</div></pre></td></tr></table></figure></p>
<p><img src="/images/output_5_0.png" alt="png"></p>
<h4 id="折线图"><a href="#折线图" class="headerlink" title="折线图"></a>折线图</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">sns.factorplot(data=winedata, x=<span class="string">'quality'</span>, y=<span class="string">'total sulfur dioxide'</span>,size=<span class="number">3</span>)</div><div class="line">sns.plt.show()</div></pre></td></tr></table></figure>
<p><img src="/images/output_7_0.png" alt="png"></p>
<h4 id="柱状图——seaborn-barplot"><a href="#柱状图——seaborn-barplot" class="headerlink" title="柱状图——seaborn.barplot()"></a>柱状图——seaborn.barplot()</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">sns.factorplot(data=winedata, x=<span class="string">'quality'</span>, y=<span class="string">'total sulfur dioxide'</span>,kind=<span class="string">'bar'</span>,size=<span class="number">3</span>)</div><div class="line"><span class="comment">#ax = sns.barplot(data=winedata, x='quality', y='total sulfur dioxide',ci=0)</span></div><div class="line">sns.plt.show()</div></pre></td></tr></table></figure>
<p><img src="/images/output_9_0.png" alt="png"></p>
<h4 id="散点图——seaborn-stripplot"><a href="#散点图——seaborn-stripplot" class="headerlink" title="散点图——seaborn.stripplot()"></a>散点图——seaborn.stripplot()</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">temp=sns.FacetGrid(winedata, hue=<span class="string">'quality'</span>, size=<span class="number">3</span>)   <span class="comment">#hue参数设置区分色彩列</span></div><div class="line">temp.map(plt.scatter, <span class="string">'volatile acidity'</span>, <span class="string">'alcohol'</span>)</div><div class="line">temp.add_legend()</div><div class="line">sns.plt.show()</div><div class="line"><span class="comment">#ax = sns.stripplot(x='quality', y='alcohol', data=winedata) #普通散点图</span></div><div class="line"><span class="comment">#ax = sns.stripplot(x='quality', y='alcohol', data=winedata, jitter=True) #带抖动的散点图</span></div><div class="line"><span class="comment">#sns.plt.show()</span></div></pre></td></tr></table></figure>
<p><img src="/images/output_11_0.png" alt="png"></p>
<h4 id="箱型图——seaborn-boxplot"><a href="#箱型图——seaborn-boxplot" class="headerlink" title="箱型图——seaborn.boxplot()"></a>箱型图——seaborn.boxplot()</h4><p>以quality为X轴，alcohol为Y轴，做出箱线图，可以看出异常值<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">ax=sns.boxplot(x=<span class="string">'quality'</span>, y=<span class="string">'alcohol'</span>, data=winedata)</div><div class="line">ax=sns.stripplot(x=<span class="string">'quality'</span>, y=<span class="string">'alcohol'</span>, data=winedata, jitter=<span class="keyword">True</span>, color=<span class="string">'.3'</span>)  <span class="comment">#加上点，jitter=True 使各个散点分开，要不然会是一条直线</span></div><div class="line">sns.plt.show()</div></pre></td></tr></table></figure></p>
<p><img src="/images/output_13_0.png" alt="png"></p>
<h4 id="小提琴图——seaborn-violinplot"><a href="#小提琴图——seaborn-violinplot" class="headerlink" title="小提琴图——seaborn.violinplot()"></a>小提琴图——seaborn.violinplot()</h4><p>可以看出密度分布<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">ax = sns.violinplot(x=<span class="string">'quality'</span>, y=<span class="string">'alcohol'</span>, data=winedata, size=<span class="number">5</span>)</div><div class="line">ax = sns.swarmplot(x=<span class="string">'quality'</span>, y=<span class="string">'alcohol'</span>, data=winedata,color=<span class="string">'.9'</span>)</div><div class="line">sns.plt.show()</div></pre></td></tr></table></figure></p>
<p><img src="/images/output_15_0.png" alt="png"></p>
<h4 id="多变量作图——seaborn-pairplot"><a href="#多变量作图——seaborn-pairplot" class="headerlink" title="多变量作图——seaborn.pairplot()"></a>多变量作图——seaborn.pairplot()</h4><p>seaborn可以一次性两两组合多个变量做出多个对比图，有n个变量，就会做出一个n × n个格子的图，相同的两个变量之间以直方图展示，不同的变量则以散点图展示，<strong>要注意的是数据中不能有NaN（缺失的数据），否则会报错。</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">sns.pairplot(winedata, vars=[<span class="string">'quality'</span>, <span class="string">'residual sugar'</span>,<span class="string">'alcohol'</span>],hue=<span class="string">'quality'</span>)</div><div class="line">sns.plt.show()</div></pre></td></tr></table></figure></p>
<p><img src="/images/output_17_0.png" alt="png"></p>
<h4 id="回归图——seaborn-lmplot-、seaborn-regplot"><a href="#回归图——seaborn-lmplot-、seaborn-regplot" class="headerlink" title="回归图——seaborn.lmplot()、seaborn.regplot()"></a>回归图——seaborn.lmplot()、seaborn.regplot()</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">sns.lmplot(x=<span class="string">'volatile acidity'</span>, y=<span class="string">'alcohol'</span>, data=winedata)   <span class="comment"># hue参数进行分组拟合，markers=['o', 'x']，col参数不同组的子图</span></div><div class="line">sns.plt.show()</div></pre></td></tr></table></figure>
<p><img src="/images/output_19_0.png" alt="png"><br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">sns.regplot(x=<span class="string">'fixed acidity'</span>, y=<span class="string">'alcohol'</span>, data=winedata)</div><div class="line">sns.plt.show()</div></pre></td></tr></table></figure></p>
<p><img src="/images/output_20_0.png" alt="png"></p>
<p><a href="http://seaborn.pydata.org/tutorial.html" target="_blank" rel="external">更多用法参考官方手册</a><br>点这查看本文<a href="https://github.com/phoebepx/normally-accumulate/blob/master/learn_seaborn.ipynb" target="_blank" rel="external">.ipynb文件</a>，欢迎纠错~</p>
]]></content>
    
    <summary type="html">
    
      &lt;blockquote class=&quot;blockquote-center&quot;&gt;&lt;h3 id=&quot;简介&quot;&gt;&lt;a href=&quot;#简介&quot; class=&quot;headerlink&quot; title=&quot;简介&quot;&gt;&lt;/a&gt;简介&lt;/h3&gt;&lt;p&gt;EDA过程中，想要更了解你的数据，选择一个合适的可视化工具，可以说会让你的工作事半功倍。&lt;br&gt;本文主要介绍一个以matplotlib作为底层，更易上手的作图库&lt;code&gt;seaborn&lt;/code&gt;。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="Data Visualization" scheme="http://www.phoebepan.cn/categories/Data-Visualization/"/>
    
    
      <category term="ML" scheme="http://www.phoebepan.cn/tags/ML/"/>
    
      <category term="Visualization" scheme="http://www.phoebepan.cn/tags/Visualization/"/>
    
  </entry>
  
  <entry>
    <title>阅读笔记——7 Techniques to Handle Imbalanced Data</title>
    <link href="http://www.phoebepan.cn/2017/06/05/Imbalanced_data/"/>
    <id>http://www.phoebepan.cn/2017/06/05/Imbalanced_data/</id>
    <published>2017-06-05T07:30:16.000Z</published>
    <updated>2017-06-28T15:15:00.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote class="blockquote-center"><p>这篇阅读笔记，主要介绍处理不平衡数据的常见7种方法。所谓<strong>不平衡数据</strong>，指在网络入侵、癌症监测、银行信用卡检测等领域，出现如下图所示的数据集中，正负样本比例严重失调的情况。<br><img src="/images/imbalanced-data-1.png" alt="imbalanced-data-1" title="正负样本分布"></p>
</blockquote>
<a id="more"></a>
<p><a href="http://www.kdnuggets.com/2017/06/7-techniques-handle-imbalanced-data.html" target="_blank" rel="external">博客中</a>介绍了7种方法帮助我们训练一个分类器，来处理这些不平衡的数据。</p>
<h3 id="1-使用正确的评价指标"><a href="#1-使用正确的评价指标" class="headerlink" title="1.使用正确的评价指标"></a>1.使用正确的评价指标</h3><p>针对上图这样的数据集，如果我们还是采用准确度(accuracy)来评估模型训练结果，那么所有的分类器将所有的测试样本都分到“0”这一类，模型准确率无疑非常好，但显然，这样的model对我们来说，是没有价值的。<br>这种情况，其他适宜的评估指标有：<code>Precision/Specificity</code>、<code>Recall/Sensitivity</code>、<code>F1 score</code>、<code>MCC</code>、<code>AUC</code>、<code>G-Mean</code></p>
<h3 id="2-训练集重新采样-Resample"><a href="#2-训练集重新采样-Resample" class="headerlink" title="2.训练集重新采样(Resample)"></a>2.训练集重新采样(Resample)</h3><p>除了使用不同的评价指标，另外可以通过<strong>下采样</strong>和<strong>过采样</strong>在不平衡数据中得到平衡数据集。</p>
<h4 id="下采样-Under-sampling"><a href="#下采样-Under-sampling" class="headerlink" title="下采样(Under-sampling)"></a>下采样(Under-sampling)</h4><p>当数据量充足时，下采样通过减少负样本数量（即多数的类），即保留正样本和随机选择相同数量的负样本，得到新的平衡训练集。</p>
<h4 id="过采样-Over-sampling"><a href="#过采样-Over-sampling" class="headerlink" title="过采样(Over-sampling)"></a>过采样(Over-sampling)</h4><p>当数据量不够时，过采样通过增加正样本数来平衡数据集，可以采用<code>repetition</code>、<code>bootstrapping</code>、<code>SMOTE</code>得到新的正样本。<br><a href="https://github.com/scikit-learn-contrib/imbalanced-learn" target="_blank" rel="external">Python实现</a></p>
<p>下采样和过采样两者之间没有谁优谁劣，具体用哪种方式取决于数据集本身，有时两者结合使用可能效果更好。</p>
<h3 id="3-正确使用K折交叉验证"><a href="#3-正确使用K折交叉验证" class="headerlink" title="3.正确使用K折交叉验证"></a>3.正确使用K折交叉验证</h3><p>值得注意的是，当我们用过采样处理不平衡训练集时，通常需要在<strong>过采样之前应用交叉验证</strong>，这样做的好处就是避免模型过拟合。</p>
<h4 id="过拟合产生原因："><a href="#过拟合产生原因：" class="headerlink" title="过拟合产生原因："></a>过拟合产生原因：</h4><ul>
<li>模型的复杂度越高，越容易overfitting</li>
<li>数据的噪声越大，越容易overfitting</li>
<li>数据量越少，越容易overfitting</li>
</ul>
<h3 id="4-重采样训练集集成-Ensemble"><a href="#4-重采样训练集集成-Ensemble" class="headerlink" title="4.重采样训练集集成(Ensemble)"></a>4.重采样训练集集成(Ensemble)</h3><p><img src="/images/imbalanced-data-2.png" alt="imbalanced-data-2" title="Ensemble different resampled datasets"><br>如上面示例图所示，使用所有的正样本和 n 个不同的负样本建立 n 个models。比如你想得到10个models，如果正样本是1000个，那么你需要随机选择10000个负样本，然后将这10000个负样本分成10份，接下来训练这10个不同的models。<br>这种方法，简单方便，易扩展，更好的泛化能力。</p>
<h3 id="5-不同比例采样"><a href="#5-不同比例采样" class="headerlink" title="5.不同比例采样"></a>5.不同比例采样</h3><p>之前的方法，都是1:1调和样本，最佳的比例取决于数据和使用的模型。与其对所有models使用同样的比例进行ensemble，更值得尝试的是采用不同的比例进行ensemble。正如下图所示：<br><img src="/images/imbalanced-data-3.png" alt="imbalanced-data-3" title="Resample with different ratios"></p>
<h3 id="6-负样本进行聚类"><a href="#6-负样本进行聚类" class="headerlink" title="6.负样本进行聚类"></a>6.负样本进行聚类</h3><p>Sergey在Quora上提出一个更完美的<a href="www.quora.com/In-classification-how-do-you-handle-an-unbalanced-training-set/answers/1144228?srid=h3G6o">方法</a>，对负样本进行聚类，只用负样本聚类的簇中心和正样本组成训练集。</p>
<h3 id="7-自己设计模型"><a href="#7-自己设计模型" class="headerlink" title="7.自己设计模型"></a>7.自己设计模型</h3><p>事实上，已经有一些models本身就可以处理非平衡数据集，无需进行重新采样，如XGBoost。<br>重设损失函数，比起负样本误分，对正样本误分设置更大的惩罚系数。</p>
<h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><p>这些处理方法只是一个起点，没有一种方法可以解决所有问题，<code>多试才是王道</code>！</p>
]]></content>
    
    <summary type="html">
    
      &lt;blockquote class=&quot;blockquote-center&quot;&gt;&lt;p&gt;这篇阅读笔记，主要介绍处理不平衡数据的常见7种方法。所谓&lt;strong&gt;不平衡数据&lt;/strong&gt;，指在网络入侵、癌症监测、银行信用卡检测等领域，出现如下图所示的数据集中，正负样本比例严重失调的情况。&lt;br&gt;&lt;img src=&quot;/images/imbalanced-data-1.png&quot; alt=&quot;imbalanced-data-1&quot; title=&quot;正负样本分布&quot;&gt;&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="http://www.phoebepan.cn/categories/Machine-Learning/"/>
    
    
      <category term="ML" scheme="http://www.phoebepan.cn/tags/ML/"/>
    
      <category term="Reading" scheme="http://www.phoebepan.cn/tags/Reading/"/>
    
  </entry>
  
  <entry>
    <title>机器学习经典算法优缺点总结</title>
    <link href="http://www.phoebepan.cn/2017/05/23/ML_good_bad/"/>
    <id>http://www.phoebepan.cn/2017/05/23/ML_good_bad/</id>
    <published>2017-05-23T07:30:16.000Z</published>
    <updated>2017-08-01T23:06:56.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote class="blockquote-center"><p>简介<br>机器学习大多数场景是搜索、广告、垃圾过滤、安全、推荐系统等等。本文是经典机器学习算法的优劣势比较，欢迎纠正。</p>
</blockquote>
<a id="more"></a>
<h3 id="朴素贝叶斯"><a href="#朴素贝叶斯" class="headerlink" title="朴素贝叶斯"></a>朴素贝叶斯</h3><p><code>生成模型</code>、<code>贝叶斯定理</code>、<code>特征条件独立</code>、<code>后验概率最大化</code></p>
<h4 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h4><blockquote>
<p>基于贝叶斯定理与特征条件独立假设的分类方法。对于给定的训练集首先基于特征条件独立假设学习输入/输出的联合概率分布，然后利用贝叶斯定理求出后验概率最大。</p>
</blockquote>
<h4 id="解决问题-适用场景"><a href="#解决问题-适用场景" class="headerlink" title="解决问题(适用场景)"></a>解决问题(适用场景)</h4><blockquote>
<p>分类问题<br>场景举例：文本主题分类、垃圾文本过滤</p>
</blockquote>
<h4 id="技术细节"><a href="#技术细节" class="headerlink" title="技术细节"></a>技术细节</h4><blockquote>
<p>概率估计方法：极大似然估计和贝叶斯估计(拉普拉斯平滑)</p>
</blockquote>
<h4 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h4><blockquote>
<p>实现简单；对小规模数据表现很好，适合多分类任务；适合增量式训练</p>
</blockquote>
<h4 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h4><blockquote>
<p>对输入数据表达形式敏感(离散、连续、值极大极小等)；需要条件独立假设，会牺牲准确率，分类性能不一定高</p>
</blockquote>
<h3 id="逻辑回归"><a href="#逻辑回归" class="headerlink" title="逻辑回归"></a>逻辑回归</h3><p><code>对数线性模型</code>、<code>似然函数为目标函数</code>、<code>最优化问题</code></p>
<h4 id="原理-1"><a href="#原理-1" class="headerlink" title="原理"></a>原理</h4><blockquote>
<p>主要计算在某个样本特征下事件发生的概率。用sigmoid函数将线性回归进行了归一化，输出值压缩到0-1之间，这个值代表的是发生的概率。</p>
</blockquote>
<h4 id="解决问题-适用场景-1"><a href="#解决问题-适用场景-1" class="headerlink" title="解决问题(适用场景)"></a>解决问题(适用场景)</h4><blockquote>
<p>分类问题(二分类推广到多分类)<br>场景举例：根据用户浏览情况预测是否购买商品</p>
</blockquote>
<h4 id="技术细节-1"><a href="#技术细节-1" class="headerlink" title="技术细节"></a>技术细节</h4><blockquote>
<ul>
<li>LogReg中，输出Y=1的对数几率是输入x的线性函数；</li>
<li>极大似然估计法估计模型参数；</li>
<li>softmax和k个LR的选择，<strong>类别之间互斥，softmax</strong>、类别之前有联系，K个LR</li>
<li>优化：梯度下降、拟牛顿法、BFGS、改进的迭代尺度法<br>梯度下降会陷入局部最优，改用随机梯度下降，收敛速度更快，且易并行。</li>
</ul>
</blockquote>
<h4 id="优点-1"><a href="#优点-1" class="headerlink" title="优点"></a>优点</h4><blockquote>
<p>简单高效；概率值输出；多重共线性可以通过L2正则化应对</p>
</blockquote>
<h4 id="缺点-1"><a href="#缺点-1" class="headerlink" title="缺点"></a>缺点</h4><blockquote>
<p>欠拟合，精度不高；<br>必须线性可分；<br>特征空间太大时表现不太好；<br>大量分类变量性能较差；</p>
</blockquote>
<h3 id="k-近邻"><a href="#k-近邻" class="headerlink" title="k-近邻"></a>k-近邻</h3><p><code>判别模型</code>、<code>多分类与回归</code>、<code>不具有显式学习过程</code></p>
<h4 id="原理-2"><a href="#原理-2" class="headerlink" title="原理"></a>原理</h4><blockquote>
<p>利用训练数据集对特征向量空间进行划分，根据k个最近邻的训练样本的类别，通过多数表决进行预测。</p>
</blockquote>
<h4 id="解决问题-适用场景-2"><a href="#解决问题-适用场景-2" class="headerlink" title="解决问题(适用场景)"></a>解决问题(适用场景)</h4><blockquote>
<p>分类与回归</p>
</blockquote>
<h4 id="技术细节-2"><a href="#技术细节-2" class="headerlink" title="技术细节"></a>技术细节</h4><blockquote>
<p>三要素：K的选择、距离度量、决策规则</p>
<blockquote>
<p><strong>交叉验证，取最优k值</strong><br>K小，模型变得复杂，过拟合；估计误差增大；<br>K大，模型变得简单，近似误差增大</p>
</blockquote>
<p><strong>kd 树</strong></p>
<blockquote>
<p>X的K个特征，一一个切分，使得每个数据最终都在切分点上(中位数)，对输入的数据搜索kd树，找到K近邻。</p>
</blockquote>
</blockquote>
<h4 id="优点-2"><a href="#优点-2" class="headerlink" title="优点"></a>优点</h4><blockquote>
<p>简单，分类与回归，可用于非线性，复杂度为O(n)，对噪声不敏感</p>
</blockquote>
<h4 id="缺点-2"><a href="#缺点-2" class="headerlink" title="缺点"></a>缺点</h4><blockquote>
<p>K需要人为设定，对大小不平衡数据易偏向大容量数据；计算量大，大量内存</p>
</blockquote>
<h3 id="决策树"><a href="#决策树" class="headerlink" title="决策树"></a>决策树</h3><p><code>判别模型</code>、<code>多分类与回归</code>、<code>正则化的极大似然估计</code></p>
<h4 id="原理-3"><a href="#原理-3" class="headerlink" title="原理"></a>原理</h4><blockquote>
<p>决策树实际上是将空间用超平面进行划分的一种方法，每次分割的时候，都将当前的空间一分为二，这样使得每一个叶子节点都是在空间中的一个不相交的区域，在进行决策的时候，会根据输入样本每一维feature的值，一步一步往下，最后使得样本落入N个区域中的一个。</p>
</blockquote>
<h4 id="解决问题-适用场景-3"><a href="#解决问题-适用场景-3" class="headerlink" title="解决问题(适用场景)"></a>解决问题(适用场景)</h4><blockquote>
<p>适用于小数据集<br>场景举例：基于规则的信用评估</p>
</blockquote>
<h4 id="技术细节-3"><a href="#技术细节-3" class="headerlink" title="技术细节"></a>技术细节</h4><blockquote>
<p><strong>特征选择</strong>准则：信息增益或信息增益比<br><strong>决策树生成</strong>，ID3、C4.5、CART(Gini index，平方误差)<br><strong>剪枝</strong>，减小模型复杂度，设定$ \alpha $ ，相当于正则化的极大似然估计；动态规划实现</p>
<blockquote>
<p><strong>CART</strong><br>通过递归方式建立决策二叉树，基尼指数最小化的特征(分类)，平方误差最小化(回归)作为划分特征</p>
</blockquote>
</blockquote>
<h4 id="优点-3"><a href="#优点-3" class="headerlink" title="优点"></a>优点</h4><blockquote>
<p>训练时间复杂度低，预测过程快速；可读性好；适合处理有缺失属性值得样本，能够处理不相关特征</p>
</blockquote>
<h4 id="缺点-3"><a href="#缺点-3" class="headerlink" title="缺点"></a>缺点</h4><blockquote>
<p>容易过拟合</p>
<blockquote>
<p>解决过拟合：剪枝、交叉验证、随机森林</p>
</blockquote>
<p>单棵树分类能力弱，对连续变量难以处理</p>
</blockquote>
<h3 id="随机森林"><a href="#随机森林" class="headerlink" title="随机森林"></a>随机森林</h3><p><code>判别模型</code>、<code>多分类与回归</code>、<code>正则化的极大似然估计</code>、<code>Random Future</code>、<code>Bagging</code></p>
<h4 id="原理-4"><a href="#原理-4" class="headerlink" title="原理"></a>原理</h4><blockquote>
<p>很多随机生成的决策树组成，预测时，每颗决策树进行判断，最后通过Bagging思想进行结果的输出。<br>RF被证明对大规模数据集和存在大量且有时不相关特征的项来说很有用。</p>
</blockquote>
<h4 id="解决问题-适用场景-4"><a href="#解决问题-适用场景-4" class="headerlink" title="解决问题(适用场景)"></a>解决问题(适用场景)</h4><blockquote>
<p>场景举例：用户流失分析、风险评估、检测离群点</p>
</blockquote>
<h4 id="技术细节-4"><a href="#技术细节-4" class="headerlink" title="技术细节"></a>技术细节</h4><blockquote>
<p>行(有放回)列(节点分裂时)采样，每个决策树使用相同参数<br>完全分裂，叶节点特征(样本特征相同、样本类别相同、样本数=1)<br>超参：树的数量、列采样特征数(通常取总特征的平方根)<br>泛化误差估计，oob(out-of-bag)</p>
</blockquote>
<h4 id="优点-4"><a href="#优点-4" class="headerlink" title="优点"></a>优点</h4><blockquote>
<ol>
<li>适合多分类问题，训练和预测速度快</li>
<li>对训练数据容错能力强，有效的估计缺失数据的方法</li>
<li>可以处理高维数据且不用特征选择</li>
<li>训练过程中检测到特征间相互影响及特征重要性</li>
<li>数据集上表现良好，避免过拟合</li>
<li>实现简单且容易并行化</li>
<li>在创建随机森林的时候，对generlization error使用的是无偏估计，所以在随机森林算法中不需要再进行交叉验证或者单独的测试集来获取测试集误差的无偏估计</li>
</ol>
</blockquote>
<h4 id="缺点-4"><a href="#缺点-4" class="headerlink" title="缺点"></a>缺点</h4><blockquote>
<p>噪声较大的分类和回归问题上会过拟合。对于类别特征的属性值较多时，RF产生的属性权重不可信。</p>
</blockquote>
<h3 id="支持向量机"><a href="#支持向量机" class="headerlink" title="支持向量机"></a>支持向量机</h3><p><code>间隔最大</code>、<code>凸二次规划</code>、<code>核函数</code></p>
<h4 id="原理-5"><a href="#原理-5" class="headerlink" title="原理"></a>原理</h4><blockquote>
<p>低维空间映射到高维空间，实现线性可分，求解正确划分训练集并(几何)间隔最大化的分离超平面。</p>
</blockquote>
<h4 id="技术细节-5"><a href="#技术细节-5" class="headerlink" title="技术细节"></a>技术细节</h4><blockquote>
<p>函数间隔，几何间隔<br>应用拉格朗日对偶性，求解对偶问题(更易求解、自然引用核函数)<br>正则化的合页损失函数最优化问题<br>核函数</p>
<blockquote>
<p>表示将输入从输入空间映射到特征空间得到的特征向量之间的内积。</p>
</blockquote>
<p>序列最小最优化算法(不断分解，对子问题求解，每个子问题中选择两个变量优化，其中一个变量是违反KKT条件的，直到所有变量都满足KKT条件为止)</p>
</blockquote>
<h4 id="优点-5"><a href="#优点-5" class="headerlink" title="优点"></a>优点</h4><blockquote>
<p>可以处理高维特征；使用核函数可以向高维空间进行映射，可以解决非线性分类；分类思想简单(间隔最大化)；分类效果好</p>
</blockquote>
<h4 id="缺点-5"><a href="#缺点-5" class="headerlink" title="缺点"></a>缺点</h4><blockquote>
<p>核函数及参数敏感；无法直接支持多分类；大量观测样本，效率低</p>
</blockquote>
<h3 id="集成学习"><a href="#集成学习" class="headerlink" title="集成学习"></a>集成学习</h3><h4 id="关键点"><a href="#关键点" class="headerlink" title="关键点"></a>关键点</h4><ol>
<li><code>差异性基分类器</code>:数据集(bagging、boosting)、数据特征、改变基分类器参数</li>
<li><code>基分类器整合</code>，回归(简单平均、加权平均)、分类(简单/加权投票、概率投票)</li>
</ol>
<p><code>Notes</code>：分类问题，每个分类器的分类精度大于0.5</p>
<h3 id="Boosting"><a href="#Boosting" class="headerlink" title="Boosting"></a>Boosting</h3><p><code>三个臭皮匠顶个诸葛亮</code>、<code>改变样本权重</code>、<code>分而治之</code>、<code>加法模型</code>、<code>前向分布算法</code>、<code>Shrinkage</code></p>
<h4 id="原理-6"><a href="#原理-6" class="headerlink" title="原理"></a>原理</h4><blockquote>
<p>在分类问题中，通过改变训练样本的权重，学习多个基分类器，最终进行线性组合。</p>
</blockquote>
<h4 id="技术细节-6"><a href="#技术细节-6" class="headerlink" title="技术细节"></a>技术细节</h4><blockquote>
<p>概念强可学习充要条件概念弱可学习</p>
<blockquote>
<ul>
<li>AdaBoost，提高前一轮弱分类器错分的<strong>样本权重</strong>，降低分对的样本权重；<strong>组合</strong>时，加大分类错误率低的基分类器权值，减小分类错误率大的权重。</li>
<li>AdaBoost二分类学习时是加法模型、损失函数为指数函数、学习算法为前向分布算法</li>
</ul>
</blockquote>
<p>boosting tree<br>基分类器，分类树或回归树，不同的问题的提升树主要区别在于<strong>损失函数</strong>不同。</p>
<blockquote>
<ul>
<li>梯度提升(gradient boosting)，每一轮计算为了减少上一次的残差，加入新的model是在之前model损失函数梯度下降方向。</li>
<li>GBDT精髓，在于训练都以上一棵树的残差为目标去提升。</li>
</ul>
</blockquote>
<p>调参：树的个数、树深度、学习速率、叶子上最大节点树、训练采样比例、训练特征采样比例……</p>
</blockquote>
<h4 id="优点-6"><a href="#优点-6" class="headerlink" title="优点"></a>优点</h4><blockquote>
<p>精度高；能处理非线性数据；能处理多特征类型；适合低维稠密数据</p>
</blockquote>
<h4 id="缺点-6"><a href="#缺点-6" class="headerlink" title="缺点"></a>缺点</h4><blockquote>
<p>并行麻烦；多分类时，复杂度大</p>
</blockquote>
<h3 id="EM"><a href="#EM" class="headerlink" title="EM"></a>EM</h3><p><code>含隐变量的概率模型</code>、<code>极大似然估计</code></p>
<h4 id="原理-7"><a href="#原理-7" class="headerlink" title="原理"></a>原理</h4><blockquote>
<p>从不完全数据或有数据丢失的数据集（存在隐含变量）中求解概率模型参数的最大似然估计方法。</p>
</blockquote>
<h4 id="解决问题-适用场景-5"><a href="#解决问题-适用场景-5" class="headerlink" title="解决问题(适用场景)"></a>解决问题(适用场景)</h4><blockquote>
<p>含有隐变量、非监督学习<br>高斯混合模型学习中的应用</p>
</blockquote>
<h4 id="技术细节-7"><a href="#技术细节-7" class="headerlink" title="技术细节"></a>技术细节</h4><blockquote>
<p>E步，求期望，根据上一步迭代的模型参数计算隐变量的后验概率；M步，求极大，似然函数最大化获得新的参数值</p>
</blockquote>
<h4 id="优点-7"><a href="#优点-7" class="headerlink" title="优点"></a>优点</h4><blockquote>
<p>思想简单，普适</p>
</blockquote>
<h4 id="缺点-7"><a href="#缺点-7" class="headerlink" title="缺点"></a>缺点</h4><blockquote>
<p>EM算法与初值的选择有关，不同初值，得到不同参数估计；</p>
</blockquote>
<h3 id="归一化使梯度下降收敛更好？"><a href="#归一化使梯度下降收敛更好？" class="headerlink" title="归一化使梯度下降收敛更好？"></a>归一化使梯度下降收敛更好？</h3><blockquote>
<p>如果不归一化，各维特征的跨度差距很大，梯度下降时，目标函数梯度方向就会偏离最小值的方向，走很多弯路，如下方左图。<br>如果归一化了，每一步梯度的方向都指向最小值，收敛更快，如下方右图。<br><img src="/images/descend.png" alt="descend"></p>
</blockquote>
<h3 id="算法横向比较"><a href="#算法横向比较" class="headerlink" title="算法横向比较"></a>算法横向比较</h3><h4 id="LR与Linear-SVM"><a href="#LR与Linear-SVM" class="headerlink" title="LR与Linear SVM"></a>LR与Linear SVM</h4><ul>
<li>都是线性分类器；</li>
<li>都会受到异常点影响；</li>
<li>本质不同——损失函数，一个hinge loss，一个logistical loss</li>
<li>Linear SVM只受支持向量影响(部分样本)，LR则受所有数据点影响；</li>
<li>Linear SVM需要归一化数据，LR不受影响；</li>
<li>Linear SVM自带正则，而LR需另加；</li>
<li>海量数据LR使用更广泛，Linear SVM内存消耗大</li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;blockquote class=&quot;blockquote-center&quot;&gt;&lt;p&gt;简介&lt;br&gt;机器学习大多数场景是搜索、广告、垃圾过滤、安全、推荐系统等等。本文是经典机器学习算法的优劣势比较，欢迎纠正。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="http://www.phoebepan.cn/categories/Machine-Learning/"/>
    
    
      <category term="ML" scheme="http://www.phoebepan.cn/tags/ML/"/>
    
  </entry>
  
  <entry>
    <title>推荐系统实战——架构</title>
    <link href="http://www.phoebepan.cn/2017/05/22/recsys/"/>
    <id>http://www.phoebepan.cn/2017/05/22/recsys/</id>
    <published>2017-05-22T07:30:16.000Z</published>
    <updated>2017-07-20T07:40:50.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote class="blockquote-center"><h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p>推荐系统的核心任务可拆解成两部分，一个是如何为给定用户生成特征，另一个是如何根据特征找到物品。用户特征包括，<code>人口统计学特征</code>，<code>用户行为特征</code>，<code>用户话题特征</code>。本文是具体的基于物品的推荐算法系统的架构学习笔记。</p>
</blockquote>
<a id="more"></a>
<h3 id="推荐系统架构图"><a href="#推荐系统架构图" class="headerlink" title="推荐系统架构图"></a>推荐系统架构图</h3><p><img src="/images/recsys_arich.png" alt="recsys_arich" title="基于物品的推荐算法架构图"></p>
<h3 id="生成用户特征向量"><a href="#生成用户特征向量" class="headerlink" title="生成用户特征向量"></a>生成用户特征向量</h3><ul>
<li><strong>用户行为的种类</strong>，一般的标准就是用户付出代价越大的行为权重越高；</li>
<li><strong>用户行为产生的时间</strong>，一般来说，用户近期的行为比较重要，而用户很久之前的行为相对比较次要；</li>
<li><strong>用户行为的次数</strong>，用户对同一个物品的同一种行为发生的次数也反映了用户对物品的兴趣，行为次数多的物品对应的特征权重越高；</li>
<li><strong>物品的热门程度</strong>，加重不热门物品对应的特征的权重。</li>
</ul>
<h3 id="特征—物品相关推荐"><a href="#特征—物品相关推荐" class="headerlink" title="特征—物品相关推荐"></a>特征—物品相关推荐</h3><p>可以离线计算很多相关表，在线服务启动时会将这些相关表按照配置的权<br>重相加，得到最终的相关表保存在内存中，而在给用户进行推荐时，用的是加权后的相关表。</p>
<h3 id="过滤模块"><a href="#过滤模块" class="headerlink" title="过滤模块"></a>过滤模块</h3><ul>
<li>用户已经产生过行为物品</li>
<li>候选物品以外的物品</li>
<li>某些质量很差的物品</li>
</ul>
<h3 id="排名模块"><a href="#排名模块" class="headerlink" title="排名模块"></a>排名模块</h3><ul>
<li>新颖性排名，尽量推荐他们不知道的、长尾中的物品，热门的物品进行降权；</li>
<li>多样性；</li>
<li>时间多样性；</li>
<li>用户反馈，点击率预测。</li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;blockquote class=&quot;blockquote-center&quot;&gt;&lt;h3 id=&quot;简介&quot;&gt;&lt;a href=&quot;#简介&quot; class=&quot;headerlink&quot; title=&quot;简介&quot;&gt;&lt;/a&gt;简介&lt;/h3&gt;&lt;p&gt;推荐系统的核心任务可拆解成两部分，一个是如何为给定用户生成特征，另一个是如何根据特征找到物品。用户特征包括，&lt;code&gt;人口统计学特征&lt;/code&gt;，&lt;code&gt;用户行为特征&lt;/code&gt;，&lt;code&gt;用户话题特征&lt;/code&gt;。本文是具体的基于物品的推荐算法系统的架构学习笔记。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="Recommendation" scheme="http://www.phoebepan.cn/categories/Recommendation/"/>
    
    
      <category term="Recommendation" scheme="http://www.phoebepan.cn/tags/Recommendation/"/>
    
  </entry>
  
  <entry>
    <title>推荐系统实战——社交网络数据</title>
    <link href="http://www.phoebepan.cn/2017/05/21/social_data/"/>
    <id>http://www.phoebepan.cn/2017/05/21/social_data/</id>
    <published>2017-05-21T07:30:16.000Z</published>
    <updated>2017-07-20T06:13:49.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote class="blockquote-center"><h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p><strong>六度原理</strong>讲的是社会中任意两个人都可以通过不超过6个人的路径相互认识。在现实社会中，很多时候我们都是通过朋友获得推荐，基于社交网络的推荐可以很好地模拟现实社会。本文是利用社交网络数据给用户进行个性化推荐相关内容的学习。</p>
</blockquote>
<a id="more"></a>
<h3 id="思维导图"><a href="#思维导图" class="headerlink" title="思维导图"></a>思维导图</h3><p><img src="/images/social_data.png" alt="social_data"></p>
<p> [1]项亮，推荐系统实战.</p>
]]></content>
    
    <summary type="html">
    
      &lt;blockquote class=&quot;blockquote-center&quot;&gt;&lt;h3 id=&quot;简介&quot;&gt;&lt;a href=&quot;#简介&quot; class=&quot;headerlink&quot; title=&quot;简介&quot;&gt;&lt;/a&gt;简介&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;六度原理&lt;/strong&gt;讲的是社会中任意两个人都可以通过不超过6个人的路径相互认识。在现实社会中，很多时候我们都是通过朋友获得推荐，基于社交网络的推荐可以很好地模拟现实社会。本文是利用社交网络数据给用户进行个性化推荐相关内容的学习。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="Recommendation" scheme="http://www.phoebepan.cn/categories/Recommendation/"/>
    
    
      <category term="Recommendation" scheme="http://www.phoebepan.cn/tags/Recommendation/"/>
    
  </entry>
  
  <entry>
    <title>推荐系统实战——上下文信息</title>
    <link href="http://www.phoebepan.cn/2017/05/20/context/"/>
    <id>http://www.phoebepan.cn/2017/05/20/context/</id>
    <published>2017-05-20T07:30:16.000Z</published>
    <updated>2017-07-20T06:13:19.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote class="blockquote-center"><h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p>将最符合用户兴趣的物品推荐给用户时，用户此时、此刻、心情等等，是提高推荐系统算法不可忽视的一环，这些信息在推荐系统中对应的专业名词是上下文(context)，本文主要说说<strong>上下文信息</strong>的事儿~</p>
</blockquote>
<a id="more"></a>
<h3 id="思维导图"><a href="#思维导图" class="headerlink" title="思维导图"></a>思维导图</h3><p><img src="/images/context.png" alt="context"></p>
<p> [1]项亮，推荐系统实战.</p>
]]></content>
    
    <summary type="html">
    
      &lt;blockquote class=&quot;blockquote-center&quot;&gt;&lt;h3 id=&quot;简介&quot;&gt;&lt;a href=&quot;#简介&quot; class=&quot;headerlink&quot; title=&quot;简介&quot;&gt;&lt;/a&gt;简介&lt;/h3&gt;&lt;p&gt;将最符合用户兴趣的物品推荐给用户时，用户此时、此刻、心情等等，是提高推荐系统算法不可忽视的一环，这些信息在推荐系统中对应的专业名词是上下文(context)，本文主要说说&lt;strong&gt;上下文信息&lt;/strong&gt;的事儿~&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="Recommendation" scheme="http://www.phoebepan.cn/categories/Recommendation/"/>
    
    
      <category term="Recommendation" scheme="http://www.phoebepan.cn/tags/Recommendation/"/>
    
  </entry>
  
  <entry>
    <title>推荐系统实战——用户标签数据</title>
    <link href="http://www.phoebepan.cn/2017/05/19/user_tags/"/>
    <id>http://www.phoebepan.cn/2017/05/19/user_tags/</id>
    <published>2017-05-19T07:30:16.000Z</published>
    <updated>2017-07-19T08:25:13.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote class="blockquote-center"><h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p>推荐系统的目的是联系用户的兴趣和物品，倘若通过一些特征(feature)联系用户和物品，给用户推荐那些具有用户喜欢的特征的物品，标签作为一种无层次化结构的、用来描述信息的关键词。本文主要介绍有关<strong>标签</strong>在推荐系统中的应用。</p>
</blockquote>
<a id="more"></a>
<h3 id="思维导图"><a href="#思维导图" class="headerlink" title="思维导图"></a>思维导图</h3><p><img src="/images/user_tag.png" alt="user_tag"></p>
<p> [1]项亮，推荐系统实战.</p>
]]></content>
    
    <summary type="html">
    
      &lt;blockquote class=&quot;blockquote-center&quot;&gt;&lt;h3 id=&quot;简介&quot;&gt;&lt;a href=&quot;#简介&quot; class=&quot;headerlink&quot; title=&quot;简介&quot;&gt;&lt;/a&gt;简介&lt;/h3&gt;&lt;p&gt;推荐系统的目的是联系用户的兴趣和物品，倘若通过一些特征(feature)联系用户和物品，给用户推荐那些具有用户喜欢的特征的物品，标签作为一种无层次化结构的、用来描述信息的关键词。本文主要介绍有关&lt;strong&gt;标签&lt;/strong&gt;在推荐系统中的应用。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="Recommendation" scheme="http://www.phoebepan.cn/categories/Recommendation/"/>
    
    
      <category term="Recommendation" scheme="http://www.phoebepan.cn/tags/Recommendation/"/>
    
  </entry>
  
  <entry>
    <title>推荐系统实战——冷启动</title>
    <link href="http://www.phoebepan.cn/2017/05/18/cold_start/"/>
    <id>http://www.phoebepan.cn/2017/05/18/cold_start/</id>
    <published>2017-05-18T07:30:16.000Z</published>
    <updated>2017-07-19T08:24:24.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote class="blockquote-center"><h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p>前一篇博文的介绍，可见大量用户行为数据是推荐系统的先决条件。那么，如何在没有大量用户数据的情况下设计个性化推荐系统并让用户对推荐结果满意而增加用户粘性，这就是本文所介绍的冷启动问题。</p>
</blockquote>
<a id="more"></a>
<h3 id="思维导图"><a href="#思维导图" class="headerlink" title="思维导图"></a>思维导图</h3><p><img src="/images/coldstart.png" alt="coldstart"></p>
<h3 id="用户注册信息"><a href="#用户注册信息" class="headerlink" title="用户注册信息"></a>用户注册信息</h3><p>基于用户注册信息的推荐算法其核心问题是计算每种特征的用户喜<br>欢的物品，将 p(f,i)定义为喜欢物品i的用户中具有特征f的比例，$$ p(f,i)=\frac{\left | N(i)\bigcap U(f) \right |}{\left | N(i) \right | + \alpha} $$，参数$\alpha$（较大的数）的目的是解决数据稀疏问题。</p>
<h4 id="如何选择合适的物品启动用户的兴趣"><a href="#如何选择合适的物品启动用户的兴趣" class="headerlink" title="如何选择合适的物品启动用户的兴趣"></a>如何选择合适的物品启动用户的兴趣</h4><p>Nadav Golbandi的算法通过建立物品区分度的决策树，来启动用户的兴趣，下图是Nadav Golbandi算法的举例，<br><img src="/images/Nadav_Golbandi.png" alt="Nadav Golbandi"></p>
<h3 id="物品内容信息"><a href="#物品内容信息" class="headerlink" title="物品内容信息"></a>物品内容信息</h3><p>内容相似度计算简单，能频繁更新，而且能够解决物品冷启动问题，那么为什么还需要协同过滤的算法?</p>
<ul>
<li>内容过滤算法忽视了用户行为，从而也忽视了物品的流行度以及用户行为中所包含的规律，所以它的精度比较低，但结果的新颖度却比较高。</li>
<li>如果用户的行为强烈受某一内容属性的影响，那么内容过滤的算法还是可以在精度上超过协同过滤算法的。</li>
</ul>
<p>所以，通常将这两种算法融合，可获得比单独使用这两种算法更好的效果。</p>
<h4 id="话题模型（topic-model）"><a href="#话题模型（topic-model）" class="headerlink" title="话题模型（topic model）"></a>话题模型（topic model）</h4><p>“推荐系统的动态特性”和“基于时间的协同过滤算法研究”，这两篇文章title关键词不同，但关键词所属话题相同。这种情况下，先知道文章的话题分布，然后才能准确地计算文章的相似度。</p>
<h4 id="KL散度"><a href="#KL散度" class="headerlink" title="KL散度"></a>KL散度</h4><p><strong>定义：</strong><br>对于<code>离散空间</code>两个概率分布P和Q，从Q到P的KL散度为<img src="/images/lisan_KL.gif" alt="lisan_KL"><br>对于<code>连续空间</code>的两个概率分布P和Q，从Q到P的KL散度为：<img src="/images/lianxu_KL.gif" alt="lianxu_KL">，p和q是概率分布P和Q的概率密度。<br><strong>简单例子计算：</strong><br>比如有四个类别，一个方法P得到四个类别的概率分别是0.1，0.2，0.3，0.4。另一种方法Q（或者说是事实情况）是得到四个类别的概率分别是0.4，0.3，0.2，0.1,那么这两个分布的 KL 散度就是：<br><img src="/images/KL_eg.gif" alt="KL_eg"><br><strong>实际案例</strong><br>参考附录2<br> [1]项亮，推荐系统实战.<br> <a href="http://chuansong.me/n/2759305" target="_blank" rel="external">[2] KL散度（从动力系统到推荐系统）</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;blockquote class=&quot;blockquote-center&quot;&gt;&lt;h3 id=&quot;简介&quot;&gt;&lt;a href=&quot;#简介&quot; class=&quot;headerlink&quot; title=&quot;简介&quot;&gt;&lt;/a&gt;简介&lt;/h3&gt;&lt;p&gt;前一篇博文的介绍，可见大量用户行为数据是推荐系统的先决条件。那么，如何在没有大量用户数据的情况下设计个性化推荐系统并让用户对推荐结果满意而增加用户粘性，这就是本文所介绍的冷启动问题。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="Recommendation" scheme="http://www.phoebepan.cn/categories/Recommendation/"/>
    
    
      <category term="Recommendation" scheme="http://www.phoebepan.cn/tags/Recommendation/"/>
    
  </entry>
  
  <entry>
    <title>推荐系统实战——用户行为分析</title>
    <link href="http://www.phoebepan.cn/2017/05/17/user_action_analysis/"/>
    <id>http://www.phoebepan.cn/2017/05/17/user_action_analysis/</id>
    <published>2017-05-17T07:30:16.000Z</published>
    <updated>2017-07-19T08:24:59.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote class="blockquote-center"><h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p>古语，听其言，观其行，用户的行为不是随机的，而是蕴含着很多模式，通过算法自动挖掘用户行为数据，从而推测出用户的兴趣和需求，做出个性化推荐。本文是自己对用户行为分析的学习笔记。</p>
</blockquote>
<a id="more"></a>
<h3 id="思维导图"><a href="#思维导图" class="headerlink" title="思维导图"></a>思维导图</h3><p><img src="/images/user_action_analysis.png" alt="user_action_analysis"></p>
<h3 id="基于邻域算法"><a href="#基于邻域算法" class="headerlink" title="基于邻域算法"></a>基于邻域算法</h3><h4 id="基于用户的协同过滤-UserCF"><a href="#基于用户的协同过滤-UserCF" class="headerlink" title="基于用户的协同过滤(UserCF)"></a>基于用户的协同过滤(UserCF)</h4><p>推荐原理：给用户推荐那些和他有共同兴趣爱好的用户喜欢的物品，着重于和用户兴趣相似的小群体的热点，更社会化。<br>主要包括两步：</p>
<ul>
<li>找到和目标用户兴趣相似的用户集合</li>
<li>找到集合中用户喜欢且目标用户没有行为的物品做出推荐</li>
</ul>
<p><img src="/images/user_item_reversort.png" alt="reserve_sort"><br>上面这张图是，用户行为数据导出用户间共现矩阵的过程，基于该共现矩阵，利用余弦相似度（或Jaccard）得到用户间兴趣相似度后，UserCF算法会给用户推荐和他兴趣最相似的K个用户喜欢的物品。用户u对物品i的感兴趣程度：<br><img src="/images/pui.png" alt="pui"><br>,S(u,K)包含和用户u兴趣最接近的K个用户，N(i)是对物品i有过行为的用户集合，w<sub>uv</sub>是用户u,v的兴趣相似度，r<sub>vi</sub>用户v对物品i的兴趣。<br><strong>改进</strong><br>计算用户间兴趣相似度时，增加用户u,v共同兴趣列表中热门物品的惩罚。原理类似TF-IDF。</p>
<h4 id="基于物品的协同过滤-ItemCF"><a href="#基于物品的协同过滤-ItemCF" class="headerlink" title="基于物品的协同过滤(ItemCF)"></a>基于物品的协同过滤(ItemCF)</h4><p>推荐原理：给用户推荐那些和他之前喜欢的物品类似的物品，着重于维系用户历史兴趣，更个性化。<br>主要包括两步：</p>
<ul>
<li>计算物品间相似度</li>
<li>根据物品相似度和用户历史行为做出推荐</li>
</ul>
<p>ItemCF结果可解释性更好。<br><strong>改进</strong></p>
<ul>
<li>计算物品相似度时，增加活跃用户贡献的惩罚；</li>
<li>物品相似度的归一化，可提高推荐的多样性。</li>
</ul>
<h4 id="UserCF和ItemCF比较："><a href="#UserCF和ItemCF比较：" class="headerlink" title="UserCF和ItemCF比较："></a>UserCF和ItemCF比较：</h4><p><img src="/images/UserCF_ItemCF.png" alt="UserCF_ItemCF"></p>
<h3 id="隐语义模型-LFM"><a href="#隐语义模型-LFM" class="headerlink" title="隐语义模型(LFM)"></a>隐语义模型(LFM)</h3><p>LFM和基于邻域的方法的比较：</p>
<ul>
<li>理论基础：LFM较好的理论依据，邻域主要是基于统计方法；</li>
<li>离线计算的空间复杂度：LFM更节省空间；</li>
<li>离线计算的时间复杂度：一般情况，LFM高于CF；</li>
<li>在线实时推荐：传统LFM难以实现实时；</li>
<li>推荐解释：ItemCF好于LFM。</li>
</ul>
<p>注：当数据集非常稀疏时，LFM的性能会明显下降，甚至不如UserCF和ItemCF的性能。</p>
<h3 id="基于图的模型"><a href="#基于图的模型" class="headerlink" title="基于图的模型"></a>基于图的模型</h3><p>图中顶点的相关性高的一对顶点一般具有如下特征：</p>
<ul>
<li>两个顶点之间有很多路径相连；</li>
<li>连接两个顶点之间的路径长度都比较短；</li>
<li>连接两个顶点之间的路径不会经过出度比较大的顶点。</li>
</ul>
<p><strong>随机游走</strong>算法思路：<br>假如要给用户A进行个性化推荐，可以从用户A对应的节点开始在用户物品<strong>二分图</strong>（如下图）上随机游走，游走到任何一个节点时，首先按照概率a决定是否继续游走，若继续，就从当前节点指向的节点中按照均匀分布随机选择一个节点作为下一次游走经过的节点；否则，停止该次游走，从用户A对应节点重新开始游走。多次迭代，直到每个物品节点被访问到的概率收敛到一个数。<br><img src="/images/part2graph.png" alt="part2graph"></p>
<p> [1]项亮，推荐系统实战.</p>
]]></content>
    
    <summary type="html">
    
      &lt;blockquote class=&quot;blockquote-center&quot;&gt;&lt;h3 id=&quot;简介&quot;&gt;&lt;a href=&quot;#简介&quot; class=&quot;headerlink&quot; title=&quot;简介&quot;&gt;&lt;/a&gt;简介&lt;/h3&gt;&lt;p&gt;古语，听其言，观其行，用户的行为不是随机的，而是蕴含着很多模式，通过算法自动挖掘用户行为数据，从而推测出用户的兴趣和需求，做出个性化推荐。本文是自己对用户行为分析的学习笔记。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="Recommendation" scheme="http://www.phoebepan.cn/categories/Recommendation/"/>
    
    
      <category term="Recommendation" scheme="http://www.phoebepan.cn/tags/Recommendation/"/>
    
  </entry>
  
  <entry>
    <title>推荐系统实战——概述</title>
    <link href="http://www.phoebepan.cn/2017/05/16/Recommendation_action1/"/>
    <id>http://www.phoebepan.cn/2017/05/16/Recommendation_action1/</id>
    <published>2017-05-16T07:30:16.000Z</published>
    <updated>2017-07-19T08:24:32.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote class="blockquote-center"><h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p>信息过载的现在，催生出了个性化推荐系统，“千人千面”，分析你的历史兴趣，发现对用户有价值的信息，同时，让信息能够展现在对它感兴趣的用户面前，可谓是双赢的酷事！选择《推荐系统实战》这本书作为自己的入门读物，一步一步深入学习个性化推荐，本文是推荐系统的概述，作为系列开篇之记。</p>
</blockquote>
<a id="more"></a>
<h3 id="思维导图"><a href="#思维导图" class="headerlink" title="思维导图"></a>思维导图</h3><p>下面这张思维导图，可以粗略了解推荐系统是什么，解决什么问题，有什么用（经典三问，是什么？为什么？怎么样？），以及如何评价推荐系统优劣？<br><img src="/images/recommend.png" alt="recommend"></p>
<h3 id="附1：覆盖率——基尼系数计算原理："><a href="#附1：覆盖率——基尼系数计算原理：" class="headerlink" title="附1：覆盖率——基尼系数计算原理："></a>附1：覆盖率——基尼系数计算原理：</h3><p>结合下图，gini系数的形象化解释为（黑色曲线表示最不热门的x%物品的总流行度占系统的比例y%）,$$Gini=\frac{A的面积}{(A+B)的面积} $$<br><img src="/images/gini.png" alt="gini"><br>由此可见，如果系统物品流行度分配很不均匀，那么分子就会很大，从而基尼系数也会很大。</p>
<h3 id="附2：一个推荐算法最终上线，需完成3个实验。"><a href="#附2：一个推荐算法最终上线，需完成3个实验。" class="headerlink" title="附2：一个推荐算法最终上线，需完成3个实验。"></a>附2：一个推荐算法最终上线，需完成3个实验。</h3><ul>
<li>离线实验证明他在很多离线指标上优于现有算法；</li>
<li>通过用户调查确定它的用户满意度不低于现有算法；</li>
<li>在线AB测试确定它在我们关心的指标上优于现有算法。</li>
</ul>
<h3 id="附3：离线实验的优化目标"><a href="#附3：离线实验的优化目标" class="headerlink" title="附3：离线实验的优化目标"></a>附3：离线实验的优化目标</h3><p>用一个数学公式表达，如下：<br>最大化预测准确度<br>使得 覆盖率 &gt; A，多样性 &gt; B，新颖性 &gt; C</p>
<p> [1]项亮，推荐系统实战.</p>
]]></content>
    
    <summary type="html">
    
      &lt;blockquote class=&quot;blockquote-center&quot;&gt;&lt;h3 id=&quot;简介&quot;&gt;&lt;a href=&quot;#简介&quot; class=&quot;headerlink&quot; title=&quot;简介&quot;&gt;&lt;/a&gt;简介&lt;/h3&gt;&lt;p&gt;信息过载的现在，催生出了个性化推荐系统，“千人千面”，分析你的历史兴趣，发现对用户有价值的信息，同时，让信息能够展现在对它感兴趣的用户面前，可谓是双赢的酷事！选择《推荐系统实战》这本书作为自己的入门读物，一步一步深入学习个性化推荐，本文是推荐系统的概述，作为系列开篇之记。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="Recommendation" scheme="http://www.phoebepan.cn/categories/Recommendation/"/>
    
    
      <category term="Recommendation" scheme="http://www.phoebepan.cn/tags/Recommendation/"/>
    
  </entry>
  
  <entry>
    <title>充实的无话可说</title>
    <link href="http://www.phoebepan.cn/2017/05/15/fine_life/"/>
    <id>http://www.phoebepan.cn/2017/05/15/fine_life/</id>
    <published>2017-05-15T07:30:16.000Z</published>
    <updated>2017-06-04T02:26:13.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote class="blockquote-center"><p>有人说，人生有两大遗憾，太晚谈恋爱，太早读经典。读懂一本书，看懂一部电影和谈对一场恋爱，都需要刚刚好的时间。太早理解不了，太晚就少了份陪伴感。</p>
</blockquote>
<a id="more"></a>
<h3 id="碎碎念1"><a href="#碎碎念1" class="headerlink" title="碎碎念1"></a><center>碎碎念1</center></h3><blockquote>
<p>好朋友问我，嘿，最近怎么不见你出去浪了呢，端午打算去哪耍去？<br>我笑着答道，宅家看书。<br>朋友诡异的看着我说，不是吧，学霸真可怕！</p>
</blockquote>
<p>回想年后这几个月自己的转变还真是蛮大的，从以前放荡自由的野孩子，变得像点研究僧的模样，而且越来越喜欢这样自律的自己。<br>研一的周末，要么在研究烘焙，要么参加户外活动，认识了很多有意思的朋友，相比较单调的校园生活，可以说外面的世界太精彩，一次次被惊艳，看着他们活出自己理想生活状态，可以说是非常羡慕。然而渐渐我发现，如果自己不够优秀，认识再多的人，你的生活也不会有所改变。现在的我，学着跟自己和平相处，控制情绪，学着量化每天的时间，每天有小目标，生活得很充实。</p>
<h3 id="碎碎念2"><a href="#碎碎念2" class="headerlink" title="碎碎念2"></a><center>碎碎念2</center></h3><p>现在的我，每天不管多忙，多晚，也要读几页书，读书，可以说是一种自嗨的事儿，一旦这样的自嗨能力养成了，你就不容易陷入无聊，不容易被别人的想法左右。<br>人只能活一次，没有可以对标的人生，上一辈的建议有时过气，同龄人其实也一样迷茫，而读书就不一样了，只要你尝试阅读不同的书，你就可以尝试不同的人生，体验不同的精彩生活，重回现实，你会活的更明白。<br>平淡的日子，不急不躁，继续丰盈自己~</p>
<p><img src="/images/Shawshank_Redemption.jpg" alt="肖申克的救赎" title="The Shawshank Redemption"></p>
]]></content>
    
    <summary type="html">
    
      &lt;blockquote class=&quot;blockquote-center&quot;&gt;&lt;p&gt;有人说，人生有两大遗憾，太晚谈恋爱，太早读经典。读懂一本书，看懂一部电影和谈对一场恋爱，都需要刚刚好的时间。太早理解不了，太晚就少了份陪伴感。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="Thinking" scheme="http://www.phoebepan.cn/categories/Thinking/"/>
    
    
      <category term="Life" scheme="http://www.phoebepan.cn/tags/Life/"/>
    
      <category term="Thinking" scheme="http://www.phoebepan.cn/tags/Thinking/"/>
    
  </entry>
  
  <entry>
    <title>Modern Machine Learning Algorithms:Strengths and Weaknesses(阅读笔记)</title>
    <link href="http://www.phoebepan.cn/2017/05/13/ML_good_weak/"/>
    <id>http://www.phoebepan.cn/2017/05/13/ML_good_weak/</id>
    <published>2017-05-13T07:30:16.000Z</published>
    <updated>2017-06-02T14:34:39.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote class="blockquote-center"><p><center><strong>推荐理由</strong></center><br>对于机器学习算法的盘点，网上有很多。但本文亮点在于结合使用场景来把问题说明白，作者结合他的实际经验，细致剖析每种算法在实践中的优势和不足。</p>
</blockquote>
<a id="more"></a>
<p>当前的「三大」最常见的机器学习任务：</p>
<ul>
<li>回归（Regression）</li>
<li>分类（Classification）</li>
<li>聚类（Clustering）</li>
</ul>
<p>下面是我阅读这篇文章后总结的的思维导图：<br><img src="/images/whole.png" alt="机器学习3大任务" title="机器学习3大任务"><br><img src="/images/regression1.png" alt="回归"><br><img src="/images/regression2.png" alt="回归" title="回归"><br><img src="/images/classfication1.png" alt="分类"><br><img src="/images/classfication2.png" alt="分类" title="分类"><br><img src="/images/cocluster1.png" alt="聚类"><br><img src="/images/cocluster2.png" alt="聚类" title="聚类"><br>最后，<a href="https://elitedatascience.com/machine-learning-algorithms" target="_blank" rel="external">附原文链接</a>，欢迎纠错。<br><blockquote class="blockquote-center"><p>Of course, the algorithms you try must be appropriate for your problem, which is where picking the right machine learning task comes in.</p>
</blockquote></p>
]]></content>
    
    <summary type="html">
    
      &lt;blockquote class=&quot;blockquote-center&quot;&gt;&lt;p&gt;&lt;center&gt;&lt;strong&gt;推荐理由&lt;/strong&gt;&lt;/center&gt;&lt;br&gt;对于机器学习算法的盘点，网上有很多。但本文亮点在于结合使用场景来把问题说明白，作者结合他的实际经验，细致剖析每种算法在实践中的优势和不足。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="http://www.phoebepan.cn/categories/Machine-Learning/"/>
    
    
      <category term="ML" scheme="http://www.phoebepan.cn/tags/ML/"/>
    
      <category term="Reading" scheme="http://www.phoebepan.cn/tags/Reading/"/>
    
  </entry>
  
  <entry>
    <title>Pandas杂碎</title>
    <link href="http://www.phoebepan.cn/2017/05/12/pandas/"/>
    <id>http://www.phoebepan.cn/2017/05/12/pandas/</id>
    <published>2017-05-12T07:30:16.000Z</published>
    <updated>2017-05-29T05:14:14.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote class="blockquote-center"><p>本文主要是平时运用Pandas的小积累，涉及到读写文件，格式转换、缺失值填充，简单统计等多个方面。</p>
</blockquote>
<a id="more"></a>
<h3 id="读写csv文件到DataFrame"><a href="#读写csv文件到DataFrame" class="headerlink" title="读写csv文件到DataFrame"></a>读写csv文件到DataFrame</h3><p><code>df=pd.read_csv(&#39;filename&#39;,encoding=&#39;utf-8&#39;)</code><br>读入csv文本，编码为utf-8<br><code>df.to_csv(&#39;filename&#39;,sep=&#39;\t&#39;,index=False)</code><br>‘\t’切分df，写入csv，不包含行索引</p>
<h3 id="格式转换、值填充、删除"><a href="#格式转换、值填充、删除" class="headerlink" title="格式转换、值填充、删除"></a>格式转换、值填充、删除</h3><p> <code>df.to_dict(outtype=&#39;dict&#39;)</code><br>将DataFrame转换成其他结构类型，outtype的参数为‘dict’、‘list’、‘series’和‘records’<br> <code>df[&#39;A&#39;].astype(float)</code><br>将DataFrame的A列类型更改成float<br><code>df.rename(columns={&#39;oldname&#39;: &#39;newname&#39;}, inplace=True)</code>   #修改列名<br><code>df.fillna(0)</code>  #NaN用0填充，处理缺失值<br><code>df.drop_duplicates(subset=[&#39;A&#39;]，keep=&#39;first&#39;)</code><br>按A列去重，保留第一行，keep（’first’,’last’,False）</p>
<h3 id="查看DataFrame概要"><a href="#查看DataFrame概要" class="headerlink" title="查看DataFrame概要"></a>查看DataFrame概要</h3><p><code>df.head()</code>     #前几行<br><code>df.tail()</code>     #末几行<br><code>df.index</code>      #行标签<br><code>df.columns</code>    #列标签<br><code>df.dtypes</code>     #每一列数据类型<br><code>df[&#39;column&#39;].count()</code>    #column列非空条数<br><code>df[&#39;column&#39;].isnull()</code>   #column列是否有NaN的数据<br><code>df[&#39;column&#39;].unique()</code>    #column列所有值</p>
<h3 id="简单统计"><a href="#简单统计" class="headerlink" title="简单统计"></a>简单统计</h3><p><code>df.describe()</code>   #各列基本描述统计值<br><code>df[&#39;A&#39;].value_counts()</code>  #计算A列每个值的频率</p>
<h3 id="排序"><a href="#排序" class="headerlink" title="排序"></a>排序</h3><p><code>df.sort_index(axis=1,ascending=False)</code><br>对DataFrame的axis=0(行)，1(列)索引排序（ascending=True(升)，False(降)）<br><code>df.sort_index(by=[&#39;A&#39;, &#39;B&#39;], ascending=[True, False])</code>&lt;==&gt;<code>df.sort_values(by=[&#39;A&#39;, &#39;B&#39;], ascending=[True, False])</code>&lt;==&gt;<code>df.sort(columns=[&#39;A&#39;,&#39;B&#39;],ascending=[1,0])</code><br>对DataFrame先按’A’升排序, 再按’B’降排序</p>
<h3 id="分组"><a href="#分组" class="headerlink" title="分组"></a>分组</h3><p><code>df.groupby(subset=[&#39;A&#39;,&#39;B&#39;],as_index=True)</code>    #以列A，B对df进行分组，默认A，B作为分组索引<br><code>df.groupby(by=[&#39;A&#39;],as_index=True).get_group(&#39;a&#39;)</code> #按A列分组后获取’a’组<br><code>df.groupby([&#39;col1&#39;,&#39;col2&#39;]).size().to_frame(name =&#39;count&#39;)</code><br>&lt;==&gt;<code>df.groupby([&#39;col1&#39;,&#39;col2&#39;]).size().reset_index(name =&#39;count&#39;)</code><br>按col1,col2对df进行分组，<code>size()</code>统计每一分组成员个数(<code>count()</code>也可以，<a href="https://stackoverflow.com/documentation/pandas/1822/grouping-data/6874/aggregating-by-size-and-count#t=201607220906502658034" target="_blank" rel="external">区别</a>就是count()过滤了NaN，size()包含)，命名为’count’</p>
<h3 id="选择DataFrame行列"><a href="#选择DataFrame行列" class="headerlink" title="选择DataFrame行列"></a>选择DataFrame行列</h3><p><a href="http://www.shanelynn.ie/select-pandas-dataframe-rows-and-columns-using-iloc-loc-and-ix/" target="_blank" rel="external">示例参考</a><br><code>df.iloc[&lt;row selection&gt;,&lt;col selection&gt;]</code>  #基于行列索引选择（Selecting rows by label/index）<br><code>df.loc[&lt;row selection&gt;,&lt;col selection&gt;]</code>   #基于条件选择（Selecting rows with a boolean / conditional lookup）<br><code>ix[]</code>  #索引，条件混合选择</p>
<p><code>df[df[&#39;A&#39;].isin(li)]</code>  #选择A列值在列表li中的行<br><code>df.saple(n=3)</code> #随机抽取3行数据，</p>
<h3 id="归一化处理"><a href="#归一化处理" class="headerlink" title="归一化处理"></a>归一化处理</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">from sklearn import preprocessing</div><div class="line">df=preprocessing.scale(df,axis=0)   #df每一个feature归一化(列)</div><div class="line">df=preprocessing.scale(df,axis=1)   #df每一个sample归一化(行)</div></pre></td></tr></table></figure>
<h3 id="离散特征one-hot编码"><a href="#离散特征one-hot编码" class="headerlink" title="离散特征one-hot编码"></a>离散特征one-hot编码</h3><p><code>df = pd.get_dummies(df[&#39;A&#39;], prefix=&#39;A&#39;)</code>  #对A列重新编码，编码后的列前缀加上’A’<br><code>df = pd.get_dummies(df)</code>   #对df所有离散特征进行one-hot编码</p>
<h3 id="合并——merge、concat、append"><a href="#合并——merge、concat、append" class="headerlink" title="合并——merge、concat、append"></a>合并——merge、concat、append</h3><p><code>df1.append(df2,ignore_index=True)</code> #在df1下面追加df2，行索引重排序<br><code>pd.concat([df1[&#39;A&#39;],df2],axis=1)</code>  #列方向合并<br><code>pd.merge(df1, df2, how=&#39;inner&#39;, on=None, left_on=None, right_on=None,left_index=False, right_index=False)</code><br>how=[‘inner’,’left’,’right’,’outer’]对应内，左，右，外连接,on指定连接key</p>
]]></content>
    
    <summary type="html">
    
      &lt;blockquote class=&quot;blockquote-center&quot;&gt;&lt;p&gt;本文主要是平时运用Pandas的小积累，涉及到读写文件，格式转换、缺失值填充，简单统计等多个方面。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="Python" scheme="http://www.phoebepan.cn/categories/Python/"/>
    
    
      <category term="Python" scheme="http://www.phoebepan.cn/tags/Python/"/>
    
      <category term="pandas" scheme="http://www.phoebepan.cn/tags/pandas/"/>
    
  </entry>
  
  <entry>
    <title>正则表达式</title>
    <link href="http://www.phoebepan.cn/2017/05/06/Regular%20Expression/"/>
    <id>http://www.phoebepan.cn/2017/05/06/Regular Expression/</id>
    <published>2017-05-06T07:30:16.000Z</published>
    <updated>2017-07-27T15:17:38.000Z</updated>
    
    <content type="html"><![CDATA[<h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p>本文主要是shell正则表达式相关的笔记，正则表达式的分类有，</p>
<ul>
<li>基本的正则表达式（Basic Regular Expression 又叫Basic RegEx 简称BREs）</li>
<li>扩展的正则表达式（Extended Regular Expression 又叫Extended RegEx 简称EREs）</li>
<li>Perl的正则表达式（Perl Regular Expression 又叫Perl RegEx 简称PREs）</li>
</ul>
<a id="more"></a>
<h3 id="基本组成部分"><a href="#基本组成部分" class="headerlink" title="基本组成部分"></a>基本组成部分</h3><table>
<thead>
<tr>
<th style="text-align:center">正则表达式</th>
<th style="text-align:left">示例</th>
<th style="text-align:center">BREs</th>
<th style="text-align:center">EREs</th>
<th style="text-align:center">Python RegEx</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">\</td>
<td style="text-align:left">转义符，如a\.b匹配a.b</td>
<td style="text-align:center">\</td>
<td style="text-align:center">\</td>
<td style="text-align:center">\</td>
</tr>
<tr>
<td style="text-align:center">^</td>
<td style="text-align:left">行首，如^tux匹配以tux开头的行</td>
<td style="text-align:center">^</td>
<td style="text-align:center">^</td>
<td style="text-align:center">^</td>
</tr>
<tr>
<td style="text-align:center">\$</td>
<td style="text-align:left">行尾，如tux\$匹配以tux结尾的行</td>
<td style="text-align:center">\$</td>
<td style="text-align:center">\$</td>
<td style="text-align:center">\$</td>
</tr>
<tr>
<td style="text-align:center">.</td>
<td style="text-align:left">匹配除换行符\n之外的任意单个字符</td>
<td style="text-align:center">.</td>
<td style="text-align:center">.</td>
<td style="text-align:center">.</td>
</tr>
<tr>
<td style="text-align:center">[]</td>
<td style="text-align:left">匹配包含在[字符]之中的任意一个字符</td>
<td style="text-align:center">[]</td>
<td style="text-align:center">[]</td>
<td style="text-align:center">[]</td>
</tr>
<tr>
<td style="text-align:center">[^]</td>
<td style="text-align:left">匹配[^字符]之外的任意一个字符，如123[^45]不能匹配到1234或1235</td>
<td style="text-align:center">[^]</td>
<td style="text-align:center">[^]</td>
<td style="text-align:center">[^]</td>
</tr>
<tr>
<td style="text-align:center">[-]</td>
<td style="text-align:left">匹配[]中指定范围内(递增)的任意一个字符，如[0-9]可以匹配0到9之间任意数</td>
<td style="text-align:center">[-]</td>
<td style="text-align:center">[-]</td>
<td style="text-align:center">[-]</td>
</tr>
<tr>
<td style="text-align:center">?</td>
<td style="text-align:left">匹配之前的项至多1次，如colou?r可以匹配color或者colour</td>
<td style="text-align:center">不支持</td>
<td style="text-align:center">?</td>
<td style="text-align:center">?</td>
</tr>
<tr>
<td style="text-align:center">+</td>
<td style="text-align:left">匹配之前的项至少1次，sa-6+匹配sa-6、sa-666</td>
<td style="text-align:center">不支持</td>
<td style="text-align:center">+</td>
<td style="text-align:center">+</td>
</tr>
<tr>
<td style="text-align:center">*</td>
<td style="text-align:left">匹配之前项0次或多次</td>
<td style="text-align:center">*</td>
<td style="text-align:center">*</td>
<td style="text-align:center">*</td>
</tr>
<tr>
<td style="text-align:center">{n}</td>
<td style="text-align:left">匹配之前项n次</td>
<td style="text-align:center">不支持</td>
<td style="text-align:center">{n}</td>
<td style="text-align:center">{n}</td>
</tr>
<tr>
<td style="text-align:center">{n,}</td>
<td style="text-align:left">之前项至少匹配n次</td>
<td style="text-align:center">不支持</td>
<td style="text-align:center">{n,}</td>
<td style="text-align:center">{n,}</td>
</tr>
<tr>
<td style="text-align:center">{n,m}</td>
<td style="text-align:left">之前项至少匹配n次，最多匹配m次，n&lt;=m</td>
<td style="text-align:center">不支持</td>
<td style="text-align:center">{n,m}</td>
<td style="text-align:center">{n,m}</td>
</tr>
<tr>
<td style="text-align:center">()</td>
<td style="text-align:left">创建一个匹配子串，如ma(tri)?匹配max或maxtrix</td>
<td style="text-align:center">不支持</td>
<td style="text-align:center">()</td>
<td style="text-align:center">()</td>
</tr>
</tbody>
</table>
<h3 id="元字符"><a href="#元字符" class="headerlink" title="元字符"></a>元字符</h3><table>
<thead>
<tr>
<th style="text-align:center">正则表达式</th>
<th style="text-align:left">示例</th>
<th style="text-align:center">BREs</th>
<th style="text-align:center">EREs</th>
<th style="text-align:center">Python RegEx</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">\b</td>
<td style="text-align:left">单词边界，如\bcool\b 匹配cool</td>
<td style="text-align:center">\b</td>
<td style="text-align:center">\b</td>
<td style="text-align:center">\b</td>
</tr>
<tr>
<td style="text-align:center">\B</td>
<td style="text-align:left">非单词边界，如cool\B 匹配coolant，不匹配cool</td>
<td style="text-align:center">\B</td>
<td style="text-align:center">\B</td>
<td style="text-align:center">\B</td>
</tr>
<tr>
<td style="text-align:center">\d</td>
<td style="text-align:left">单个数字字符，如b\db 匹配b2b，不匹配bcb</td>
<td style="text-align:center">不支持</td>
<td style="text-align:center">不支持</td>
<td style="text-align:center">\d</td>
</tr>
<tr>
<td style="text-align:center">\D</td>
<td style="text-align:left">单个非数字字符，如b\Db 匹配bcb，不匹配b2b</td>
<td style="text-align:center">不支持</td>
<td style="text-align:center">不支持</td>
<td style="text-align:center">\D</td>
</tr>
<tr>
<td style="text-align:center">\w</td>
<td style="text-align:left">单个单词字符</td>
<td style="text-align:center">\w</td>
<td style="text-align:center">\w</td>
<td style="text-align:center">\w</td>
</tr>
<tr>
<td style="text-align:center">\W</td>
<td style="text-align:left">单个非单词字符</td>
<td style="text-align:center">\W</td>
<td style="text-align:center">\W</td>
<td style="text-align:center">\W</td>
</tr>
<tr>
<td style="text-align:center">\s</td>
<td style="text-align:left">单个空白字符</td>
<td style="text-align:center">不支持</td>
<td style="text-align:center">不支持</td>
<td style="text-align:center">\s</td>
</tr>
<tr>
<td style="text-align:center">\S</td>
<td style="text-align:left">单个非空白字符</td>
<td style="text-align:center">不支持</td>
<td style="text-align:center">不支持</td>
<td style="text-align:center">\S</td>
</tr>
</tbody>
</table>
<p><a href="http://man.linuxde.net/docs/shell_regex.html" target="_blank" rel="external">更多参考访问</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;简介&quot;&gt;&lt;a href=&quot;#简介&quot; class=&quot;headerlink&quot; title=&quot;简介&quot;&gt;&lt;/a&gt;简介&lt;/h3&gt;&lt;p&gt;本文主要是shell正则表达式相关的笔记，正则表达式的分类有，&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;基本的正则表达式（Basic Regular Expression 又叫Basic RegEx 简称BREs）&lt;/li&gt;
&lt;li&gt;扩展的正则表达式（Extended Regular Expression 又叫Extended RegEx 简称EREs）&lt;/li&gt;
&lt;li&gt;Perl的正则表达式（Perl Regular Expression 又叫Perl RegEx 简称PREs）&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="Daily Accumulation" scheme="http://www.phoebepan.cn/categories/Daily-Accumulation/"/>
    
    
      <category term="Regular Expression" scheme="http://www.phoebepan.cn/tags/Regular-Expression/"/>
    
  </entry>
  
  <entry>
    <title>如何决定K-Means聚类个数——silhouette analysis</title>
    <link href="http://www.phoebepan.cn/2017/05/05/silhouette%20analysis/"/>
    <id>http://www.phoebepan.cn/2017/05/05/silhouette analysis/</id>
    <published>2017-05-05T07:30:16.000Z</published>
    <updated>2017-06-12T23:44:18.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote class="blockquote-center"><h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p>在K-Means聚类时，我们常常会纠结，K该取多大呢？今天无意当中查看Sklearn时，发现了<code>silhouette analysis</code>，翻译过来，就是轮廓分析，具体来说，就是通过结果簇之间的分隔距离来辅助决定K的取值。偷个懒，直接参照手册，学习下。</p>
</blockquote>
<a id="more"></a>
<h3 id="数据集分布"><a href="#数据集分布" class="headerlink" title="数据集分布"></a>数据集分布</h3><p>下图是500个样本含有2个feature的数据分布情况：<br><img src="/images/scatter.png" alt="scatter"></p>
<h3 id="轮廓系数"><a href="#轮廓系数" class="headerlink" title="轮廓系数"></a>轮廓系数</h3><p>接下来看下，n_clusters 分别为 2，3，4，5，6时，平均的轮廓分值（结果簇之间平均的分隔距离）如下，这个轮廓分值是介于[-1,1]之间的度量指标。每次聚类后，每个样本都会得到一个轮廓系数，当它为1时，说明这个点与周围簇距离较远，结果非常好，当它为0，说明这个点可能处在两个簇的边界上，当值为负时，暗含该点可能被误分了。<br>从平均轮廓分值结果来看，K取3，5，6是不明智的。对于2,4的选择还是有点纠结的，因为它们值相差并不大。</p>
<blockquote>
<p>For n_clusters = 2 The average silhouette_score is : 0.704978749608<br>For n_clusters = 3 The average silhouette_score is : 0.588200401213<br>For n_clusters = 4 The average silhouette_score is : 0.650518663273<br>For n_clusters = 5 The average silhouette_score is : 0.563764690262<br>For n_clusters = 6 The average silhouette_score is : 0.450466629437</p>
</blockquote>
<h3 id="轮廓宽度"><a href="#轮廓宽度" class="headerlink" title="轮廓宽度"></a>轮廓宽度</h3><p>下面是簇为2,3,4,5,6相对应的轮廓图，左图可以看出，当n_clusters = 2时，第0簇的宽度远宽于第1簇，但n_clusters = 4时，所聚的簇宽度相差不大，这样分析下来，自然而然会选择K=4，作为最终聚类个数。<br><img src="/images/figure_1.png" alt="figure1"><br><img src="/images/figure_2.png" alt="figure2"><br><img src="/images/figure_3.png" alt="figure3"><br><img src="/images/figure_4.png" alt="figure4"><br><img src="/images/figure_5.png" alt="figure5"></p>
<p>本例参考官方手册，<a href="http://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html#sphx-glr-auto-examples-cluster-plot-kmeans-silhouette-analysis-py" target="_blank" rel="external">详情戳这</a></p>
<h3 id="附脚本"><a href="#附脚本" class="headerlink" title="附脚本"></a>附脚本</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_blobs</div><div class="line"><span class="keyword">from</span> sklearn.cluster <span class="keyword">import</span> KMeans</div><div class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> silhouette_samples, silhouette_score</div><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line"><span class="keyword">import</span> matplotlib.cm <span class="keyword">as</span> cm</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="comment"># Generating the sample data from make_blobs</span></div><div class="line"><span class="comment"># This particular setting has one distinct cluster and 3 clusters placed close</span></div><div class="line"><span class="comment"># together.</span></div><div class="line">X, y = make_blobs(n_samples=<span class="number">500</span>,</div><div class="line">                  n_features=<span class="number">2</span>,</div><div class="line">                  centers=<span class="number">4</span>,</div><div class="line">                  cluster_std=<span class="number">1</span>,</div><div class="line">                  center_box=(<span class="number">-10.0</span>, <span class="number">10.0</span>),</div><div class="line">                  shuffle=<span class="keyword">True</span>,</div><div class="line">                  random_state=<span class="number">1</span>)  <span class="comment"># For reproducibility</span></div><div class="line"></div><div class="line">range_n_clusters = [<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]</div><div class="line"></div><div class="line"><span class="keyword">for</span> n_clusters <span class="keyword">in</span> range_n_clusters:</div><div class="line">    <span class="comment"># Create a subplot with 1 row and 2 columns</span></div><div class="line">    fig, (ax1, ax2) = plt.subplots(<span class="number">1</span>, <span class="number">2</span>)</div><div class="line">    fig.set_size_inches(<span class="number">18</span>, <span class="number">7</span>)</div><div class="line"></div><div class="line">    <span class="comment"># The 1st subplot is the silhouette plot</span></div><div class="line">    <span class="comment"># The silhouette coefficient can range from -1, 1 but in this example all</span></div><div class="line">    <span class="comment"># lie within [-0.1, 1]</span></div><div class="line">    ax1.set_xlim([<span class="number">-0.1</span>, <span class="number">1</span>])</div><div class="line">    <span class="comment"># The (n_clusters+1)*10 is for inserting blank space between silhouette</span></div><div class="line">    <span class="comment"># plots of individual clusters, to demarcate them clearly.</span></div><div class="line">    ax1.set_ylim([<span class="number">0</span>, len(X) + (n_clusters + <span class="number">1</span>) * <span class="number">10</span>])</div><div class="line"></div><div class="line">    <span class="comment"># Initialize the clusterer with n_clusters value and a random generator</span></div><div class="line">    <span class="comment"># seed of 10 for reproducibility.</span></div><div class="line">    clusterer = KMeans(n_clusters=n_clusters, random_state=<span class="number">10</span>)</div><div class="line">    cluster_labels = clusterer.fit_predict(X)</div><div class="line"></div><div class="line">    <span class="comment"># The silhouette_score gives the average value for all the samples.</span></div><div class="line">    <span class="comment"># This gives a perspective into the density and separation of the formed</span></div><div class="line">    <span class="comment"># clusters</span></div><div class="line">    silhouette_avg = silhouette_score(X, cluster_labels)</div><div class="line">    print(<span class="string">"For n_clusters ="</span>, n_clusters,</div><div class="line">          <span class="string">"The average silhouette_score is :"</span>, silhouette_avg)</div><div class="line"></div><div class="line">    <span class="comment"># Compute the silhouette scores for each sample</span></div><div class="line">    sample_silhouette_values = silhouette_samples(X, cluster_labels)</div><div class="line"></div><div class="line">    y_lower = <span class="number">10</span></div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(n_clusters):</div><div class="line">        <span class="comment"># Aggregate the silhouette scores for samples belonging to</span></div><div class="line">        <span class="comment"># cluster i, and sort them</span></div><div class="line">        ith_cluster_silhouette_values = \</div><div class="line">            sample_silhouette_values[cluster_labels == i]</div><div class="line"></div><div class="line">        ith_cluster_silhouette_values.sort()</div><div class="line"></div><div class="line">        size_cluster_i = ith_cluster_silhouette_values.shape[<span class="number">0</span>]</div><div class="line">        y_upper = y_lower + size_cluster_i</div><div class="line"></div><div class="line">        color = cm.spectral(float(i) / n_clusters)</div><div class="line">        ax1.fill_betweenx(np.arange(y_lower, y_upper),</div><div class="line">                          <span class="number">0</span>, ith_cluster_silhouette_values,</div><div class="line">                          facecolor=color, edgecolor=color, alpha=<span class="number">0.7</span>)</div><div class="line"></div><div class="line">        <span class="comment"># Label the silhouette plots with their cluster numbers at the middle</span></div><div class="line">        ax1.text(<span class="number">-0.05</span>, y_lower + <span class="number">0.5</span> * size_cluster_i, str(i))</div><div class="line"></div><div class="line">        <span class="comment"># Compute the new y_lower for next plot</span></div><div class="line">        y_lower = y_upper + <span class="number">10</span>  <span class="comment"># 10 for the 0 samples</span></div><div class="line"></div><div class="line">    ax1.set_title(<span class="string">"The silhouette plot for the various clusters."</span>)</div><div class="line">    ax1.set_xlabel(<span class="string">"The silhouette coefficient values"</span>)</div><div class="line">    ax1.set_ylabel(<span class="string">"Cluster label"</span>)</div><div class="line"></div><div class="line">    <span class="comment"># The vertical line for average silhouette score of all the values</span></div><div class="line">    ax1.axvline(x=silhouette_avg, color=<span class="string">"red"</span>, linestyle=<span class="string">"--"</span>)</div><div class="line"></div><div class="line">    ax1.set_yticks([])  <span class="comment"># Clear the yaxis labels / ticks</span></div><div class="line">    ax1.set_xticks([<span class="number">-0.1</span>, <span class="number">0</span>, <span class="number">0.2</span>, <span class="number">0.4</span>, <span class="number">0.6</span>, <span class="number">0.8</span>, <span class="number">1</span>])</div><div class="line"></div><div class="line">    <span class="comment"># 2nd Plot showing the actual clusters formed</span></div><div class="line">    colors = cm.spectral(cluster_labels.astype(float) / n_clusters)</div><div class="line">    ax2.scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], marker=<span class="string">'.'</span>, s=<span class="number">30</span>, lw=<span class="number">0</span>, alpha=<span class="number">0.7</span>,</div><div class="line">                c=colors)</div><div class="line"></div><div class="line">    <span class="comment"># Labeling the clusters</span></div><div class="line">    centers = clusterer.cluster_centers_</div><div class="line">    <span class="comment"># Draw white circles at cluster centers</span></div><div class="line">    ax2.scatter(centers[:, <span class="number">0</span>], centers[:, <span class="number">1</span>],</div><div class="line">                marker=<span class="string">'o'</span>, c=<span class="string">"white"</span>, alpha=<span class="number">1</span>, s=<span class="number">200</span>)</div><div class="line"></div><div class="line">    <span class="keyword">for</span> i, c <span class="keyword">in</span> enumerate(centers):</div><div class="line">        ax2.scatter(c[<span class="number">0</span>], c[<span class="number">1</span>], marker=<span class="string">'$%d$'</span> % i, alpha=<span class="number">1</span>, s=<span class="number">50</span>)</div><div class="line"></div><div class="line">    ax2.set_title(<span class="string">"The visualization of the clustered data."</span>)</div><div class="line">    ax2.set_xlabel(<span class="string">"Feature space for the 1st feature"</span>)</div><div class="line">    ax2.set_ylabel(<span class="string">"Feature space for the 2nd feature"</span>)</div><div class="line"></div><div class="line">    plt.suptitle((<span class="string">"Silhouette analysis for KMeans clustering on sample data "</span></div><div class="line">                  <span class="string">"with n_clusters = %d"</span> % n_clusters),</div><div class="line">                 fontsize=<span class="number">14</span>, fontweight=<span class="string">'bold'</span>)</div><div class="line"></div><div class="line">    plt.show()</div></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;blockquote class=&quot;blockquote-center&quot;&gt;&lt;h3 id=&quot;简介&quot;&gt;&lt;a href=&quot;#简介&quot; class=&quot;headerlink&quot; title=&quot;简介&quot;&gt;&lt;/a&gt;简介&lt;/h3&gt;&lt;p&gt;在K-Means聚类时，我们常常会纠结，K该取多大呢？今天无意当中查看Sklearn时，发现了&lt;code&gt;silhouette analysis&lt;/code&gt;，翻译过来，就是轮廓分析，具体来说，就是通过结果簇之间的分隔距离来辅助决定K的取值。偷个懒，直接参照手册，学习下。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="http://www.phoebepan.cn/categories/Machine-Learning/"/>
    
    
      <category term="ML" scheme="http://www.phoebepan.cn/tags/ML/"/>
    
  </entry>
  
  <entry>
    <title>支持向量机</title>
    <link href="http://www.phoebepan.cn/2017/05/04/SVM/"/>
    <id>http://www.phoebepan.cn/2017/05/04/SVM/</id>
    <published>2017-05-04T07:30:16.000Z</published>
    <updated>2017-07-28T00:53:52.000Z</updated>
    
    <content type="html"><![CDATA[<p>这篇笔记主要总结一些关于支持向量机的知识。<br><blockquote class="blockquote-center"><h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><ol>
<li>二分类模型，定义在<strong>特征空间</strong>上的间隔最大的线性分类器，加入核技巧，成为非线性分类器。</li>
<li>学习目标——特征空间找到一个分离超平面，能正确划分训练集，并且几何间隔最大化。</li>
</ol>
</blockquote><br><a id="more"></a></p>
<h3 id="横向比较"><a href="#横向比较" class="headerlink" title="横向比较"></a>横向比较</h3><ul>
<li>感知机（误分类最小，无穷多解）</li>
<li>SVM——间隔最大化（凸二次规划最优化问题，唯一解），直观解释，即，不仅将正负实例点分开，而且对最难分的实例点（离超平面最近的点）也有足够大的确信度将它们分开。</li>
</ul>
<h3 id="3种情况"><a href="#3种情况" class="headerlink" title="3种情况"></a>3种情况</h3><blockquote>
<p>训练集线性可分 -&gt; hard margin maximization -&gt; 硬间隔SVM；</p>
<p>训练集近似线性可分 -&gt; soft margin maximization -&gt; 软间隔SVM；</p>
<p>训练集线性不可分 -&gt; kernel trick/soft margin maximization -&gt; 非线性SVM。</p>
</blockquote>
<p><strong>数据集的线性可分性</strong>——给定一个数据集T，如果存在某个超平面S(w·x+b=0)能够将数据集的正负样本完全正确的划分到超平面两侧，则T是线性可分数据集。</p>
<p><strong>函数间隔</strong>可以表示分类预测的准确性及确信度。min(y<sub>i</sub>(wx<sub>i</sub>+b))—&lt;规范化||w||=1&gt;—&gt; <strong>几何间隔</strong></p>
<p><strong>最大间隔超平面</strong>，最大化超平面（w,b）关于训练集的几何间隔，几何间隔最小的样本为支持向量。</p>
<h3 id="线性可分SVM"><a href="#线性可分SVM" class="headerlink" title="线性可分SVM"></a>线性可分SVM</h3><p>线性可分SVM对应着将两类数据正确划分并且间隔最大的直线，等价于求解对应的凸二次规划问题。如下图所示线性可分训练集，<br><img src="/images/line.png" alt="line" title="线性可分SVM"><br><img src="/images/max_margin.png" alt="max_margin"></p>
<p><strong>在决定分离超平面时只有支持向量起作用，其他实例点并不起作用。</strong>支持向量的个数一般很少，所以支持向量机由很少的“重要的”训练样本确定。</p>
<h4 id="学习的对偶算法"><a href="#学习的对偶算法" class="headerlink" title="学习的对偶算法"></a>学习的对偶算法</h4><p>关于凸优化对偶，（<a href="http://phoebepan.cn/2017/05/01/convex/" target="_blank" rel="external">参考我的博文</a>）。针对上面的最优化问题的对偶问题是拉格朗日极大极小问题，$$\underset{\alpha }{max} \underset{w,b}{min}L(w,b,\alpha )$$，为得到对偶问题的解，需要先求$L(w,b,\alpha)$对w,b的极小，再求对$\alpha$的极大。</p>
<p><img src="/images/larg_learn.png" alt="larg_learn"></p>
<p>训练集中对应于$\alpha _{i}&gt;0$的样本称为支持向量。</p>
<h3 id="线性SVM"><a href="#线性SVM" class="headerlink" title="线性SVM"></a>线性SVM</h3><p>当训练集近似线性可分，如下图，即，训练集中有一些特异点，将这些特异点排除后，剩下大部分的样本点组成集合是线性可分的(也就是存在某些不符合函数间隔大于等于1的样本点)。<img src="/images/soft_margin.png" alt="soft_margin" title="近似线性可分"><br>解决这一问题，可以对每个样本点引进一个松弛变量 a<sub>i</sub>(大于等于0) ，使得函数间隔加上松弛变量大于等于1，满足线性可分的约束条件，同时，对每个松弛变量a<sub>i</sub>，支付一个代价a<sub>i</sub>，<strong>目标函数</strong>由原来的$ \frac{1}{2} \left | W \right |^2 $变成<img src="/images/soft_f.png" alt="soft_f">，这里<code>C&gt;0</code>称为<strong>惩罚参数</strong>，最小化目标函数就是，使$ \frac{1}{2} \left | W \right |^2 $尽量小（间隔尽量大），同时是误分类点的个数尽量小，C是调和二者的系数。这就是所谓的<strong>软间隔最大化</strong>。<br>同样可以通过求解对偶问题的解得到原始问题的最优解。</p>
<h4 id="合页损失函数"><a href="#合页损失函数" class="headerlink" title="合页损失函数"></a>合页损失函数</h4><p>线性SVM原始最优化问题：<br><img src="/images/heye.png" alt="heye"></p>
<p>等价于最优化问题<br><img src="/images/best_heye.png" alt="best_heye"></p>
<p>，上式的第1项是经验损失，称为合页损失函数，下标表示取正值的函数，第2项是正则化项。</p>
<h3 id="非线性SVM"><a href="#非线性SVM" class="headerlink" title="非线性SVM"></a>非线性SVM</h3><p>SVM是线性分类器，但训练集线性不可分时，如下图情况，该如何学习呢？<br><img src="/images/notline.png" alt="notline" title="线性不可分"><br>此时，核技巧的引入，完美的解决了该问题。<strong>核技巧</strong>应用到SVM，基本想法就是通过非线性变换将输入空间（欧式空间或离散集合）对应于一个特征空间（希尔伯特空间），使得在输入空间的超曲面模型对应于特征空间的超平面模型。这样，线性不可分的分类问题的学习任务通过在特征空间中求解线性SVM就可以了。<br>核技巧，在学习与预测中只定义<a href="https://en.wikipedia.org/wiki/Kernel_method" target="_blank" rel="external">核函数</a>，而不显式地定义映射函数，避免了“维度灾难”。<br>线性SVM的对偶问题中，目标函数，决策函数都只涉及输入样本与样本间的内积，可以自然引入核函数，学习到含有核函数的非线性SVM。<br>常用的核函数有，多项式核函数，高斯核函数，线性核函数等。</p>
<h3 id="实例"><a href="#实例" class="headerlink" title="实例"></a>实例</h3><p>未完待续……</p>
<p>参考文献：<br>[1].《统计学习方法》，李航著</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这篇笔记主要总结一些关于支持向量机的知识。&lt;br&gt;&lt;blockquote class=&quot;blockquote-center&quot;&gt;&lt;h3 id=&quot;简介&quot;&gt;&lt;a href=&quot;#简介&quot; class=&quot;headerlink&quot; title=&quot;简介&quot;&gt;&lt;/a&gt;简介&lt;/h3&gt;&lt;ol&gt;
&lt;li&gt;二分类模型，定义在&lt;strong&gt;特征空间&lt;/strong&gt;上的间隔最大的线性分类器，加入核技巧，成为非线性分类器。&lt;/li&gt;
&lt;li&gt;学习目标——特征空间找到一个分离超平面，能正确划分训练集，并且几何间隔最大化。&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;&lt;br&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="http://www.phoebepan.cn/categories/Machine-Learning/"/>
    
    
      <category term="Mathematics in ML" scheme="http://www.phoebepan.cn/tags/Mathematics-in-ML/"/>
    
      <category term="Model" scheme="http://www.phoebepan.cn/tags/Model/"/>
    
  </entry>
  
  <entry>
    <title>凸优化</title>
    <link href="http://www.phoebepan.cn/2017/05/01/convex/"/>
    <id>http://www.phoebepan.cn/2017/05/01/convex/</id>
    <published>2017-05-01T07:30:16.000Z</published>
    <updated>2017-07-27T15:35:04.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote class="blockquote-center"><h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p>机器学习中常用到数学优化技巧，最常见的优化就属凸优化了，本文参考Stanford CS229 Machine Learning的教学资料：<a href="http://cs229.stanford.edu/section/cs229-cvxopt.pdf" target="_blank" rel="external">http://cs229.stanford.edu/section/cs229-cvxopt.pdf</a>，对凸优化问题的初步认识，以下是几个重要的概念笔记。</p>
</blockquote>
<a id="more"></a>
<h3 id="凸集"><a href="#凸集" class="headerlink" title="凸集"></a>凸集</h3><p><strong>几何意义</strong>为，集合C中任意两点间的线段仍然在C中，其示意图如下：<img src="/images/convex.png" alt="convex set"><br><strong>数学定义</strong>，对任意x<sub>1</sub>,x<sub>2</sub>$ \epsilon C$,$0 \leq \Theta \leq 1$，都有<img src="/images/convexf.png" alt="convexf"><br>常见的凸集有，n维实数空间；一些范数约束形式的集合；仿射子空间；凸集的交集；n维半正定矩阵集。</p>
<h3 id="凸函数"><a href="#凸函数" class="headerlink" title="凸函数"></a>凸函数</h3><p><strong>几何意义</strong>为，函数任意两点连线上的值大于对应自变量处的函数值，示例图如下：<img src="/images/convex_function.jpg" alt="convex_function"><br><strong>数学定义</strong>，如果函数定义域(dom f)是凸集，且对于任意$x,y\epsilon dom f$和任意$0\leq \Theta \leq 1$，有$$f(\Theta x+(1-\Theta )y)\leq \Theta f(x)+(1-\Theta )f(y).$$<br>常见的凸函数有，指数函数族；非负对数函数；仿射函数；二次函数；常见的范数函数；凸函数非负加权的和等</p>
<h3 id="凸优化问题"><a href="#凸优化问题" class="headerlink" title="凸优化问题"></a>凸优化问题</h3><p>研究定义于<code>凸集</code>中的<code>凸函数</code>最小化问题。它要求<strong>目标函数是凸函数</strong>，<strong>变量所属集合是凸集</strong>的优化问题。或者目标函数是凸函数，<strong>变量约束函数是凸函数（不等式），或仿射函数（等式约束）</strong><br><strong>数学定义</strong><br><img src="/images/convex_prob.jpg" alt="convex_prob"><br>常见的凸优化问题有，线性规划、二次规划、二次约束的二次规划、半正定规划。</p>
<h3 id="对偶"><a href="#对偶" class="headerlink" title="对偶"></a>对偶</h3><p>在约束最优化问题中，常常利用拉格朗日对偶性将原始问题转换成对偶问题，通过解对偶问题而得到原始问题的解。这样做的优点是对偶问题往往更容易求解。<br>Lagrange对偶的基本思想是在目标函数中考虑上图中的约束条件，也就是添加约束条件的加权和，得到增广的目标函数。定义上面问题的Lagrange函数<img src="/images/lagr.png" alt="lagr"></p>
<h3 id="附：仿射函数"><a href="#附：仿射函数" class="headerlink" title="附：仿射函数"></a>附：仿射函数</h3><p>如果f(x)=a·x+b，$a \epsilon R^{n},b\epsilon R,x \epsilon R^{n}$，则f(x)为仿射函数。</p>
]]></content>
    
    <summary type="html">
    
      &lt;blockquote class=&quot;blockquote-center&quot;&gt;&lt;h3 id=&quot;简介&quot;&gt;&lt;a href=&quot;#简介&quot; class=&quot;headerlink&quot; title=&quot;简介&quot;&gt;&lt;/a&gt;简介&lt;/h3&gt;&lt;p&gt;机器学习中常用到数学优化技巧，最常见的优化就属凸优化了，本文参考Stanford CS229 Machine Learning的教学资料：&lt;a href=&quot;http://cs229.stanford.edu/section/cs229-cvxopt.pdf&quot;&gt;http://cs229.stanford.edu/section/cs229-cvxopt.pdf&lt;/a&gt;，对凸优化问题的初步认识，以下是几个重要的概念笔记。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="Mathematics in ML" scheme="http://www.phoebepan.cn/categories/Mathematics-in-ML/"/>
    
    
      <category term="Mathematics in ML" scheme="http://www.phoebepan.cn/tags/Mathematics-in-ML/"/>
    
  </entry>
  
</feed>
