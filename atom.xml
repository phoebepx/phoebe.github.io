<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>技忆</title>
  <subtitle>Phoebe&#39;s little progress</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://www.phoebepan.cn/"/>
  <updated>2017-07-16T04:40:44.000Z</updated>
  <id>http://www.phoebepan.cn/</id>
  
  <author>
    <name>Phoebe</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>python中time模块</title>
    <link href="http://www.phoebepan.cn/2017/07/16/time/"/>
    <id>http://www.phoebepan.cn/2017/07/16/time/</id>
    <published>2017-07-16T07:30:16.000Z</published>
    <updated>2017-07-16T04:40:44.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote class="blockquote-center"><h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p>在日常数据处理中，常常需要与时间打交道，python中与时间处理有关的模块有：<code>time</code>，<code>datetime</code>，<code>calendar</code>。本文主要介绍<strong>time</strong>模块。<br><img src="/images/time_convert.png" alt="time"></p>
</blockquote>
<a id="more"></a>
<p>Python中，表示时间的方式有：</p>
<ul>
<li>时间戳：通常来说，表示的是从<strong>1970年1月1日00:00:00</strong>开始按秒计算的偏移量；</li>
<li>格式化的时间字符串；</li>
<li>元组(struct_time)。</li>
</ul>
<p>time模块常用的几个函数：</p>
<h3 id="time-localtime"><a href="#time-localtime" class="headerlink" title="time.localtime()"></a>time.localtime()</h3><p>将一个时间戳转换成当前时区的struct_time。</p>
<h3 id="time-time"><a href="#time-time" class="headerlink" title="time.time()"></a>time.time()</h3><p>返回当前时间的时间戳。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt;time.time()</div><div class="line"><span class="number">1500176454.689554</span></div></pre></td></tr></table></figure></p>
<h3 id="time-mktime"><a href="#time-mktime" class="headerlink" title="time.mktime()"></a>time.mktime()</h3><p>将一个struct_time转化为时间戳<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt;time.mktime(time.localtime())</div><div class="line"><span class="number">1500176622.0</span></div></pre></td></tr></table></figure></p>
<h3 id="time-sleep"><a href="#time-sleep" class="headerlink" title="time.sleep()"></a>time.sleep()</h3><p>线程推迟运行，单位为秒</p>
<h3 id="time-strftime"><a href="#time-strftime" class="headerlink" title="time.strftime()"></a>time.strftime()</h3><p>把一个代表时间的元组转化为格式化的时间字符串。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt;time.strftime(<span class="string">'%Y-%m-%d %X'</span>,time.localtime())</div><div class="line"><span class="string">'2017-07-16 11:58:07'</span></div></pre></td></tr></table></figure></p>
<h3 id="time-strptime"><a href="#time-strptime" class="headerlink" title="time.strptime()"></a>time.strptime()</h3><p>格式化时间字符串转化成struct_time。与strftime()操作互逆。<br><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt;time.strptime('2017-07-16 11:58:07','%Y-%m-%d %X'')</div><div class="line">time.struct_time(tm_year=2017, tm_mon=7, tm_mday=16, tm_hour=11, tm_min=58, tm_sec=7, tm_wday=6, tm_yday=197, tm_isdst=-1)</div></pre></td></tr></table></figure></p>
<p>了解更多，请参考<a href="https://docs.python.org/3/library/time.html" target="_blank" rel="external">time模块的官方文档</a>。</p>
]]></content>
    
    <summary type="html">
    
      &lt;blockquote class=&quot;blockquote-center&quot;&gt;&lt;h3 id=&quot;简介&quot;&gt;&lt;a href=&quot;#简介&quot; class=&quot;headerlink&quot; title=&quot;简介&quot;&gt;&lt;/a&gt;简介&lt;/h3&gt;&lt;p&gt;在日常数据处理中，常常需要与时间打交道，python中与时间处理有关的模块有：&lt;code&gt;time&lt;/code&gt;，&lt;code&gt;datetime&lt;/code&gt;，&lt;code&gt;calendar&lt;/code&gt;。本文主要介绍&lt;strong&gt;time&lt;/strong&gt;模块。&lt;br&gt;&lt;img src=&quot;/images/time_convert.png&quot; alt=&quot;time&quot;&gt;&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="Python" scheme="http://www.phoebepan.cn/categories/Python/"/>
    
    
      <category term="Python" scheme="http://www.phoebepan.cn/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>调超参神器——GridSearchCV</title>
    <link href="http://www.phoebepan.cn/2017/06/16/GridSearchCV/"/>
    <id>http://www.phoebepan.cn/2017/06/16/GridSearchCV/</id>
    <published>2017-06-16T07:30:16.000Z</published>
    <updated>2017-07-19T02:41:47.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote class="blockquote-center"><h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p>所谓超参，就是机器学习算法中，不能通过自身学习设定的参数，如SVM的惩罚因子C，核函数kernel，gamma参数等，参数间的组合很是繁琐，人工调节这些超参数时间成本太高，易出错。本文主要介绍sklearn模块的调参神器<code>GridSearchCV</code>模块，它能够在指定范围内自动搜索具有不同超参数的不同模型组合，寻找最佳参数，大大提高调参效率。</p>
</blockquote>
<a id="more"></a>
<h3 id="缘起"><a href="#缘起" class="headerlink" title="缘起"></a>缘起</h3><p>这两天，闲来参加下Ctrip的一个数据竞赛，model选择的是XGboost，好用是自然，但是参数有很多，最迫切需要一个自动调节参数工具，于是接触到GridSearchCV模块。</p>
<h3 id="官方手册"><a href="#官方手册" class="headerlink" title="官方手册"></a>官方手册</h3><p><a href="http://scikit-learn.org/stable/modules/generated/sklearn.grid_search.GridSearchCV.html#sklearn.grid_search.GridSearchCV" target="_blank" rel="external">手册链接</a></p>
<h3 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h3><p>自己训练的代码如下(XGboost+5-fold Cross Validation)，清晰易懂，无须解释。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> xgboost <span class="keyword">as</span> xgb</div><div class="line"><span class="keyword">from</span> sklearn.grid_search <span class="keyword">import</span> GridSearchCV</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">xgbmodel_train</span><span class="params">(train)</span>:</span></div><div class="line">    xgb_model = xgb.XGBClassifier()</div><div class="line">    train_feature, train_label = train.drop(<span class="string">'orderlabel'</span>, axis=<span class="number">1</span>), train[<span class="string">'orderlabel'</span>]</div><div class="line"></div><div class="line">    parameters = &#123;<span class="string">'nthread'</span>: [<span class="number">4</span>],</div><div class="line">                  <span class="string">'objective'</span>: [<span class="string">'binary:logistic'</span>],</div><div class="line">                  <span class="string">'learning_rate'</span>: [<span class="number">0.05</span>,<span class="number">0.06</span>,<span class="number">0.1</span>],</div><div class="line">                  <span class="string">'max_depth'</span>: [<span class="number">5</span>, <span class="number">6</span>],</div><div class="line">                  <span class="string">'min_child_weight'</span>: [<span class="number">1</span>, <span class="number">3</span>],</div><div class="line">                  <span class="string">'silent'</span>: [<span class="number">1</span>],</div><div class="line">                  <span class="string">'gamma'</span>: [<span class="number">0</span>, <span class="number">0.1</span>],</div><div class="line">                  <span class="string">'subsample'</span>: [<span class="number">0.6</span>, <span class="number">0.7</span>, <span class="number">0.8</span>],</div><div class="line">                  <span class="string">'colsample_bytree'</span>: [<span class="number">0.7</span>, <span class="number">0.5</span>, <span class="number">0.6</span>],</div><div class="line">                  <span class="string">'n_estimators'</span>: [<span class="number">5</span>],</div><div class="line">                  <span class="string">'missing'</span>: [<span class="number">-999</span>],</div><div class="line">                  <span class="string">'seed'</span>: [<span class="number">12455</span>]&#125;</div><div class="line"></div><div class="line">    clf = GridSearchCV(xgb_model, parameters, n_jobs=<span class="number">1</span>,</div><div class="line">                       cv=StratifiedKFold(train[<span class="string">'orderlabel'</span>], n_folds=<span class="number">5</span>, shuffle=<span class="keyword">True</span>),</div><div class="line">                       scoring=<span class="string">'roc_auc'</span>,</div><div class="line">                       verbose=<span class="number">2</span>, refit=<span class="keyword">True</span>)</div><div class="line"></div><div class="line"></div><div class="line">    clf.fit(train_feature, train_label)</div><div class="line">   </div><div class="line">    best_parameters, score, _ = max(clf.grid_scores_, key=<span class="keyword">lambda</span> x: x[<span class="number">1</span>])</div><div class="line">    print(<span class="string">'AUC score:'</span>, score)</div><div class="line">    <span class="keyword">for</span> param_name <span class="keyword">in</span> sorted(best_parameters.keys()):</div><div class="line">        print(<span class="string">'%s: %r'</span> % (param_name, best_parameters[param_name]))</div></pre></td></tr></table></figure></p>
<h3 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h3><p><img src="/images/process.png" alt="process"><br><img src="/images/best_score.png" alt="best para"></p>
]]></content>
    
    <summary type="html">
    
      &lt;blockquote class=&quot;blockquote-center&quot;&gt;&lt;h3 id=&quot;简介&quot;&gt;&lt;a href=&quot;#简介&quot; class=&quot;headerlink&quot; title=&quot;简介&quot;&gt;&lt;/a&gt;简介&lt;/h3&gt;&lt;p&gt;所谓超参，就是机器学习算法中，不能通过自身学习设定的参数，如SVM的惩罚因子C，核函数kernel，gamma参数等，参数间的组合很是繁琐，人工调节这些超参数时间成本太高，易出错。本文主要介绍sklearn模块的调参神器&lt;code&gt;GridSearchCV&lt;/code&gt;模块，它能够在指定范围内自动搜索具有不同超参数的不同模型组合，寻找最佳参数，大大提高调参效率。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="http://www.phoebepan.cn/categories/Machine-Learning/"/>
    
    
      <category term="ML" scheme="http://www.phoebepan.cn/tags/ML/"/>
    
      <category term="Tune" scheme="http://www.phoebepan.cn/tags/Tune/"/>
    
  </entry>
  
  <entry>
    <title>数据可视化——Seaborn</title>
    <link href="http://www.phoebepan.cn/2017/06/06/learn_seaborn/"/>
    <id>http://www.phoebepan.cn/2017/06/06/learn_seaborn/</id>
    <published>2017-06-06T07:30:16.000Z</published>
    <updated>2017-06-28T15:14:42.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote class="blockquote-center"><h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p>EDA过程中，想要更了解你的数据，选择一个合适的可视化工具，可以说会让你的工作事半功倍。<br>本文主要介绍一个以matplotlib作为底层，更易上手的作图库<code>seaborn</code>。</p>
</blockquote>
<a id="more"></a>
<h3 id="Seaborn"><a href="#Seaborn" class="headerlink" title="Seaborn"></a>Seaborn</h3><p>基于matplotlib的可视化库，旨在使默认的数据可视化更加悦目，简化复杂图表创建，可以与pandas很好的集成。</p>
<h3 id="简易用法"><a href="#简易用法" class="headerlink" title="简易用法"></a>简易用法</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</div><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns <span class="comment">#一旦导入了seaborn，matplotlib的默认作图风格就会被覆盖成seaborn的格式</span></div><div class="line">%matplotlib inline </div><div class="line"><span class="comment">#在jupyter notebook里作图，需要用到这个命令</span></div></pre></td></tr></table></figure>
<h4 id="读取原始数据（这是一份红酒成分与口感评分数据）"><a href="#读取原始数据（这是一份红酒成分与口感评分数据）" class="headerlink" title="读取原始数据（这是一份红酒成分与口感评分数据）"></a>读取原始数据（这是一份红酒成分与口感评分数据）</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">winedata=pd.read_csv(<span class="string">'winequality-red.csv'</span>)</div><div class="line">winedata.head()</div></pre></td></tr></table></figure>
<p><img src="/images/winedata.png" alt="png"></p>
<h4 id="直方图——seaborn-distplot"><a href="#直方图——seaborn-distplot" class="headerlink" title="直方图——seaborn.distplot()"></a><strong>直方图</strong>——seaborn.distplot()</h4><p>如对上面的quality列做直方图，保留概率密度曲线<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">sns.distplot(winedata[<span class="string">'quality'</span>])   <span class="comment"># 不需要概率密度曲线直接将 kde=False 即可</span></div><div class="line">sns.set_style(<span class="string">'dark'</span>)    <span class="comment">#设置背景色</span></div><div class="line">sns.utils.axlabel(<span class="string">'Quality'</span>, <span class="string">'Frequency'</span>) <span class="comment">#设置X,Y坐标名</span></div><div class="line">sns.plt.show()</div></pre></td></tr></table></figure></p>
<p><img src="/images/output_5_0.png" alt="png"></p>
<h4 id="折线图"><a href="#折线图" class="headerlink" title="折线图"></a>折线图</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">sns.factorplot(data=winedata, x=<span class="string">'quality'</span>, y=<span class="string">'total sulfur dioxide'</span>,size=<span class="number">3</span>)</div><div class="line">sns.plt.show()</div></pre></td></tr></table></figure>
<p><img src="/images/output_7_0.png" alt="png"></p>
<h4 id="柱状图——seaborn-barplot"><a href="#柱状图——seaborn-barplot" class="headerlink" title="柱状图——seaborn.barplot()"></a>柱状图——seaborn.barplot()</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">sns.factorplot(data=winedata, x=<span class="string">'quality'</span>, y=<span class="string">'total sulfur dioxide'</span>,kind=<span class="string">'bar'</span>,size=<span class="number">3</span>)</div><div class="line"><span class="comment">#ax = sns.barplot(data=winedata, x='quality', y='total sulfur dioxide',ci=0)</span></div><div class="line">sns.plt.show()</div></pre></td></tr></table></figure>
<p><img src="/images/output_9_0.png" alt="png"></p>
<h4 id="散点图——seaborn-stripplot"><a href="#散点图——seaborn-stripplot" class="headerlink" title="散点图——seaborn.stripplot()"></a>散点图——seaborn.stripplot()</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">temp=sns.FacetGrid(winedata, hue=<span class="string">'quality'</span>, size=<span class="number">3</span>)   <span class="comment">#hue参数设置区分色彩列</span></div><div class="line">temp.map(plt.scatter, <span class="string">'volatile acidity'</span>, <span class="string">'alcohol'</span>)</div><div class="line">temp.add_legend()</div><div class="line">sns.plt.show()</div><div class="line"><span class="comment">#ax = sns.stripplot(x='quality', y='alcohol', data=winedata) #普通散点图</span></div><div class="line"><span class="comment">#ax = sns.stripplot(x='quality', y='alcohol', data=winedata, jitter=True) #带抖动的散点图</span></div><div class="line"><span class="comment">#sns.plt.show()</span></div></pre></td></tr></table></figure>
<p><img src="/images/output_11_0.png" alt="png"></p>
<h4 id="箱型图——seaborn-boxplot"><a href="#箱型图——seaborn-boxplot" class="headerlink" title="箱型图——seaborn.boxplot()"></a>箱型图——seaborn.boxplot()</h4><p>以quality为X轴，alcohol为Y轴，做出箱线图，可以看出异常值<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">ax=sns.boxplot(x=<span class="string">'quality'</span>, y=<span class="string">'alcohol'</span>, data=winedata)</div><div class="line">ax=sns.stripplot(x=<span class="string">'quality'</span>, y=<span class="string">'alcohol'</span>, data=winedata, jitter=<span class="keyword">True</span>, color=<span class="string">'.3'</span>)  <span class="comment">#加上点，jitter=True 使各个散点分开，要不然会是一条直线</span></div><div class="line">sns.plt.show()</div></pre></td></tr></table></figure></p>
<p><img src="/images/output_13_0.png" alt="png"></p>
<h4 id="小提琴图——seaborn-violinplot"><a href="#小提琴图——seaborn-violinplot" class="headerlink" title="小提琴图——seaborn.violinplot()"></a>小提琴图——seaborn.violinplot()</h4><p>可以看出密度分布<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">ax = sns.violinplot(x=<span class="string">'quality'</span>, y=<span class="string">'alcohol'</span>, data=winedata, size=<span class="number">5</span>)</div><div class="line">ax = sns.swarmplot(x=<span class="string">'quality'</span>, y=<span class="string">'alcohol'</span>, data=winedata,color=<span class="string">'.9'</span>)</div><div class="line">sns.plt.show()</div></pre></td></tr></table></figure></p>
<p><img src="/images/output_15_0.png" alt="png"></p>
<h4 id="多变量作图——seaborn-pairplot"><a href="#多变量作图——seaborn-pairplot" class="headerlink" title="多变量作图——seaborn.pairplot()"></a>多变量作图——seaborn.pairplot()</h4><p>seaborn可以一次性两两组合多个变量做出多个对比图，有n个变量，就会做出一个n × n个格子的图，相同的两个变量之间以直方图展示，不同的变量则以散点图展示，<strong>要注意的是数据中不能有NaN（缺失的数据），否则会报错。</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">sns.pairplot(winedata, vars=[<span class="string">'quality'</span>, <span class="string">'residual sugar'</span>,<span class="string">'alcohol'</span>],hue=<span class="string">'quality'</span>)</div><div class="line">sns.plt.show()</div></pre></td></tr></table></figure></p>
<p><img src="/images/output_17_0.png" alt="png"></p>
<h4 id="回归图——seaborn-lmplot-、seaborn-regplot"><a href="#回归图——seaborn-lmplot-、seaborn-regplot" class="headerlink" title="回归图——seaborn.lmplot()、seaborn.regplot()"></a>回归图——seaborn.lmplot()、seaborn.regplot()</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">sns.lmplot(x=<span class="string">'volatile acidity'</span>, y=<span class="string">'alcohol'</span>, data=winedata)   <span class="comment"># hue参数进行分组拟合，markers=['o', 'x']，col参数不同组的子图</span></div><div class="line">sns.plt.show()</div></pre></td></tr></table></figure>
<p><img src="/images/output_19_0.png" alt="png"><br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">sns.regplot(x=<span class="string">'fixed acidity'</span>, y=<span class="string">'alcohol'</span>, data=winedata)</div><div class="line">sns.plt.show()</div></pre></td></tr></table></figure></p>
<p><img src="/images/output_20_0.png" alt="png"></p>
<p><a href="http://seaborn.pydata.org/tutorial.html" target="_blank" rel="external">更多用法参考官方手册</a><br>点这查看本文<a href="https://github.com/phoebepx/normally-accumulate/blob/master/learn_seaborn.ipynb" target="_blank" rel="external">.ipynb文件</a>，欢迎纠错~</p>
]]></content>
    
    <summary type="html">
    
      &lt;blockquote class=&quot;blockquote-center&quot;&gt;&lt;h3 id=&quot;简介&quot;&gt;&lt;a href=&quot;#简介&quot; class=&quot;headerlink&quot; title=&quot;简介&quot;&gt;&lt;/a&gt;简介&lt;/h3&gt;&lt;p&gt;EDA过程中，想要更了解你的数据，选择一个合适的可视化工具，可以说会让你的工作事半功倍。&lt;br&gt;本文主要介绍一个以matplotlib作为底层，更易上手的作图库&lt;code&gt;seaborn&lt;/code&gt;。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="Data Visualization" scheme="http://www.phoebepan.cn/categories/Data-Visualization/"/>
    
    
      <category term="ML" scheme="http://www.phoebepan.cn/tags/ML/"/>
    
      <category term="Visualization" scheme="http://www.phoebepan.cn/tags/Visualization/"/>
    
  </entry>
  
  <entry>
    <title>阅读笔记——7 Techniques to Handle Imbalanced Data</title>
    <link href="http://www.phoebepan.cn/2017/06/05/Imbalanced_data/"/>
    <id>http://www.phoebepan.cn/2017/06/05/Imbalanced_data/</id>
    <published>2017-06-05T07:30:16.000Z</published>
    <updated>2017-06-28T15:15:00.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote class="blockquote-center"><p>这篇阅读笔记，主要介绍处理不平衡数据的常见7种方法。所谓<strong>不平衡数据</strong>，指在网络入侵、癌症监测、银行信用卡检测等领域，出现如下图所示的数据集中，正负样本比例严重失调的情况。<br><img src="/images/imbalanced-data-1.png" alt="imbalanced-data-1" title="正负样本分布"></p>
</blockquote>
<a id="more"></a>
<p><a href="http://www.kdnuggets.com/2017/06/7-techniques-handle-imbalanced-data.html" target="_blank" rel="external">博客中</a>介绍了7种方法帮助我们训练一个分类器，来处理这些不平衡的数据。</p>
<h3 id="1-使用正确的评价指标"><a href="#1-使用正确的评价指标" class="headerlink" title="1.使用正确的评价指标"></a>1.使用正确的评价指标</h3><p>针对上图这样的数据集，如果我们还是采用准确度(accuracy)来评估模型训练结果，那么所有的分类器将所有的测试样本都分到“0”这一类，模型准确率无疑非常好，但显然，这样的model对我们来说，是没有价值的。<br>这种情况，其他适宜的评估指标有：<code>Precision/Specificity</code>、<code>Recall/Sensitivity</code>、<code>F1 score</code>、<code>MCC</code>、<code>AUC</code>、<code>G-Mean</code></p>
<h3 id="2-训练集重新采样-Resample"><a href="#2-训练集重新采样-Resample" class="headerlink" title="2.训练集重新采样(Resample)"></a>2.训练集重新采样(Resample)</h3><p>除了使用不同的评价指标，另外可以通过<strong>下采样</strong>和<strong>过采样</strong>在不平衡数据中得到平衡数据集。</p>
<h4 id="下采样-Under-sampling"><a href="#下采样-Under-sampling" class="headerlink" title="下采样(Under-sampling)"></a>下采样(Under-sampling)</h4><p>当数据量充足时，下采样通过减少负样本数量（即多数的类），即保留正样本和随机选择相同数量的负样本，得到新的平衡训练集。</p>
<h4 id="过采样-Over-sampling"><a href="#过采样-Over-sampling" class="headerlink" title="过采样(Over-sampling)"></a>过采样(Over-sampling)</h4><p>当数据量不够时，过采样通过增加正样本数来平衡数据集，可以采用<code>repetition</code>、<code>bootstrapping</code>、<code>SMOTE</code>得到新的正样本。<br><a href="https://github.com/scikit-learn-contrib/imbalanced-learn" target="_blank" rel="external">Python实现</a></p>
<p>下采样和过采样两者之间没有谁优谁劣，具体用哪种方式取决于数据集本身，有时两者结合使用可能效果更好。</p>
<h3 id="3-正确使用K折交叉验证"><a href="#3-正确使用K折交叉验证" class="headerlink" title="3.正确使用K折交叉验证"></a>3.正确使用K折交叉验证</h3><p>值得注意的是，当我们用过采样处理不平衡训练集时，通常需要在<strong>过采样之前应用交叉验证</strong>，这样做的好处就是避免模型过拟合。</p>
<h4 id="过拟合产生原因："><a href="#过拟合产生原因：" class="headerlink" title="过拟合产生原因："></a>过拟合产生原因：</h4><ul>
<li>模型的复杂度越高，越容易overfitting</li>
<li>数据的噪声越大，越容易overfitting</li>
<li>数据量越少，越容易overfitting</li>
</ul>
<h3 id="4-重采样训练集集成-Ensemble"><a href="#4-重采样训练集集成-Ensemble" class="headerlink" title="4.重采样训练集集成(Ensemble)"></a>4.重采样训练集集成(Ensemble)</h3><p><img src="/images/imbalanced-data-2.png" alt="imbalanced-data-2" title="Ensemble different resampled datasets"><br>如上面示例图所示，使用所有的正样本和 n 个不同的负样本建立 n 个models。比如你想得到10个models，如果正样本是1000个，那么你需要随机选择10000个负样本，然后将这10000个负样本分成10份，接下来训练这10个不同的models。<br>这种方法，简单方便，易扩展，更好的泛化能力。</p>
<h3 id="5-不同比例采样"><a href="#5-不同比例采样" class="headerlink" title="5.不同比例采样"></a>5.不同比例采样</h3><p>之前的方法，都是1:1调和样本，最佳的比例取决于数据和使用的模型。与其对所有models使用同样的比例进行ensemble，更值得尝试的是采用不同的比例进行ensemble。正如下图所示：<br><img src="/images/imbalanced-data-3.png" alt="imbalanced-data-3" title="Resample with different ratios"></p>
<h3 id="6-负样本进行聚类"><a href="#6-负样本进行聚类" class="headerlink" title="6.负样本进行聚类"></a>6.负样本进行聚类</h3><p>Sergey在Quora上提出一个更完美的<a href="www.quora.com/In-classification-how-do-you-handle-an-unbalanced-training-set/answers/1144228?srid=h3G6o">方法</a>，对负样本进行聚类，只用负样本聚类的簇中心和正样本组成训练集。</p>
<h3 id="7-自己设计模型"><a href="#7-自己设计模型" class="headerlink" title="7.自己设计模型"></a>7.自己设计模型</h3><p>事实上，已经有一些models本身就可以处理非平衡数据集，无需进行重新采样，如XGBoost。<br>重设损失函数，比起负样本误分，对正样本误分设置更大的惩罚系数。</p>
<h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><p>这些处理方法只是一个起点，没有一种方法可以解决所有问题，<code>多试才是王道</code>！</p>
]]></content>
    
    <summary type="html">
    
      &lt;blockquote class=&quot;blockquote-center&quot;&gt;&lt;p&gt;这篇阅读笔记，主要介绍处理不平衡数据的常见7种方法。所谓&lt;strong&gt;不平衡数据&lt;/strong&gt;，指在网络入侵、癌症监测、银行信用卡检测等领域，出现如下图所示的数据集中，正负样本比例严重失调的情况。&lt;br&gt;&lt;img src=&quot;/images/imbalanced-data-1.png&quot; alt=&quot;imbalanced-data-1&quot; title=&quot;正负样本分布&quot;&gt;&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="http://www.phoebepan.cn/categories/Machine-Learning/"/>
    
    
      <category term="ML" scheme="http://www.phoebepan.cn/tags/ML/"/>
    
      <category term="Reading" scheme="http://www.phoebepan.cn/tags/Reading/"/>
    
  </entry>
  
  <entry>
    <title>推荐系统实战——上下文信息</title>
    <link href="http://www.phoebepan.cn/2017/05/20/context/"/>
    <id>http://www.phoebepan.cn/2017/05/20/context/</id>
    <published>2017-05-20T07:30:16.000Z</published>
    <updated>2017-07-20T02:05:48.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote class="blockquote-center"><h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p>将最符合用户兴趣的物品推荐给用户时，用户此时、此刻、心情等等，是提高推荐系统算法不可忽视的一环，这些信息在推荐系统中对应的专业名词是上下文(context)，本文主要说说<strong>上下文信息</strong>的事儿~</p>
</blockquote>
<a id="more"></a>
<h3 id="思维导图"><a href="#思维导图" class="headerlink" title="思维导图"></a>思维导图</h3><p><img src="/images/context.png" alt="context"></p>
]]></content>
    
    <summary type="html">
    
      &lt;blockquote class=&quot;blockquote-center&quot;&gt;&lt;h3 id=&quot;简介&quot;&gt;&lt;a href=&quot;#简介&quot; class=&quot;headerlink&quot; title=&quot;简介&quot;&gt;&lt;/a&gt;简介&lt;/h3&gt;&lt;p&gt;将最符合用户兴趣的物品推荐给用户时，用户此时、此刻、心情等等，是提高推荐系统算法不可忽视的一环，这些信息在推荐系统中对应的专业名词是上下文(context)，本文主要说说&lt;strong&gt;上下文信息&lt;/strong&gt;的事儿~&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="Recommendation" scheme="http://www.phoebepan.cn/categories/Recommendation/"/>
    
    
      <category term="Recommendation" scheme="http://www.phoebepan.cn/tags/Recommendation/"/>
    
  </entry>
  
  <entry>
    <title>推荐系统实战——用户标签数据</title>
    <link href="http://www.phoebepan.cn/2017/05/19/user_tags/"/>
    <id>http://www.phoebepan.cn/2017/05/19/user_tags/</id>
    <published>2017-05-19T07:30:16.000Z</published>
    <updated>2017-07-19T08:25:13.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote class="blockquote-center"><h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p>推荐系统的目的是联系用户的兴趣和物品，倘若通过一些特征(feature)联系用户和物品，给用户推荐那些具有用户喜欢的特征的物品，标签作为一种无层次化结构的、用来描述信息的关键词。本文主要介绍有关<strong>标签</strong>在推荐系统中的应用。</p>
</blockquote>
<a id="more"></a>
<h3 id="思维导图"><a href="#思维导图" class="headerlink" title="思维导图"></a>思维导图</h3><p><img src="/images/user_tag.png" alt="user_tag"></p>
<p> [1]项亮，推荐系统实战.</p>
]]></content>
    
    <summary type="html">
    
      &lt;blockquote class=&quot;blockquote-center&quot;&gt;&lt;h3 id=&quot;简介&quot;&gt;&lt;a href=&quot;#简介&quot; class=&quot;headerlink&quot; title=&quot;简介&quot;&gt;&lt;/a&gt;简介&lt;/h3&gt;&lt;p&gt;推荐系统的目的是联系用户的兴趣和物品，倘若通过一些特征(feature)联系用户和物品，给用户推荐那些具有用户喜欢的特征的物品，标签作为一种无层次化结构的、用来描述信息的关键词。本文主要介绍有关&lt;strong&gt;标签&lt;/strong&gt;在推荐系统中的应用。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="Recommendation" scheme="http://www.phoebepan.cn/categories/Recommendation/"/>
    
    
      <category term="Recommendation" scheme="http://www.phoebepan.cn/tags/Recommendation/"/>
    
  </entry>
  
  <entry>
    <title>推荐系统实战——冷启动</title>
    <link href="http://www.phoebepan.cn/2017/05/18/cold_start/"/>
    <id>http://www.phoebepan.cn/2017/05/18/cold_start/</id>
    <published>2017-05-18T07:30:16.000Z</published>
    <updated>2017-07-19T08:24:24.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote class="blockquote-center"><h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p>前一篇博文的介绍，可见大量用户行为数据是推荐系统的先决条件。那么，如何在没有大量用户数据的情况下设计个性化推荐系统并让用户对推荐结果满意而增加用户粘性，这就是本文所介绍的冷启动问题。</p>
</blockquote>
<a id="more"></a>
<h3 id="思维导图"><a href="#思维导图" class="headerlink" title="思维导图"></a>思维导图</h3><p><img src="/images/coldstart.png" alt="coldstart"></p>
<h3 id="用户注册信息"><a href="#用户注册信息" class="headerlink" title="用户注册信息"></a>用户注册信息</h3><p>基于用户注册信息的推荐算法其核心问题是计算每种特征的用户喜<br>欢的物品，将 p(f,i)定义为喜欢物品i的用户中具有特征f的比例，$$ p(f,i)=\frac{\left | N(i)\bigcap U(f) \right |}{\left | N(i) \right | + \alpha} $$，参数$\alpha$（较大的数）的目的是解决数据稀疏问题。</p>
<h4 id="如何选择合适的物品启动用户的兴趣"><a href="#如何选择合适的物品启动用户的兴趣" class="headerlink" title="如何选择合适的物品启动用户的兴趣"></a>如何选择合适的物品启动用户的兴趣</h4><p>Nadav Golbandi的算法通过建立物品区分度的决策树，来启动用户的兴趣，下图是Nadav Golbandi算法的举例，<br><img src="/images/Nadav_Golbandi.png" alt="Nadav Golbandi"></p>
<h3 id="物品内容信息"><a href="#物品内容信息" class="headerlink" title="物品内容信息"></a>物品内容信息</h3><p>内容相似度计算简单，能频繁更新，而且能够解决物品冷启动问题，那么为什么还需要协同过滤的算法?</p>
<ul>
<li>内容过滤算法忽视了用户行为，从而也忽视了物品的流行度以及用户行为中所包含的规律，所以它的精度比较低，但结果的新颖度却比较高。</li>
<li>如果用户的行为强烈受某一内容属性的影响，那么内容过滤的算法还是可以在精度上超过协同过滤算法的。</li>
</ul>
<p>所以，通常将这两种算法融合，可获得比单独使用这两种算法更好的效果。</p>
<h4 id="话题模型（topic-model）"><a href="#话题模型（topic-model）" class="headerlink" title="话题模型（topic model）"></a>话题模型（topic model）</h4><p>“推荐系统的动态特性”和“基于时间的协同过滤算法研究”，这两篇文章title关键词不同，但关键词所属话题相同。这种情况下，先知道文章的话题分布，然后才能准确地计算文章的相似度。</p>
<h4 id="KL散度"><a href="#KL散度" class="headerlink" title="KL散度"></a>KL散度</h4><p><strong>定义：</strong><br>对于<code>离散空间</code>两个概率分布P和Q，从Q到P的KL散度为<img src="/images/lisan_KL.gif" alt="lisan_KL"><br>对于<code>连续空间</code>的两个概率分布P和Q，从Q到P的KL散度为：<img src="/images/lianxu_KL.gif" alt="lianxu_KL">，p和q是概率分布P和Q的概率密度。<br><strong>简单例子计算：</strong><br>比如有四个类别，一个方法P得到四个类别的概率分别是0.1，0.2，0.3，0.4。另一种方法Q（或者说是事实情况）是得到四个类别的概率分别是0.4，0.3，0.2，0.1,那么这两个分布的 KL 散度就是：<br><img src="/images/KL_eg.gif" alt="KL_eg"><br><strong>实际案例</strong><br>参考附录2<br> [1]项亮，推荐系统实战.<br> <a href="http://chuansong.me/n/2759305" target="_blank" rel="external">[2] KL散度（从动力系统到推荐系统）</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;blockquote class=&quot;blockquote-center&quot;&gt;&lt;h3 id=&quot;简介&quot;&gt;&lt;a href=&quot;#简介&quot; class=&quot;headerlink&quot; title=&quot;简介&quot;&gt;&lt;/a&gt;简介&lt;/h3&gt;&lt;p&gt;前一篇博文的介绍，可见大量用户行为数据是推荐系统的先决条件。那么，如何在没有大量用户数据的情况下设计个性化推荐系统并让用户对推荐结果满意而增加用户粘性，这就是本文所介绍的冷启动问题。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="Recommendation" scheme="http://www.phoebepan.cn/categories/Recommendation/"/>
    
    
      <category term="Recommendation" scheme="http://www.phoebepan.cn/tags/Recommendation/"/>
    
  </entry>
  
  <entry>
    <title>推荐系统实战——用户行为分析</title>
    <link href="http://www.phoebepan.cn/2017/05/17/user_action_analysis/"/>
    <id>http://www.phoebepan.cn/2017/05/17/user_action_analysis/</id>
    <published>2017-05-17T07:30:16.000Z</published>
    <updated>2017-07-19T08:24:59.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote class="blockquote-center"><h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p>古语，听其言，观其行，用户的行为不是随机的，而是蕴含着很多模式，通过算法自动挖掘用户行为数据，从而推测出用户的兴趣和需求，做出个性化推荐。本文是自己对用户行为分析的学习笔记。</p>
</blockquote>
<a id="more"></a>
<h3 id="思维导图"><a href="#思维导图" class="headerlink" title="思维导图"></a>思维导图</h3><p><img src="/images/user_action_analysis.png" alt="user_action_analysis"></p>
<h3 id="基于邻域算法"><a href="#基于邻域算法" class="headerlink" title="基于邻域算法"></a>基于邻域算法</h3><h4 id="基于用户的协同过滤-UserCF"><a href="#基于用户的协同过滤-UserCF" class="headerlink" title="基于用户的协同过滤(UserCF)"></a>基于用户的协同过滤(UserCF)</h4><p>推荐原理：给用户推荐那些和他有共同兴趣爱好的用户喜欢的物品，着重于和用户兴趣相似的小群体的热点，更社会化。<br>主要包括两步：</p>
<ul>
<li>找到和目标用户兴趣相似的用户集合</li>
<li>找到集合中用户喜欢且目标用户没有行为的物品做出推荐</li>
</ul>
<p><img src="/images/user_item_reversort.png" alt="reserve_sort"><br>上面这张图是，用户行为数据导出用户间共现矩阵的过程，基于该共现矩阵，利用余弦相似度（或Jaccard）得到用户间兴趣相似度后，UserCF算法会给用户推荐和他兴趣最相似的K个用户喜欢的物品。用户u对物品i的感兴趣程度：<br><img src="/images/pui.png" alt="pui"><br>,S(u,K)包含和用户u兴趣最接近的K个用户，N(i)是对物品i有过行为的用户集合，w<sub>uv</sub>是用户u,v的兴趣相似度，r<sub>vi</sub>用户v对物品i的兴趣。<br><strong>改进</strong><br>计算用户间兴趣相似度时，增加用户u,v共同兴趣列表中热门物品的惩罚。原理类似TF-IDF。</p>
<h4 id="基于物品的协同过滤-ItemCF"><a href="#基于物品的协同过滤-ItemCF" class="headerlink" title="基于物品的协同过滤(ItemCF)"></a>基于物品的协同过滤(ItemCF)</h4><p>推荐原理：给用户推荐那些和他之前喜欢的物品类似的物品，着重于维系用户历史兴趣，更个性化。<br>主要包括两步：</p>
<ul>
<li>计算物品间相似度</li>
<li>根据物品相似度和用户历史行为做出推荐</li>
</ul>
<p>ItemCF结果可解释性更好。<br><strong>改进</strong></p>
<ul>
<li>计算物品相似度时，增加活跃用户贡献的惩罚；</li>
<li>物品相似度的归一化，可提高推荐的多样性。</li>
</ul>
<h4 id="UserCF和ItemCF比较："><a href="#UserCF和ItemCF比较：" class="headerlink" title="UserCF和ItemCF比较："></a>UserCF和ItemCF比较：</h4><p><img src="/images/UserCF_ItemCF.png" alt="UserCF_ItemCF"></p>
<h3 id="隐语义模型-LFM"><a href="#隐语义模型-LFM" class="headerlink" title="隐语义模型(LFM)"></a>隐语义模型(LFM)</h3><p>LFM和基于邻域的方法的比较：</p>
<ul>
<li>理论基础：LFM较好的理论依据，邻域主要是基于统计方法；</li>
<li>离线计算的空间复杂度：LFM更节省空间；</li>
<li>离线计算的时间复杂度：一般情况，LFM高于CF；</li>
<li>在线实时推荐：传统LFM难以实现实时；</li>
<li>推荐解释：ItemCF好于LFM。</li>
</ul>
<p>注：当数据集非常稀疏时，LFM的性能会明显下降，甚至不如UserCF和ItemCF的性能。</p>
<h3 id="基于图的模型"><a href="#基于图的模型" class="headerlink" title="基于图的模型"></a>基于图的模型</h3><p>图中顶点的相关性高的一对顶点一般具有如下特征：</p>
<ul>
<li>两个顶点之间有很多路径相连；</li>
<li>连接两个顶点之间的路径长度都比较短；</li>
<li>连接两个顶点之间的路径不会经过出度比较大的顶点。</li>
</ul>
<p><strong>随机游走</strong>算法思路：<br>假如要给用户A进行个性化推荐，可以从用户A对应的节点开始在用户物品<strong>二分图</strong>（如下图）上随机游走，游走到任何一个节点时，首先按照概率a决定是否继续游走，若继续，就从当前节点指向的节点中按照均匀分布随机选择一个节点作为下一次游走经过的节点；否则，停止该次游走，从用户A对应节点重新开始游走。多次迭代，直到每个物品节点被访问到的概率收敛到一个数。<br><img src="/images/part2graph.png" alt="part2graph"></p>
<p> [1]项亮，推荐系统实战.</p>
]]></content>
    
    <summary type="html">
    
      &lt;blockquote class=&quot;blockquote-center&quot;&gt;&lt;h3 id=&quot;简介&quot;&gt;&lt;a href=&quot;#简介&quot; class=&quot;headerlink&quot; title=&quot;简介&quot;&gt;&lt;/a&gt;简介&lt;/h3&gt;&lt;p&gt;古语，听其言，观其行，用户的行为不是随机的，而是蕴含着很多模式，通过算法自动挖掘用户行为数据，从而推测出用户的兴趣和需求，做出个性化推荐。本文是自己对用户行为分析的学习笔记。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="Recommendation" scheme="http://www.phoebepan.cn/categories/Recommendation/"/>
    
    
      <category term="Recommendation" scheme="http://www.phoebepan.cn/tags/Recommendation/"/>
    
  </entry>
  
  <entry>
    <title>推荐系统实战——概述</title>
    <link href="http://www.phoebepan.cn/2017/05/16/Recommendation_action1/"/>
    <id>http://www.phoebepan.cn/2017/05/16/Recommendation_action1/</id>
    <published>2017-05-16T07:30:16.000Z</published>
    <updated>2017-07-19T08:24:32.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote class="blockquote-center"><h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p>信息过载的现在，催生出了个性化推荐系统，“千人千面”，分析你的历史兴趣，发现对用户有价值的信息，同时，让信息能够展现在对它感兴趣的用户面前，可谓是双赢的酷事！选择《推荐系统实战》这本书作为自己的入门读物，一步一步深入学习个性化推荐，本文是推荐系统的概述，作为系列开篇之记。</p>
</blockquote>
<a id="more"></a>
<h3 id="思维导图"><a href="#思维导图" class="headerlink" title="思维导图"></a>思维导图</h3><p>下面这张思维导图，可以粗略了解推荐系统是什么，解决什么问题，有什么用（经典三问，是什么？为什么？怎么样？），以及如何评价推荐系统优劣？<br><img src="/images/recommend.png" alt="recommend"></p>
<h3 id="附1：覆盖率——基尼系数计算原理："><a href="#附1：覆盖率——基尼系数计算原理：" class="headerlink" title="附1：覆盖率——基尼系数计算原理："></a>附1：覆盖率——基尼系数计算原理：</h3><p>结合下图，gini系数的形象化解释为（黑色曲线表示最不热门的x%物品的总流行度占系统的比例y%）,$$Gini=\frac{A的面积}{(A+B)的面积} $$<br><img src="/images/gini.png" alt="gini"><br>由此可见，如果系统物品流行度分配很不均匀，那么分子就会很大，从而基尼系数也会很大。</p>
<h3 id="附2：一个推荐算法最终上线，需完成3个实验。"><a href="#附2：一个推荐算法最终上线，需完成3个实验。" class="headerlink" title="附2：一个推荐算法最终上线，需完成3个实验。"></a>附2：一个推荐算法最终上线，需完成3个实验。</h3><ul>
<li>离线实验证明他在很多离线指标上优于现有算法；</li>
<li>通过用户调查确定它的用户满意度不低于现有算法；</li>
<li>在线AB测试确定它在我们关心的指标上优于现有算法。</li>
</ul>
<h3 id="附3：离线实验的优化目标"><a href="#附3：离线实验的优化目标" class="headerlink" title="附3：离线实验的优化目标"></a>附3：离线实验的优化目标</h3><p>用一个数学公式表达，如下：<br>最大化预测准确度<br>使得 覆盖率 &gt; A，多样性 &gt; B，新颖性 &gt; C</p>
<p> [1]项亮，推荐系统实战.</p>
]]></content>
    
    <summary type="html">
    
      &lt;blockquote class=&quot;blockquote-center&quot;&gt;&lt;h3 id=&quot;简介&quot;&gt;&lt;a href=&quot;#简介&quot; class=&quot;headerlink&quot; title=&quot;简介&quot;&gt;&lt;/a&gt;简介&lt;/h3&gt;&lt;p&gt;信息过载的现在，催生出了个性化推荐系统，“千人千面”，分析你的历史兴趣，发现对用户有价值的信息，同时，让信息能够展现在对它感兴趣的用户面前，可谓是双赢的酷事！选择《推荐系统实战》这本书作为自己的入门读物，一步一步深入学习个性化推荐，本文是推荐系统的概述，作为系列开篇之记。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="Recommendation" scheme="http://www.phoebepan.cn/categories/Recommendation/"/>
    
    
      <category term="Recommendation" scheme="http://www.phoebepan.cn/tags/Recommendation/"/>
    
  </entry>
  
  <entry>
    <title>充实的无话可说</title>
    <link href="http://www.phoebepan.cn/2017/05/15/fine_life/"/>
    <id>http://www.phoebepan.cn/2017/05/15/fine_life/</id>
    <published>2017-05-15T07:30:16.000Z</published>
    <updated>2017-06-04T02:26:13.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote class="blockquote-center"><p>有人说，人生有两大遗憾，太晚谈恋爱，太早读经典。读懂一本书，看懂一部电影和谈对一场恋爱，都需要刚刚好的时间。太早理解不了，太晚就少了份陪伴感。</p>
</blockquote>
<a id="more"></a>
<h3 id="碎碎念1"><a href="#碎碎念1" class="headerlink" title="碎碎念1"></a><center>碎碎念1</center></h3><blockquote>
<p>好朋友问我，嘿，最近怎么不见你出去浪了呢，端午打算去哪耍去？<br>我笑着答道，宅家看书。<br>朋友诡异的看着我说，不是吧，学霸真可怕！</p>
</blockquote>
<p>回想年后这几个月自己的转变还真是蛮大的，从以前放荡自由的野孩子，变得像点研究僧的模样，而且越来越喜欢这样自律的自己。<br>研一的周末，要么在研究烘焙，要么参加户外活动，认识了很多有意思的朋友，相比较单调的校园生活，可以说外面的世界太精彩，一次次被惊艳，看着他们活出自己理想生活状态，可以说是非常羡慕。然而渐渐我发现，如果自己不够优秀，认识再多的人，你的生活也不会有所改变。现在的我，学着跟自己和平相处，控制情绪，学着量化每天的时间，每天有小目标，生活得很充实。</p>
<h3 id="碎碎念2"><a href="#碎碎念2" class="headerlink" title="碎碎念2"></a><center>碎碎念2</center></h3><p>现在的我，每天不管多忙，多晚，也要读几页书，读书，可以说是一种自嗨的事儿，一旦这样的自嗨能力养成了，你就不容易陷入无聊，不容易被别人的想法左右。<br>人只能活一次，没有可以对标的人生，上一辈的建议有时过气，同龄人其实也一样迷茫，而读书就不一样了，只要你尝试阅读不同的书，你就可以尝试不同的人生，体验不同的精彩生活，重回现实，你会活的更明白。<br>平淡的日子，不急不躁，继续丰盈自己~</p>
<p><img src="/images/Shawshank_Redemption.jpg" alt="肖申克的救赎" title="The Shawshank Redemption"></p>
]]></content>
    
    <summary type="html">
    
      &lt;blockquote class=&quot;blockquote-center&quot;&gt;&lt;p&gt;有人说，人生有两大遗憾，太晚谈恋爱，太早读经典。读懂一本书，看懂一部电影和谈对一场恋爱，都需要刚刚好的时间。太早理解不了，太晚就少了份陪伴感。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="Thinking" scheme="http://www.phoebepan.cn/categories/Thinking/"/>
    
    
      <category term="Life" scheme="http://www.phoebepan.cn/tags/Life/"/>
    
      <category term="Thinking" scheme="http://www.phoebepan.cn/tags/Thinking/"/>
    
  </entry>
  
  <entry>
    <title>Modern Machine Learning Algorithms:Strengths and Weaknesses(阅读笔记)</title>
    <link href="http://www.phoebepan.cn/2017/05/13/ML_good_weak/"/>
    <id>http://www.phoebepan.cn/2017/05/13/ML_good_weak/</id>
    <published>2017-05-13T07:30:16.000Z</published>
    <updated>2017-06-02T14:34:39.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote class="blockquote-center"><p><center><strong>推荐理由</strong></center><br>对于机器学习算法的盘点，网上有很多。但本文亮点在于结合使用场景来把问题说明白，作者结合他的实际经验，细致剖析每种算法在实践中的优势和不足。</p>
</blockquote>
<a id="more"></a>
<p>当前的「三大」最常见的机器学习任务：</p>
<ul>
<li>回归（Regression）</li>
<li>分类（Classification）</li>
<li>聚类（Clustering）</li>
</ul>
<p>下面是我阅读这篇文章后总结的的思维导图：<br><img src="/images/whole.png" alt="机器学习3大任务" title="机器学习3大任务"><br><img src="/images/regression1.png" alt="回归"><br><img src="/images/regression2.png" alt="回归" title="回归"><br><img src="/images/classfication1.png" alt="分类"><br><img src="/images/classfication2.png" alt="分类" title="分类"><br><img src="/images/cocluster1.png" alt="聚类"><br><img src="/images/cocluster2.png" alt="聚类" title="聚类"><br>最后，<a href="https://elitedatascience.com/machine-learning-algorithms" target="_blank" rel="external">附原文链接</a>，欢迎纠错。<br><blockquote class="blockquote-center"><p>Of course, the algorithms you try must be appropriate for your problem, which is where picking the right machine learning task comes in.</p>
</blockquote></p>
]]></content>
    
    <summary type="html">
    
      &lt;blockquote class=&quot;blockquote-center&quot;&gt;&lt;p&gt;&lt;center&gt;&lt;strong&gt;推荐理由&lt;/strong&gt;&lt;/center&gt;&lt;br&gt;对于机器学习算法的盘点，网上有很多。但本文亮点在于结合使用场景来把问题说明白，作者结合他的实际经验，细致剖析每种算法在实践中的优势和不足。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="http://www.phoebepan.cn/categories/Machine-Learning/"/>
    
    
      <category term="ML" scheme="http://www.phoebepan.cn/tags/ML/"/>
    
      <category term="Reading" scheme="http://www.phoebepan.cn/tags/Reading/"/>
    
  </entry>
  
  <entry>
    <title>Pandas杂碎</title>
    <link href="http://www.phoebepan.cn/2017/05/12/pandas/"/>
    <id>http://www.phoebepan.cn/2017/05/12/pandas/</id>
    <published>2017-05-12T07:30:16.000Z</published>
    <updated>2017-05-29T05:14:14.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote class="blockquote-center"><p>本文主要是平时运用Pandas的小积累，涉及到读写文件，格式转换、缺失值填充，简单统计等多个方面。</p>
</blockquote>
<a id="more"></a>
<h3 id="读写csv文件到DataFrame"><a href="#读写csv文件到DataFrame" class="headerlink" title="读写csv文件到DataFrame"></a>读写csv文件到DataFrame</h3><p><code>df=pd.read_csv(&#39;filename&#39;,encoding=&#39;utf-8&#39;)</code><br>读入csv文本，编码为utf-8<br><code>df.to_csv(&#39;filename&#39;,sep=&#39;\t&#39;,index=False)</code><br>‘\t’切分df，写入csv，不包含行索引</p>
<h3 id="格式转换、值填充、删除"><a href="#格式转换、值填充、删除" class="headerlink" title="格式转换、值填充、删除"></a>格式转换、值填充、删除</h3><p> <code>df.to_dict(outtype=&#39;dict&#39;)</code><br>将DataFrame转换成其他结构类型，outtype的参数为‘dict’、‘list’、‘series’和‘records’<br> <code>df[&#39;A&#39;].astype(float)</code><br>将DataFrame的A列类型更改成float<br><code>df.rename(columns={&#39;oldname&#39;: &#39;newname&#39;}, inplace=True)</code>   #修改列名<br><code>df.fillna(0)</code>  #NaN用0填充，处理缺失值<br><code>df.drop_duplicates(subset=[&#39;A&#39;]，keep=&#39;first&#39;)</code><br>按A列去重，保留第一行，keep（’first’,’last’,False）</p>
<h3 id="查看DataFrame概要"><a href="#查看DataFrame概要" class="headerlink" title="查看DataFrame概要"></a>查看DataFrame概要</h3><p><code>df.head()</code>     #前几行<br><code>df.tail()</code>     #末几行<br><code>df.index</code>      #行标签<br><code>df.columns</code>    #列标签<br><code>df.dtypes</code>     #每一列数据类型<br><code>df[&#39;column&#39;].count()</code>    #column列非空条数<br><code>df[&#39;column&#39;].isnull()</code>   #column列是否有NaN的数据<br><code>df[&#39;column&#39;].unique()</code>    #column列所有值</p>
<h3 id="简单统计"><a href="#简单统计" class="headerlink" title="简单统计"></a>简单统计</h3><p><code>df.describe()</code>   #各列基本描述统计值<br><code>df[&#39;A&#39;].value_counts()</code>  #计算A列每个值的频率</p>
<h3 id="排序"><a href="#排序" class="headerlink" title="排序"></a>排序</h3><p><code>df.sort_index(axis=1,ascending=False)</code><br>对DataFrame的axis=0(行)，1(列)索引排序（ascending=True(升)，False(降)）<br><code>df.sort_index(by=[&#39;A&#39;, &#39;B&#39;], ascending=[True, False])</code>&lt;==&gt;<code>df.sort_values(by=[&#39;A&#39;, &#39;B&#39;], ascending=[True, False])</code>&lt;==&gt;<code>df.sort(columns=[&#39;A&#39;,&#39;B&#39;],ascending=[1,0])</code><br>对DataFrame先按’A’升排序, 再按’B’降排序</p>
<h3 id="分组"><a href="#分组" class="headerlink" title="分组"></a>分组</h3><p><code>df.groupby(subset=[&#39;A&#39;,&#39;B&#39;],as_index=True)</code>    #以列A，B对df进行分组，默认A，B作为分组索引<br><code>df.groupby(by=[&#39;A&#39;],as_index=True).get_group(&#39;a&#39;)</code> #按A列分组后获取’a’组<br><code>df.groupby([&#39;col1&#39;,&#39;col2&#39;]).size().to_frame(name =&#39;count&#39;)</code><br>&lt;==&gt;<code>df.groupby([&#39;col1&#39;,&#39;col2&#39;]).size().reset_index(name =&#39;count&#39;)</code><br>按col1,col2对df进行分组，<code>size()</code>统计每一分组成员个数(<code>count()</code>也可以，<a href="https://stackoverflow.com/documentation/pandas/1822/grouping-data/6874/aggregating-by-size-and-count#t=201607220906502658034" target="_blank" rel="external">区别</a>就是count()过滤了NaN，size()包含)，命名为’count’</p>
<h3 id="选择DataFrame行列"><a href="#选择DataFrame行列" class="headerlink" title="选择DataFrame行列"></a>选择DataFrame行列</h3><p><a href="http://www.shanelynn.ie/select-pandas-dataframe-rows-and-columns-using-iloc-loc-and-ix/" target="_blank" rel="external">示例参考</a><br><code>df.iloc[&lt;row selection&gt;,&lt;col selection&gt;]</code>  #基于行列索引选择（Selecting rows by label/index）<br><code>df.loc[&lt;row selection&gt;,&lt;col selection&gt;]</code>   #基于条件选择（Selecting rows with a boolean / conditional lookup）<br><code>ix[]</code>  #索引，条件混合选择</p>
<p><code>df[df[&#39;A&#39;].isin(li)]</code>  #选择A列值在列表li中的行<br><code>df.saple(n=3)</code> #随机抽取3行数据，</p>
<h3 id="归一化处理"><a href="#归一化处理" class="headerlink" title="归一化处理"></a>归一化处理</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">from sklearn import preprocessing</div><div class="line">df=preprocessing.scale(df,axis=0)   #df每一个feature归一化(列)</div><div class="line">df=preprocessing.scale(df,axis=1)   #df每一个sample归一化(行)</div></pre></td></tr></table></figure>
<h3 id="离散特征one-hot编码"><a href="#离散特征one-hot编码" class="headerlink" title="离散特征one-hot编码"></a>离散特征one-hot编码</h3><p><code>df = pd.get_dummies(df[&#39;A&#39;], prefix=&#39;A&#39;)</code>  #对A列重新编码，编码后的列前缀加上’A’<br><code>df = pd.get_dummies(df)</code>   #对df所有离散特征进行one-hot编码</p>
<h3 id="合并——merge、concat、append"><a href="#合并——merge、concat、append" class="headerlink" title="合并——merge、concat、append"></a>合并——merge、concat、append</h3><p><code>df1.append(df2,ignore_index=True)</code> #在df1下面追加df2，行索引重排序<br><code>pd.concat([df1[&#39;A&#39;],df2],axis=1)</code>  #列方向合并<br><code>pd.merge(df1, df2, how=&#39;inner&#39;, on=None, left_on=None, right_on=None,left_index=False, right_index=False)</code><br>how=[‘inner’,’left’,’right’,’outer’]对应内，左，右，外连接,on指定连接key</p>
]]></content>
    
    <summary type="html">
    
      &lt;blockquote class=&quot;blockquote-center&quot;&gt;&lt;p&gt;本文主要是平时运用Pandas的小积累，涉及到读写文件，格式转换、缺失值填充，简单统计等多个方面。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="Python" scheme="http://www.phoebepan.cn/categories/Python/"/>
    
    
      <category term="Python" scheme="http://www.phoebepan.cn/tags/Python/"/>
    
      <category term="pandas" scheme="http://www.phoebepan.cn/tags/pandas/"/>
    
  </entry>
  
  <entry>
    <title>如何决定K-Means聚类个数——silhouette analysis</title>
    <link href="http://www.phoebepan.cn/2017/05/05/silhouette%20analysis/"/>
    <id>http://www.phoebepan.cn/2017/05/05/silhouette analysis/</id>
    <published>2017-05-05T07:30:16.000Z</published>
    <updated>2017-06-12T23:44:18.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote class="blockquote-center"><h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p>在K-Means聚类时，我们常常会纠结，K该取多大呢？今天无意当中查看Sklearn时，发现了<code>silhouette analysis</code>，翻译过来，就是轮廓分析，具体来说，就是通过结果簇之间的分隔距离来辅助决定K的取值。偷个懒，直接参照手册，学习下。</p>
</blockquote>
<a id="more"></a>
<h3 id="数据集分布"><a href="#数据集分布" class="headerlink" title="数据集分布"></a>数据集分布</h3><p>下图是500个样本含有2个feature的数据分布情况：<br><img src="/images/scatter.png" alt="scatter"></p>
<h3 id="轮廓系数"><a href="#轮廓系数" class="headerlink" title="轮廓系数"></a>轮廓系数</h3><p>接下来看下，n_clusters 分别为 2，3，4，5，6时，平均的轮廓分值（结果簇之间平均的分隔距离）如下，这个轮廓分值是介于[-1,1]之间的度量指标。每次聚类后，每个样本都会得到一个轮廓系数，当它为1时，说明这个点与周围簇距离较远，结果非常好，当它为0，说明这个点可能处在两个簇的边界上，当值为负时，暗含该点可能被误分了。<br>从平均轮廓分值结果来看，K取3，5，6是不明智的。对于2,4的选择还是有点纠结的，因为它们值相差并不大。</p>
<blockquote>
<p>For n_clusters = 2 The average silhouette_score is : 0.704978749608<br>For n_clusters = 3 The average silhouette_score is : 0.588200401213<br>For n_clusters = 4 The average silhouette_score is : 0.650518663273<br>For n_clusters = 5 The average silhouette_score is : 0.563764690262<br>For n_clusters = 6 The average silhouette_score is : 0.450466629437</p>
</blockquote>
<h3 id="轮廓宽度"><a href="#轮廓宽度" class="headerlink" title="轮廓宽度"></a>轮廓宽度</h3><p>下面是簇为2,3,4,5,6相对应的轮廓图，左图可以看出，当n_clusters = 2时，第0簇的宽度远宽于第1簇，但n_clusters = 4时，所聚的簇宽度相差不大，这样分析下来，自然而然会选择K=4，作为最终聚类个数。<br><img src="/images/figure_1.png" alt="figure1"><br><img src="/images/figure_2.png" alt="figure2"><br><img src="/images/figure_3.png" alt="figure3"><br><img src="/images/figure_4.png" alt="figure4"><br><img src="/images/figure_5.png" alt="figure5"></p>
<p>本例参考官方手册，<a href="http://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html#sphx-glr-auto-examples-cluster-plot-kmeans-silhouette-analysis-py" target="_blank" rel="external">详情戳这</a></p>
<h3 id="附脚本"><a href="#附脚本" class="headerlink" title="附脚本"></a>附脚本</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_blobs</div><div class="line"><span class="keyword">from</span> sklearn.cluster <span class="keyword">import</span> KMeans</div><div class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> silhouette_samples, silhouette_score</div><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line"><span class="keyword">import</span> matplotlib.cm <span class="keyword">as</span> cm</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="comment"># Generating the sample data from make_blobs</span></div><div class="line"><span class="comment"># This particular setting has one distinct cluster and 3 clusters placed close</span></div><div class="line"><span class="comment"># together.</span></div><div class="line">X, y = make_blobs(n_samples=<span class="number">500</span>,</div><div class="line">                  n_features=<span class="number">2</span>,</div><div class="line">                  centers=<span class="number">4</span>,</div><div class="line">                  cluster_std=<span class="number">1</span>,</div><div class="line">                  center_box=(<span class="number">-10.0</span>, <span class="number">10.0</span>),</div><div class="line">                  shuffle=<span class="keyword">True</span>,</div><div class="line">                  random_state=<span class="number">1</span>)  <span class="comment"># For reproducibility</span></div><div class="line"></div><div class="line">range_n_clusters = [<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]</div><div class="line"></div><div class="line"><span class="keyword">for</span> n_clusters <span class="keyword">in</span> range_n_clusters:</div><div class="line">    <span class="comment"># Create a subplot with 1 row and 2 columns</span></div><div class="line">    fig, (ax1, ax2) = plt.subplots(<span class="number">1</span>, <span class="number">2</span>)</div><div class="line">    fig.set_size_inches(<span class="number">18</span>, <span class="number">7</span>)</div><div class="line"></div><div class="line">    <span class="comment"># The 1st subplot is the silhouette plot</span></div><div class="line">    <span class="comment"># The silhouette coefficient can range from -1, 1 but in this example all</span></div><div class="line">    <span class="comment"># lie within [-0.1, 1]</span></div><div class="line">    ax1.set_xlim([<span class="number">-0.1</span>, <span class="number">1</span>])</div><div class="line">    <span class="comment"># The (n_clusters+1)*10 is for inserting blank space between silhouette</span></div><div class="line">    <span class="comment"># plots of individual clusters, to demarcate them clearly.</span></div><div class="line">    ax1.set_ylim([<span class="number">0</span>, len(X) + (n_clusters + <span class="number">1</span>) * <span class="number">10</span>])</div><div class="line"></div><div class="line">    <span class="comment"># Initialize the clusterer with n_clusters value and a random generator</span></div><div class="line">    <span class="comment"># seed of 10 for reproducibility.</span></div><div class="line">    clusterer = KMeans(n_clusters=n_clusters, random_state=<span class="number">10</span>)</div><div class="line">    cluster_labels = clusterer.fit_predict(X)</div><div class="line"></div><div class="line">    <span class="comment"># The silhouette_score gives the average value for all the samples.</span></div><div class="line">    <span class="comment"># This gives a perspective into the density and separation of the formed</span></div><div class="line">    <span class="comment"># clusters</span></div><div class="line">    silhouette_avg = silhouette_score(X, cluster_labels)</div><div class="line">    print(<span class="string">"For n_clusters ="</span>, n_clusters,</div><div class="line">          <span class="string">"The average silhouette_score is :"</span>, silhouette_avg)</div><div class="line"></div><div class="line">    <span class="comment"># Compute the silhouette scores for each sample</span></div><div class="line">    sample_silhouette_values = silhouette_samples(X, cluster_labels)</div><div class="line"></div><div class="line">    y_lower = <span class="number">10</span></div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(n_clusters):</div><div class="line">        <span class="comment"># Aggregate the silhouette scores for samples belonging to</span></div><div class="line">        <span class="comment"># cluster i, and sort them</span></div><div class="line">        ith_cluster_silhouette_values = \</div><div class="line">            sample_silhouette_values[cluster_labels == i]</div><div class="line"></div><div class="line">        ith_cluster_silhouette_values.sort()</div><div class="line"></div><div class="line">        size_cluster_i = ith_cluster_silhouette_values.shape[<span class="number">0</span>]</div><div class="line">        y_upper = y_lower + size_cluster_i</div><div class="line"></div><div class="line">        color = cm.spectral(float(i) / n_clusters)</div><div class="line">        ax1.fill_betweenx(np.arange(y_lower, y_upper),</div><div class="line">                          <span class="number">0</span>, ith_cluster_silhouette_values,</div><div class="line">                          facecolor=color, edgecolor=color, alpha=<span class="number">0.7</span>)</div><div class="line"></div><div class="line">        <span class="comment"># Label the silhouette plots with their cluster numbers at the middle</span></div><div class="line">        ax1.text(<span class="number">-0.05</span>, y_lower + <span class="number">0.5</span> * size_cluster_i, str(i))</div><div class="line"></div><div class="line">        <span class="comment"># Compute the new y_lower for next plot</span></div><div class="line">        y_lower = y_upper + <span class="number">10</span>  <span class="comment"># 10 for the 0 samples</span></div><div class="line"></div><div class="line">    ax1.set_title(<span class="string">"The silhouette plot for the various clusters."</span>)</div><div class="line">    ax1.set_xlabel(<span class="string">"The silhouette coefficient values"</span>)</div><div class="line">    ax1.set_ylabel(<span class="string">"Cluster label"</span>)</div><div class="line"></div><div class="line">    <span class="comment"># The vertical line for average silhouette score of all the values</span></div><div class="line">    ax1.axvline(x=silhouette_avg, color=<span class="string">"red"</span>, linestyle=<span class="string">"--"</span>)</div><div class="line"></div><div class="line">    ax1.set_yticks([])  <span class="comment"># Clear the yaxis labels / ticks</span></div><div class="line">    ax1.set_xticks([<span class="number">-0.1</span>, <span class="number">0</span>, <span class="number">0.2</span>, <span class="number">0.4</span>, <span class="number">0.6</span>, <span class="number">0.8</span>, <span class="number">1</span>])</div><div class="line"></div><div class="line">    <span class="comment"># 2nd Plot showing the actual clusters formed</span></div><div class="line">    colors = cm.spectral(cluster_labels.astype(float) / n_clusters)</div><div class="line">    ax2.scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], marker=<span class="string">'.'</span>, s=<span class="number">30</span>, lw=<span class="number">0</span>, alpha=<span class="number">0.7</span>,</div><div class="line">                c=colors)</div><div class="line"></div><div class="line">    <span class="comment"># Labeling the clusters</span></div><div class="line">    centers = clusterer.cluster_centers_</div><div class="line">    <span class="comment"># Draw white circles at cluster centers</span></div><div class="line">    ax2.scatter(centers[:, <span class="number">0</span>], centers[:, <span class="number">1</span>],</div><div class="line">                marker=<span class="string">'o'</span>, c=<span class="string">"white"</span>, alpha=<span class="number">1</span>, s=<span class="number">200</span>)</div><div class="line"></div><div class="line">    <span class="keyword">for</span> i, c <span class="keyword">in</span> enumerate(centers):</div><div class="line">        ax2.scatter(c[<span class="number">0</span>], c[<span class="number">1</span>], marker=<span class="string">'$%d$'</span> % i, alpha=<span class="number">1</span>, s=<span class="number">50</span>)</div><div class="line"></div><div class="line">    ax2.set_title(<span class="string">"The visualization of the clustered data."</span>)</div><div class="line">    ax2.set_xlabel(<span class="string">"Feature space for the 1st feature"</span>)</div><div class="line">    ax2.set_ylabel(<span class="string">"Feature space for the 2nd feature"</span>)</div><div class="line"></div><div class="line">    plt.suptitle((<span class="string">"Silhouette analysis for KMeans clustering on sample data "</span></div><div class="line">                  <span class="string">"with n_clusters = %d"</span> % n_clusters),</div><div class="line">                 fontsize=<span class="number">14</span>, fontweight=<span class="string">'bold'</span>)</div><div class="line"></div><div class="line">    plt.show()</div></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;blockquote class=&quot;blockquote-center&quot;&gt;&lt;h3 id=&quot;简介&quot;&gt;&lt;a href=&quot;#简介&quot; class=&quot;headerlink&quot; title=&quot;简介&quot;&gt;&lt;/a&gt;简介&lt;/h3&gt;&lt;p&gt;在K-Means聚类时，我们常常会纠结，K该取多大呢？今天无意当中查看Sklearn时，发现了&lt;code&gt;silhouette analysis&lt;/code&gt;，翻译过来，就是轮廓分析，具体来说，就是通过结果簇之间的分隔距离来辅助决定K的取值。偷个懒，直接参照手册，学习下。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="http://www.phoebepan.cn/categories/Machine-Learning/"/>
    
    
      <category term="ML" scheme="http://www.phoebepan.cn/tags/ML/"/>
    
  </entry>
  
  <entry>
    <title>Scikit-Learn如何做特征选择</title>
    <link href="http://www.phoebepan.cn/2017/04/30/feature%20selection/"/>
    <id>http://www.phoebepan.cn/2017/04/30/feature selection/</id>
    <published>2017-04-30T07:30:16.000Z</published>
    <updated>2017-07-19T02:41:30.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote class="blockquote-center"><h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p>特征选择是<a href="http://phoebepan.cn/2017/04/28/feature%20selection/" target="_blank" rel="external">特征工程</a>相对重要的一环节，太多不相关的特征会降低模型的准确性，特征选择，可以有效降低过拟合、训练时间等问题。本文简单介绍使用Scikit-Learn做特征选择。</p>
</blockquote>
<a id="more"></a>
<p>Scikit-Learn提供了几种不同的特征选择方法，其一，单变量选择（Univariate Selection）；其二，主成分分析（Principal Component Analysis）；其三，递归特征排除法（Recursive Feature Elimination）；其四，特征重要性排名（feature importance ranking）。</p>
<h3 id="Univariate-Selection"><a href="#Univariate-Selection" class="headerlink" title="Univariate Selection"></a>Univariate Selection</h3><p>原理：使用统计检验方法选择与输出变量最相关的特征。<br>使用方法：Scikit-Learn库中的<a href="http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html#sklearn.feature_selection.SelectKBest" target="_blank" rel="external"><code>SelectKBest</code></a>类可用不同的统计检验方法挑选指定数量的重要特征。<br>举例：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Feature Extraction with Univariate Statistical Tests (Chi-squared for classification)</span></div><div class="line"><span class="keyword">import</span> pandas</div><div class="line"><span class="keyword">import</span> numpy</div><div class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> SelectKBest</div><div class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> chi2</div><div class="line"><span class="comment"># load data</span></div><div class="line">url = <span class="string">"https://archive.ics.uci.edu/ml/machine-learning-databases/pima-indians-diabetes/pima-indians-diabetes.data"</span></div><div class="line">names = [<span class="string">'preg'</span>, <span class="string">'plas'</span>, <span class="string">'pres'</span>, <span class="string">'skin'</span>, <span class="string">'test'</span>, <span class="string">'mass'</span>, <span class="string">'pedi'</span>, <span class="string">'age'</span>, <span class="string">'class'</span>]</div><div class="line">dataframe = pandas.read_csv(url, names=names)</div><div class="line">array = dataframe.values</div><div class="line">X = array[:,<span class="number">0</span>:<span class="number">8</span>]</div><div class="line">Y = array[:,<span class="number">8</span>]</div><div class="line"><span class="comment"># feature extraction</span></div><div class="line">test = SelectKBest(score_func=chi2, k=<span class="number">4</span>)</div><div class="line">fit = test.fit(X, Y)</div><div class="line"><span class="comment"># summarize scores</span></div><div class="line">print(fit.scores_)  </div><div class="line"><span class="comment">#[111.52 1411.887 17.605 53.108 2175.565 127.669 5.393 181.304]</span></div><div class="line">X_new = fit.transform(X)</div><div class="line">print(X.shape)              <span class="comment">#(768,8)</span></div><div class="line">print(X_new.shape)          <span class="comment">#(768,4)</span></div></pre></td></tr></table></figure></p>
<h3 id="Principal-Component-Analysis"><a href="#Principal-Component-Analysis" class="headerlink" title="Principal Component Analysis"></a><a href="http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html" target="_blank" rel="external">Principal Component Analysis</a></h3><p>原理：使用线性代数的方式将数据集进行压缩，本质上，是一种降维方法。<br>举例：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Feature Extraction with PCA</span></div><div class="line"><span class="keyword">import</span> numpy</div><div class="line"><span class="keyword">from</span> pandas <span class="keyword">import</span> read_csv</div><div class="line"><span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> PCA</div><div class="line"><span class="comment"># load data</span></div><div class="line">url = <span class="string">"https://archive.ics.uci.edu/ml/machine-learning-databases/pima-indians-diabetes/pima-indians-diabetes.data"</span></div><div class="line">names = [<span class="string">'preg'</span>, <span class="string">'plas'</span>, <span class="string">'pres'</span>, <span class="string">'skin'</span>, <span class="string">'test'</span>, <span class="string">'mass'</span>, <span class="string">'pedi'</span>, <span class="string">'age'</span>, <span class="string">'class'</span>]</div><div class="line">dataframe = read_csv(url, names=names)</div><div class="line">array = dataframe.values</div><div class="line">X = array[:,<span class="number">0</span>:<span class="number">8</span>]</div><div class="line">Y = array[:,<span class="number">8</span>]</div><div class="line"><span class="comment"># feature extraction</span></div><div class="line">pca = PCA(n_components=<span class="number">3</span>)</div><div class="line">fit = pca.fit(X)</div><div class="line"><span class="comment"># summarize components</span></div><div class="line">print(fit.explained_variance_ratio_)</div><div class="line">print(fit.components_)</div><div class="line">X_new = fit.transform(X)</div><div class="line">print(X_new.shape)      <span class="comment">#(768,3)</span></div></pre></td></tr></table></figure></p>
<h3 id="Recursive-Feature-Elimination-RFE"><a href="#Recursive-Feature-Elimination-RFE" class="headerlink" title="Recursive Feature Elimination (RFE)"></a><a href="http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html#sklearn.feature_selection.RFE" target="_blank" rel="external">Recursive Feature Elimination (RFE)</a></h3><p>原理：递归移除特征，剩余特征重新建模，通过模型准确性识别出贡献度最好的特征。<br>使用方法：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Recursive Feature Elimination</span></div><div class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</div><div class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> RFE</div><div class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</div><div class="line"><span class="comment"># load the iris datasets</span></div><div class="line">dataset = datasets.load_iris()</div><div class="line"><span class="comment"># create a base classifier used to evaluate a subset of attributes</span></div><div class="line">model = LogisticRegression()</div><div class="line"><span class="comment"># create the RFE model and select 3 attributes</span></div><div class="line">rfe = RFE(model, <span class="number">3</span>)</div><div class="line">rfe = rfe.fit(dataset.data, dataset.target)</div><div class="line"><span class="comment"># summarize the selection of the attributes</span></div><div class="line">print(rfe.support_)     <span class="comment">#[False  True  True  True]</span></div><div class="line">print(rfe.ranking_)     <span class="comment">#[2 1 1 1]</span></div></pre></td></tr></table></figure></p>
<h3 id="Feature-Importance"><a href="#Feature-Importance" class="headerlink" title="Feature Importance"></a>Feature Importance</h3><p>方法：Random Forest，Extra Trees等基于决策树的ensemble方法，可以直接得到每一个特征的重要性分值，这个分值可以用来做特征选择。查阅手册，了解更多<a href="http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html" target="_blank" rel="external">ExtraTreesClassifier</a>。<br>举例：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Feature Importance</span></div><div class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</div><div class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> metrics</div><div class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> ExtraTreesClassifier</div><div class="line"><span class="comment"># load the iris datasets</span></div><div class="line">dataset = datasets.load_iris()</div><div class="line"><span class="comment"># fit an Extra Trees model to the data</span></div><div class="line">model = ExtraTreesClassifier()</div><div class="line">model.fit(dataset.data, dataset.target)</div><div class="line"><span class="comment"># display the relative importance of each attribute</span></div><div class="line">print(model.feature_importances_)</div></pre></td></tr></table></figure></p>
<p>总之，特征选择主要有3大类方法，过滤(Filter)，重组(Wrapper)，正则化(如LASSO, Elastic Net and Ridge Regression)。</p>
<h3 id="番外篇"><a href="#番外篇" class="headerlink" title="番外篇"></a>番外篇</h3> <blockquote class="blockquote-center"><p>A mistake would be to perform feature selection first to prepare your data, then perform model selection and training on the selected features.</p>
</blockquote>
<p> 这是进行特征选择，可能会犯的错误，我们需要在交叉验证的内循环中嵌入特征选择，即，对每一折进行特征选择。</p>
<p> 另附一份特征选择清单(Checklist)：<br> <img src="/images/feature_selection.png" alt="checklist"><br> 文献阅读：<a href="http://jmlr.csail.mit.edu/papers/volume3/guyon03a/guyon03a.pdf" target="_blank" rel="external">An Introduction to Variable and Feature Selection</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;blockquote class=&quot;blockquote-center&quot;&gt;&lt;h3 id=&quot;简介&quot;&gt;&lt;a href=&quot;#简介&quot; class=&quot;headerlink&quot; title=&quot;简介&quot;&gt;&lt;/a&gt;简介&lt;/h3&gt;&lt;p&gt;特征选择是&lt;a href=&quot;http://phoebepan.cn/2017/04/28/feature%20selection/&quot;&gt;特征工程&lt;/a&gt;相对重要的一环节，太多不相关的特征会降低模型的准确性，特征选择，可以有效降低过拟合、训练时间等问题。本文简单介绍使用Scikit-Learn做特征选择。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="http://www.phoebepan.cn/categories/Machine-Learning/"/>
    
    
      <category term="ML" scheme="http://www.phoebepan.cn/tags/ML/"/>
    
      <category term="FeatureSelection" scheme="http://www.phoebepan.cn/tags/FeatureSelection/"/>
    
  </entry>
  
  <entry>
    <title>机器学习WorkFlow——Model training</title>
    <link href="http://www.phoebepan.cn/2017/04/29/train_model/"/>
    <id>http://www.phoebepan.cn/2017/04/29/train_model/</id>
    <published>2017-04-29T07:30:16.000Z</published>
    <updated>2017-06-27T03:51:47.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote class="blockquote-center"><h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p>经过了EDA，数据清洗，特征工程，终于可以聊聊建模型啦，本文简单介绍整个建模过程，最大限度提高性能，同时避免过拟合。</p>
</blockquote>
<a id="more"></a>
<h3 id="划分数据集"><a href="#划分数据集" class="headerlink" title="划分数据集"></a>划分数据集</h3><p><img src="/images/split_dataset.png" alt="split_dataset"></p>
<h3 id="超参数"><a href="#超参数" class="headerlink" title="超参数"></a>超参数</h3><p>调参，特指调超参，通常在机器学习中有两种类型的参数，其一，模型参数，如回归系数(regression coefficients)、决策树切分点，这些都是可以利用训练数据直接学习得到的；其二，超参，如随机森林中的树个数选择，L1，L2正则化选择等，这些参数是无法通过学习获得的，需要人为指定。</p>
<h3 id="拟合与调参"><a href="#拟合与调参" class="headerlink" title="拟合与调参"></a>拟合与调参</h3><p>通常的做法就是：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">for 每一算法(LR,RF,SVM,etc.):</div><div class="line">    for 每一组超参设置：</div><div class="line">        对训练集做交叉验证；</div><div class="line">        计算cross-validated分值</div></pre></td></tr></table></figure></p>
<p>这样就会得到，每一个算法，每一个超参集下的CV分值，然后：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">for 每一个算法：</div><div class="line">    CV值最大的超参集；</div><div class="line">    对整个训练集再次训练(不进行交叉验证)</div></pre></td></tr></table></figure></p>
<p>这就得到每个算法，一个代表性的训练结果。</p>
<h3 id="挑选最优模型"><a href="#挑选最优模型" class="headerlink" title="挑选最优模型"></a>挑选最优模型</h3><p>下面就需要依据评估指标，利用测试集（之前预留出的一部分训练集），挑选最优模型。<br>评估指标有很多，对回归问题，有均方误差(MSE)或均值绝对误差(MAE)。（值越低越好）；对分类问题，有AUC(值越高越好)，准确率，召回率等。</p>
<p>最后，再考虑你的模型，鲁棒性，一致性、可解释性如何~</p>
]]></content>
    
    <summary type="html">
    
      &lt;blockquote class=&quot;blockquote-center&quot;&gt;&lt;h3 id=&quot;简介&quot;&gt;&lt;a href=&quot;#简介&quot; class=&quot;headerlink&quot; title=&quot;简介&quot;&gt;&lt;/a&gt;简介&lt;/h3&gt;&lt;p&gt;经过了EDA，数据清洗，特征工程，终于可以聊聊建模型啦，本文简单介绍整个建模过程，最大限度提高性能，同时避免过拟合。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="http://www.phoebepan.cn/categories/Machine-Learning/"/>
    
    
      <category term="ML" scheme="http://www.phoebepan.cn/tags/ML/"/>
    
  </entry>
  
  <entry>
    <title>机器学习WorkFlow——Feature Engineering</title>
    <link href="http://www.phoebepan.cn/2017/04/28/feature_engineering/"/>
    <id>http://www.phoebepan.cn/2017/04/28/feature_engineering/</id>
    <published>2017-04-28T07:30:16.000Z</published>
    <updated>2017-07-19T02:41:09.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote class="blockquote-center"><h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p>特征工程，是机器学习过程中最富有想象空间的环节，你可以大胆开脑洞。业界广为流传的一句话：数据和特征决定机器学习上限，而模型和算法只是逼近这个上限而已。本文只是简单介绍特征工程打开思路的一般套路。</p>
</blockquote>
<a id="more"></a>
<p>提到特征工程，脑海中该有的框架，如下图所示：<br><img src="/images/feature.png" alt="feature_engineering"><br>具体特征工程做些什么，可参考这篇博文，查阅请点击<a href="http://www.cnblogs.com/jasonfreak/p/5448385.html" target="_blank" rel="external">这里</a>。<br><img src="/images/feature_engineering.jpg" alt="feature_engineering"></p>
<p>下面是目前自己在特征工程环节，经常考虑的几个方面：<br><img src="/images/feature_engineering_1.png" alt="feature_engineering"></p>
]]></content>
    
    <summary type="html">
    
      &lt;blockquote class=&quot;blockquote-center&quot;&gt;&lt;h3 id=&quot;简介&quot;&gt;&lt;a href=&quot;#简介&quot; class=&quot;headerlink&quot; title=&quot;简介&quot;&gt;&lt;/a&gt;简介&lt;/h3&gt;&lt;p&gt;特征工程，是机器学习过程中最富有想象空间的环节，你可以大胆开脑洞。业界广为流传的一句话：数据和特征决定机器学习上限，而模型和算法只是逼近这个上限而已。本文只是简单介绍特征工程打开思路的一般套路。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="http://www.phoebepan.cn/categories/Machine-Learning/"/>
    
    
      <category term="ML" scheme="http://www.phoebepan.cn/tags/ML/"/>
    
      <category term="FeatureEngineering" scheme="http://www.phoebepan.cn/tags/FeatureEngineering/"/>
    
  </entry>
  
  <entry>
    <title>机器学习WorkFlow——Data cleaning</title>
    <link href="http://www.phoebepan.cn/2017/04/27/data_clean/"/>
    <id>http://www.phoebepan.cn/2017/04/27/data_clean/</id>
    <published>2017-04-27T07:30:16.000Z</published>
    <updated>2017-06-27T02:00:05.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote class="blockquote-center"><p>Garbage in gets you garbage out.<br>Better data beats fancier algorithms.</p>
</blockquote>
<h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a><center>简介</center></h3><p>数据清洗是每个数据人的必备技能，诚然这不是机器学习最‘骨感’的地方，没有什么隐藏的技能值得来说，但这确是最耗时的环节。不同类型数据需要不同处理方式，本文是自己系统学习数据清洗的起点。</p>
<a id="more"></a>
<h3 id="删除不需观测的数据"><a href="#删除不需观测的数据" class="headerlink" title="删除不需观测的数据"></a>删除不需观测的数据</h3><p>第一步，从数据集中去除不需观测的数据，包括重复，或者不相关数据，所谓不相关，就是与当前解决的特定问题无关。</p>
<h3 id="结构错误"><a href="#结构错误" class="headerlink" title="结构错误"></a>结构错误</h3><p>结构错误，指在测量，数据传输或其他形式的内部管理中出现的错误。例如，数据格式，拼写错误，大小写不一致等等。</p>
<h3 id="异常值"><a href="#异常值" class="headerlink" title="异常值"></a>异常值</h3><p>识别连续变量异常值，通常可以用：</p>
<ul>
<li>箱形图</li>
<li>Z记分法（当变量符合正态分布，Z&gt;3通常标记为异常值）<br>有充分理由说明其是异常值，才可以将其删除，否则，尽量保留。</li>
</ul>
<h3 id="缺失值"><a href="#缺失值" class="headerlink" title="缺失值"></a>缺失值</h3><p>缺失值，在机器学习中是非常棘手的事情。最常规的2种套路，其一，当数据量很充足的时候，删除没太大影响，直接Dropping；其二，利用其它观测对象重新填补缺失，填补的方式有：</p>
<ul>
<li>特殊值填补（中位数、平均值）</li>
<li>聚类均值填补（找出有缺失值的观测所属的簇）</li>
</ul>
<p>最好的处理<strong>缺失类别数据</strong>的方式就是，直接归为<strong>缺失类</strong>。<br><strong>缺失数值型数据</strong>，要么填补它，要么flag它<strong>NaN</strong>，处理方式，还需要考虑你套用的算法，是否支持。</p>
<p>完成数据清洗步骤后，算是解决机器学习的一大麻烦啦~</p>
]]></content>
    
    <summary type="html">
    
      &lt;blockquote class=&quot;blockquote-center&quot;&gt;&lt;p&gt;Garbage in gets you garbage out.&lt;br&gt;Better data beats fancier algorithms.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&quot;简介&quot;&gt;&lt;a href=&quot;#简介&quot; class=&quot;headerlink&quot; title=&quot;简介&quot;&gt;&lt;/a&gt;&lt;center&gt;简介&lt;/center&gt;&lt;/h3&gt;&lt;p&gt;数据清洗是每个数据人的必备技能，诚然这不是机器学习最‘骨感’的地方，没有什么隐藏的技能值得来说，但这确是最耗时的环节。不同类型数据需要不同处理方式，本文是自己系统学习数据清洗的起点。&lt;/p&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="http://www.phoebepan.cn/categories/Machine-Learning/"/>
    
    
      <category term="ML" scheme="http://www.phoebepan.cn/tags/ML/"/>
    
  </entry>
  
  <entry>
    <title>机器学习WorkFlow——EDA</title>
    <link href="http://www.phoebepan.cn/2017/04/26/ML_EDA/"/>
    <id>http://www.phoebepan.cn/2017/04/26/ML_EDA/</id>
    <published>2017-04-26T07:30:16.000Z</published>
    <updated>2017-06-09T00:02:52.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote class="blockquote-center"><h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p>机器学习想要获得很好的效果，就像做菜一样，离不开3个基本要素：好的厨师(human guidance)、干净的食材(clean, relevant data)、适量(avoid overfitting)。围绕这3要素，机器学习有下图5个核心步骤。本文先谈谈EDA过程。<br><img src="/images/machine_pipeline.JPG" alt="machine_pipeline"></p>
</blockquote>
<a id="more"></a>
<h3 id="Exploratory-Data-Analysis-EDA"><a href="#Exploratory-Data-Analysis-EDA" class="headerlink" title="Exploratory Data Analysis(EDA)"></a>Exploratory Data Analysis(EDA)</h3><p>探索性数据分析阶段，目的就是更好地了解你的数据，对你的数据有那么点 <strong>feel</strong>，这对下面的数据清洗、特征工程都是有帮助的。EDA对于ML来说，要<strong>快速，高效，决定性的</strong>。<br>这一步中，首先需要回答出这些问题：</p>
<ul>
<li>观测数据、特征有多少？</li>
<li>特征的数据类型？数值 or 类别？</li>
<li>是否有目标变量？</li>
<li>哪些列是有意义的？列中值是否有意义？值占比合适？</li>
<li>缺失值严重？</li>
</ul>
<h3 id="数值型特征分布"><a href="#数值型特征分布" class="headerlink" title="数值型特征分布"></a>数值型特征分布</h3><p><strong>直方图(histograms)</strong>就可以很好的观测出<strong>数值型features</strong>的分布。主要关注这几点：</p>
<ul>
<li>分布合理？</li>
<li>二值？</li>
<li>有潜在异常值？</li>
<li>有潜在测试误差？</li>
<li><p>边界合理？</p>
<p>如果发现数据有异常，可以咨询相关人士，从而更清楚的认识你的数据。</p>
</li>
</ul>
<h3 id="类别型特征分布"><a href="#类别型特征分布" class="headerlink" title="类别型特征分布"></a>类别型特征分布</h3><p><strong>条形图(Bar plot)</strong>可以有效观测<strong>类别features</strong>的分布。对于稀缺类别，后续可以考虑进行合并等等处理。</p>
<p><strong>盒图(Box plot)</strong>有效的观测<strong>类别feature</strong>与<strong>数值型feature</strong>之间的关系。</p>
<h3 id="协方差矩阵"><a href="#协方差矩阵" class="headerlink" title="协方差矩阵"></a>协方差矩阵</h3><p>数值型特征之间协方差矩阵，潜在的反应它们的关系。<strong>热图(heatmaps)</strong>可以很好的可视化，这样的相关性。在这你可以找出那些特征与目标变量相关性更强？强相关的特征有意义？</p>
<h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><p>总之，EDA，能够让我们更好的了解数据集，给下面的数据清洗，特征工程提供思路。</p>
]]></content>
    
    <summary type="html">
    
      &lt;blockquote class=&quot;blockquote-center&quot;&gt;&lt;h3 id=&quot;简介&quot;&gt;&lt;a href=&quot;#简介&quot; class=&quot;headerlink&quot; title=&quot;简介&quot;&gt;&lt;/a&gt;简介&lt;/h3&gt;&lt;p&gt;机器学习想要获得很好的效果，就像做菜一样，离不开3个基本要素：好的厨师(human guidance)、干净的食材(clean, relevant data)、适量(avoid overfitting)。围绕这3要素，机器学习有下图5个核心步骤。本文先谈谈EDA过程。&lt;br&gt;&lt;img src=&quot;/images/machine_pipeline.JPG&quot; alt=&quot;machine_pipeline&quot;&gt;&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="http://www.phoebepan.cn/categories/Machine-Learning/"/>
    
    
      <category term="ML" scheme="http://www.phoebepan.cn/tags/ML/"/>
    
  </entry>
  
  <entry>
    <title>Python实现大文件分割</title>
    <link href="http://www.phoebepan.cn/2017/04/25/split_bigtxt/"/>
    <id>http://www.phoebepan.cn/2017/04/25/split_bigtxt/</id>
    <published>2017-04-25T07:30:16.000Z</published>
    <updated>2017-06-04T23:59:50.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote class="blockquote-center"><h3 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h3><p>最近处理数据，碰到有一个按行存储的文本文件，含有七百多万条数据，大小近7G，本地直接报出<code>MemoryError</code>，处理不了。</p>
<h3 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a>解决方法</h3><p>用python写个脚本，将大文件拆分成小文件，对小文件进行处理，问题不就解决了么。</p>
</blockquote>
<a id="more"></a>
<h3 id="分割脚本"><a href="#分割脚本" class="headerlink" title="分割脚本"></a>分割脚本</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> os</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">split</span><span class="params">(fromfile,todir,chunksize=<span class="number">22</span>*<span class="number">10000</span>)</span>:</span></div><div class="line">	<span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(todir):<span class="comment">#check whether todir exists or not</span></div><div class="line">        os.mkdir(todir)</div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        <span class="keyword">for</span> fname <span class="keyword">in</span> os.listdir(todir):</div><div class="line">            os.remove(os.path.join(todir,fname))</div><div class="line">    partnum = <span class="number">0</span></div><div class="line">    inputfile = open(fromfile,<span class="string">'rb'</span>)<span class="comment">#open the fromfile</span></div><div class="line">    <span class="keyword">while</span> <span class="keyword">True</span>:</div><div class="line">        partnum += <span class="number">1</span></div><div class="line">        filename = os.path.join(todir,(<span class="string">'part-%04d'</span> %partnum))</div><div class="line">        fileobj = open(filename,<span class="string">'wb'</span>)<span class="comment">#make partfile</span></div><div class="line">        linenum = <span class="number">0</span></div><div class="line">        <span class="keyword">while</span>( linenum&lt;= batchSize ):</div><div class="line">            read_content = inputfile.readline()</div><div class="line">            <span class="keyword">if</span>( read_content ):</div><div class="line">                fileobj.write(read_content)</div><div class="line">            <span class="keyword">else</span>:</div><div class="line">                <span class="keyword">break</span></div><div class="line">            linenum += <span class="number">1</span></div><div class="line">        fileobj.close()</div><div class="line">        <span class="keyword">if</span> (linenum &lt; batchSize): <span class="comment">#last part file</span></div><div class="line">            <span class="keyword">break</span></div><div class="line">    <span class="keyword">return</span> partnum</div></pre></td></tr></table></figure>
<p>上面的脚本，就能够就将大文件，分割成小文件，且每个小文件含有220001行记录，大小约为178M，在todir目录看到分割后的文件如下：<br><img src="/images/partfile.png" alt="partfile" title="分割后部分文件"></p>
<p>欢迎大神，推荐更棒的方法！</p>
]]></content>
    
    <summary type="html">
    
      &lt;blockquote class=&quot;blockquote-center&quot;&gt;&lt;h3 id=&quot;问题&quot;&gt;&lt;a href=&quot;#问题&quot; class=&quot;headerlink&quot; title=&quot;问题&quot;&gt;&lt;/a&gt;问题&lt;/h3&gt;&lt;p&gt;最近处理数据，碰到有一个按行存储的文本文件，含有七百多万条数据，大小近7G，本地直接报出&lt;code&gt;MemoryError&lt;/code&gt;，处理不了。&lt;/p&gt;
&lt;h3 id=&quot;解决方法&quot;&gt;&lt;a href=&quot;#解决方法&quot; class=&quot;headerlink&quot; title=&quot;解决方法&quot;&gt;&lt;/a&gt;解决方法&lt;/h3&gt;&lt;p&gt;用python写个脚本，将大文件拆分成小文件，对小文件进行处理，问题不就解决了么。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="Python" scheme="http://www.phoebepan.cn/categories/Python/"/>
    
    
      <category term="Python" scheme="http://www.phoebepan.cn/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>魔力Python——pickle持久化对象</title>
    <link href="http://www.phoebepan.cn/2017/04/22/pickle/"/>
    <id>http://www.phoebepan.cn/2017/04/22/pickle/</id>
    <published>2017-04-22T07:30:16.000Z</published>
    <updated>2017-06-02T23:53:42.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote class="blockquote-center"><p><center><strong>简介</strong></center><br>本文主要介绍Python pickle模块的用法，实例分析<code>pickle</code>模块的功能与相关适用技巧。<code>pickle</code>提供了一个简单的<strong>持久化</strong>功能，可以将对象以文件的形式存放在磁盘上，方便多次读取与共享。</p>
</blockquote>
<a id="more"></a>
<h3 id="具体用法"><a href="#具体用法" class="headerlink" title="具体用法"></a>具体用法</h3><h4 id="pickle-dump-obj-file-protocol"><a href="#pickle-dump-obj-file-protocol" class="headerlink" title="pickle.dump(obj,file[,protocol])"></a>pickle.dump(obj,file[,protocol])</h4><p>这条语句序列化对象obj，并将结果数据流写入到文件file中。可选参数protocol是序列化模式，默认为0，表示以文本形式序列化，如果设置为 1 或 True，则以高压缩的二进制格式保存序列化后的对象，否则以ASCII格式保存。</p>
<h4 id="pickle-load-file"><a href="#pickle-load-file" class="headerlink" title="pickle.load(file)"></a>pickle.load(file)</h4><p>反序列化对象，将文件中的数据解析为一个python对象。</p>
<h3 id="基本用例"><a href="#基本用例" class="headerlink" title="基本用例"></a>基本用例</h3><p>下面这段代码，实现将4维矩阵obj对象持久化：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> pickle</div><div class="line"><span class="keyword">import</span> os</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">if</span> os.path.exists(<span class="string">'./test.pkl'</span>):    <span class="comment">#当前路径下是否已经存在序列化的test.pkl</span></div><div class="line">    obj=pickle.load(open(<span class="string">'./test.pkl'</span>,<span class="string">'rb'</span>))    <span class="comment">#如果已经存在直接load</span></div><div class="line"><span class="keyword">else</span>:   <span class="comment">#否则，将4维矩阵序列化到存储到当前路径下的test.pkl中</span></div><div class="line">    obj = np.ones((<span class="number">4</span>,<span class="number">4</span>))    </div><div class="line">    pickle.dump(obj,open(<span class="string">'./test.pkl'</span>,<span class="string">'wb'</span>))</div><div class="line">print(obj)</div></pre></td></tr></table></figure></p>
]]></content>
    
    <summary type="html">
    
      &lt;blockquote class=&quot;blockquote-center&quot;&gt;&lt;p&gt;&lt;center&gt;&lt;strong&gt;简介&lt;/strong&gt;&lt;/center&gt;&lt;br&gt;本文主要介绍Python pickle模块的用法，实例分析&lt;code&gt;pickle&lt;/code&gt;模块的功能与相关适用技巧。&lt;code&gt;pickle&lt;/code&gt;提供了一个简单的&lt;strong&gt;持久化&lt;/strong&gt;功能，可以将对象以文件的形式存放在磁盘上，方便多次读取与共享。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="Python" scheme="http://www.phoebepan.cn/categories/Python/"/>
    
    
      <category term="Python" scheme="http://www.phoebepan.cn/tags/Python/"/>
    
  </entry>
  
</feed>
