<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>技忆</title>
  <subtitle>Phoebe&#39;s little progress</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://www.phoebepan.cn/"/>
  <updated>2017-08-12T02:41:42.000Z</updated>
  <id>http://www.phoebepan.cn/</id>
  
  <author>
    <name>Phoebe</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>搜索广告</title>
    <link href="http://www.phoebepan.cn/2017/08/02/searchad/"/>
    <id>http://www.phoebepan.cn/2017/08/02/searchad/</id>
    <published>2017-08-02T07:30:16.000Z</published>
    <updated>2017-08-12T02:41:42.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote class="blockquote-center"><h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p>搜索广告指，在搜索过程中，搜索引擎推送给我们的互联网广告。参考Google财报，绝大部分收入来自于搜索广告。一般来说，当用户输入一个查询后，广告系统会经过：广告检索、广告排序、流量分配，三个模块为用户提供广告。</p>
</blockquote>
<a id="more"></a>
<h3 id="思维导图"><a href="#思维导图" class="headerlink" title="思维导图"></a>思维导图</h3><p><img src="/images/searchad.png" alt="searchad"></p>
<h3 id="在线学习"><a href="#在线学习" class="headerlink" title="在线学习"></a>在线学习</h3><p>模型获得一个训练样本<x,y>，利用一个迭代方法更新模型变量，使得当前期望loss最小。</x,y></p>
<blockquote>
<p>实际使用中，特征向量高维稀疏性，需要采用特征缩减技术进行特征稀疏化处理，可使用L1泛数加入目标函数。<br>工业界需要CTR预估模型具有自适应性，能够迅速适应数据变化。如，逻辑回归模型采用随机梯度下降法就具备在线学习能力。</p>
</blockquote>
<p>SGD简单易行，但很难得到特征向量稀疏结果，且精度低，收敛慢，Google提出的FTRL-Proximal方法可以得到稀疏性更好地训练结果。FTRL算法融合了RDA算法能产生稀疏模型的特性和SGD算法能产生更有效模型的特性，在处理诸如LR之类的非光滑正则化项的凸优化问题上性能更出色。算法详细理解参考博文<sup>1</sup>。<br><img src="/images/ftrl.png" alt="ftrl"></p>
<p>除了运用L1正则化降低特征维度，其他常见的方法也可以降低特征维度，比如，</p>
<ul>
<li>泊松选择法，不同特征表中的新特征以P的概率接纳其进入特征表。</li>
</ul>
<p>参考文献：</p>
<ol>
<li><a href="http://vividfree.github.io/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/2015/12/05/understanding-FTRL-algorithm" target="_blank" rel="external">理解FTRL算法</a></li>
<li>《互联网广告算法和系统实践》</li>
</ol>
]]></content>
    
    <summary type="html">
    
      &lt;blockquote class=&quot;blockquote-center&quot;&gt;&lt;h3 id=&quot;简介&quot;&gt;&lt;a href=&quot;#简介&quot; class=&quot;headerlink&quot; title=&quot;简介&quot;&gt;&lt;/a&gt;简介&lt;/h3&gt;&lt;p&gt;搜索广告指，在搜索过程中，搜索引擎推送给我们的互联网广告。参考Google财报，绝大部分收入来自于搜索广告。一般来说，当用户输入一个查询后，广告系统会经过：广告检索、广告排序、流量分配，三个模块为用户提供广告。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="ComputationalAdvertising" scheme="http://www.phoebepan.cn/categories/ComputationalAdvertising/"/>
    
    
      <category term="计算广告" scheme="http://www.phoebepan.cn/tags/%E8%AE%A1%E7%AE%97%E5%B9%BF%E5%91%8A/"/>
    
  </entry>
  
  <entry>
    <title>互联网广告简介</title>
    <link href="http://www.phoebepan.cn/2017/08/01/AdvertisingSummary/"/>
    <id>http://www.phoebepan.cn/2017/08/01/AdvertisingSummary/</id>
    <published>2017-08-01T07:30:16.000Z</published>
    <updated>2017-08-12T01:16:36.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote class="blockquote-center"><p>广告是由已确定的出资人通过各种媒介进行的有关产品（商品、服务和观点）的、有偿的、有组织的、综合的、劝服性的非人员的信息传播活动。<br>——William F.Arens</p>
</blockquote>
<p><code>Stay Hungry, Stay Foolish.</code> 从这篇笔记开始学习计算广告。<br><a id="more"></a></p>
<h3 id="角色"><a href="#角色" class="headerlink" title="角色"></a>角色</h3><p>广告主、媒体、受众；三者博弈的生态系统。</p>
<h3 id="广告类型"><a href="#广告类型" class="headerlink" title="广告类型"></a>广告类型</h3><ul>
<li>条幅广告</li>
<li>邮件直接营销广告</li>
<li>富媒体广告</li>
<li>视频广告</li>
<li>文字链广告</li>
<li>社交广告</li>
<li>移动端广告</li>
</ul>
<h3 id="系统架构"><a href="#系统架构" class="headerlink" title="系统架构"></a>系统架构</h3><ul>
<li>前端引擎</li>
<li>检索引擎</li>
<li>实时点击率预估服务</li>
<li>广告主操作消息更新服务</li>
<li>用户行为数据收集和更新系统</li>
<li>特征提取和行为分析</li>
<li>反作弊系统</li>
<li>广告主后台(建立投放计划、增加投放创意、出价、设定投放参数、阅读报表等)</li>
<li>存储系统</li>
<li>计算系统(人群属性、意图挖掘；多特征任务)</li>
</ul>
<h3 id="机制设计"><a href="#机制设计" class="headerlink" title="机制设计"></a>机制设计</h3><blockquote>
<p>在经济学中，机制设计所讨论的问题是：在给定一个社会目标或者经济目标，以及自由选择、自愿交换的分散化决策条件下，能否并且怎样设计一个经济机制（包括制约条件、资源配置等），使得参与者的个人利益和设计者既定的目标一致。</p>
</blockquote>
<h4 id="广告机制设计"><a href="#广告机制设计" class="headerlink" title="广告机制设计"></a>广告机制设计</h4><blockquote>
<p>主要研究的是，如何针对不同广告受众，将广告平台上有限的展现位置分配给不同的广告，以达到某种既定的利益目标。</p>
</blockquote>
<h3 id="优化目标"><a href="#优化目标" class="headerlink" title="优化目标"></a>优化目标</h3><p>互联网广告算法的核心问题，是根据用户、环境、广告的全部有效信息，找到最合适的投放策略和模型，兼顾浏览者、广告主、广告平台的最大利益，并不断调整。</p>
<h3 id="思维导图"><a href="#思维导图" class="headerlink" title="思维导图"></a>思维导图</h3><p><img src="/images/advertising.png" alt="advertising"></p>
<p>参考书目：《互联网广告算法和系统实践》</p>
]]></content>
    
    <summary type="html">
    
      &lt;blockquote class=&quot;blockquote-center&quot;&gt;&lt;p&gt;广告是由已确定的出资人通过各种媒介进行的有关产品（商品、服务和观点）的、有偿的、有组织的、综合的、劝服性的非人员的信息传播活动。&lt;br&gt;——William F.Arens&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;code&gt;Stay Hungry, Stay Foolish.&lt;/code&gt; 从这篇笔记开始学习计算广告。&lt;br&gt;
    
    </summary>
    
      <category term="ComputationalAdvertising" scheme="http://www.phoebepan.cn/categories/ComputationalAdvertising/"/>
    
    
      <category term="计算广告" scheme="http://www.phoebepan.cn/tags/%E8%AE%A1%E7%AE%97%E5%B9%BF%E5%91%8A/"/>
    
  </entry>
  
  <entry>
    <title>python中time模块</title>
    <link href="http://www.phoebepan.cn/2017/07/16/time/"/>
    <id>http://www.phoebepan.cn/2017/07/16/time/</id>
    <published>2017-07-16T07:30:16.000Z</published>
    <updated>2017-07-16T04:40:44.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote class="blockquote-center"><h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p>在日常数据处理中，常常需要与时间打交道，python中与时间处理有关的模块有：<code>time</code>，<code>datetime</code>，<code>calendar</code>。本文主要介绍<strong>time</strong>模块。<br><img src="/images/time_convert.png" alt="time"></p>
</blockquote>
<a id="more"></a>
<p>Python中，表示时间的方式有：</p>
<ul>
<li>时间戳：通常来说，表示的是从<strong>1970年1月1日00:00:00</strong>开始按秒计算的偏移量；</li>
<li>格式化的时间字符串；</li>
<li>元组(struct_time)。</li>
</ul>
<p>time模块常用的几个函数：</p>
<h3 id="time-localtime"><a href="#time-localtime" class="headerlink" title="time.localtime()"></a>time.localtime()</h3><p>将一个时间戳转换成当前时区的struct_time。</p>
<h3 id="time-time"><a href="#time-time" class="headerlink" title="time.time()"></a>time.time()</h3><p>返回当前时间的时间戳。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt;time.time()</div><div class="line"><span class="number">1500176454.689554</span></div></pre></td></tr></table></figure></p>
<h3 id="time-mktime"><a href="#time-mktime" class="headerlink" title="time.mktime()"></a>time.mktime()</h3><p>将一个struct_time转化为时间戳<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt;time.mktime(time.localtime())</div><div class="line"><span class="number">1500176622.0</span></div></pre></td></tr></table></figure></p>
<h3 id="time-sleep"><a href="#time-sleep" class="headerlink" title="time.sleep()"></a>time.sleep()</h3><p>线程推迟运行，单位为秒</p>
<h3 id="time-strftime"><a href="#time-strftime" class="headerlink" title="time.strftime()"></a>time.strftime()</h3><p>把一个代表时间的元组转化为格式化的时间字符串。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt;time.strftime(<span class="string">'%Y-%m-%d %X'</span>,time.localtime())</div><div class="line"><span class="string">'2017-07-16 11:58:07'</span></div></pre></td></tr></table></figure></p>
<h3 id="time-strptime"><a href="#time-strptime" class="headerlink" title="time.strptime()"></a>time.strptime()</h3><p>格式化时间字符串转化成struct_time。与strftime()操作互逆。<br><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt;time.strptime('2017-07-16 11:58:07','%Y-%m-%d %X'')</div><div class="line">time.struct_time(tm_year=2017, tm_mon=7, tm_mday=16, tm_hour=11, tm_min=58, tm_sec=7, tm_wday=6, tm_yday=197, tm_isdst=-1)</div></pre></td></tr></table></figure></p>
<p>了解更多，请参考<a href="https://docs.python.org/3/library/time.html" target="_blank" rel="external">time模块的官方文档</a>。</p>
]]></content>
    
    <summary type="html">
    
      &lt;blockquote class=&quot;blockquote-center&quot;&gt;&lt;h3 id=&quot;简介&quot;&gt;&lt;a href=&quot;#简介&quot; class=&quot;headerlink&quot; title=&quot;简介&quot;&gt;&lt;/a&gt;简介&lt;/h3&gt;&lt;p&gt;在日常数据处理中，常常需要与时间打交道，python中与时间处理有关的模块有：&lt;code&gt;time&lt;/code&gt;，&lt;code&gt;datetime&lt;/code&gt;，&lt;code&gt;calendar&lt;/code&gt;。本文主要介绍&lt;strong&gt;time&lt;/strong&gt;模块。&lt;br&gt;&lt;img src=&quot;/images/time_convert.png&quot; alt=&quot;time&quot;&gt;&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="Python" scheme="http://www.phoebepan.cn/categories/Python/"/>
    
    
      <category term="Python" scheme="http://www.phoebepan.cn/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>调超参神器——GridSearchCV</title>
    <link href="http://www.phoebepan.cn/2017/06/16/GridSearchCV/"/>
    <id>http://www.phoebepan.cn/2017/06/16/GridSearchCV/</id>
    <published>2017-06-16T07:30:16.000Z</published>
    <updated>2017-07-19T02:41:47.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote class="blockquote-center"><h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p>所谓超参，就是机器学习算法中，不能通过自身学习设定的参数，如SVM的惩罚因子C，核函数kernel，gamma参数等，参数间的组合很是繁琐，人工调节这些超参数时间成本太高，易出错。本文主要介绍sklearn模块的调参神器<code>GridSearchCV</code>模块，它能够在指定范围内自动搜索具有不同超参数的不同模型组合，寻找最佳参数，大大提高调参效率。</p>
</blockquote>
<a id="more"></a>
<h3 id="缘起"><a href="#缘起" class="headerlink" title="缘起"></a>缘起</h3><p>这两天，闲来参加下Ctrip的一个数据竞赛，model选择的是XGboost，好用是自然，但是参数有很多，最迫切需要一个自动调节参数工具，于是接触到GridSearchCV模块。</p>
<h3 id="官方手册"><a href="#官方手册" class="headerlink" title="官方手册"></a>官方手册</h3><p><a href="http://scikit-learn.org/stable/modules/generated/sklearn.grid_search.GridSearchCV.html#sklearn.grid_search.GridSearchCV" target="_blank" rel="external">手册链接</a></p>
<h3 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h3><p>自己训练的代码如下(XGboost+5-fold Cross Validation)，清晰易懂，无须解释。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> xgboost <span class="keyword">as</span> xgb</div><div class="line"><span class="keyword">from</span> sklearn.grid_search <span class="keyword">import</span> GridSearchCV</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">xgbmodel_train</span><span class="params">(train)</span>:</span></div><div class="line">    xgb_model = xgb.XGBClassifier()</div><div class="line">    train_feature, train_label = train.drop(<span class="string">'orderlabel'</span>, axis=<span class="number">1</span>), train[<span class="string">'orderlabel'</span>]</div><div class="line"></div><div class="line">    parameters = &#123;<span class="string">'nthread'</span>: [<span class="number">4</span>],</div><div class="line">                  <span class="string">'objective'</span>: [<span class="string">'binary:logistic'</span>],</div><div class="line">                  <span class="string">'learning_rate'</span>: [<span class="number">0.05</span>,<span class="number">0.06</span>,<span class="number">0.1</span>],</div><div class="line">                  <span class="string">'max_depth'</span>: [<span class="number">5</span>, <span class="number">6</span>],</div><div class="line">                  <span class="string">'min_child_weight'</span>: [<span class="number">1</span>, <span class="number">3</span>],</div><div class="line">                  <span class="string">'silent'</span>: [<span class="number">1</span>],</div><div class="line">                  <span class="string">'gamma'</span>: [<span class="number">0</span>, <span class="number">0.1</span>],</div><div class="line">                  <span class="string">'subsample'</span>: [<span class="number">0.6</span>, <span class="number">0.7</span>, <span class="number">0.8</span>],</div><div class="line">                  <span class="string">'colsample_bytree'</span>: [<span class="number">0.7</span>, <span class="number">0.5</span>, <span class="number">0.6</span>],</div><div class="line">                  <span class="string">'n_estimators'</span>: [<span class="number">5</span>],</div><div class="line">                  <span class="string">'missing'</span>: [<span class="number">-999</span>],</div><div class="line">                  <span class="string">'seed'</span>: [<span class="number">12455</span>]&#125;</div><div class="line"></div><div class="line">    clf = GridSearchCV(xgb_model, parameters, n_jobs=<span class="number">1</span>,</div><div class="line">                       cv=StratifiedKFold(train[<span class="string">'orderlabel'</span>], n_folds=<span class="number">5</span>, shuffle=<span class="keyword">True</span>),</div><div class="line">                       scoring=<span class="string">'roc_auc'</span>,</div><div class="line">                       verbose=<span class="number">2</span>, refit=<span class="keyword">True</span>)</div><div class="line"></div><div class="line"></div><div class="line">    clf.fit(train_feature, train_label)</div><div class="line">   </div><div class="line">    best_parameters, score, _ = max(clf.grid_scores_, key=<span class="keyword">lambda</span> x: x[<span class="number">1</span>])</div><div class="line">    print(<span class="string">'AUC score:'</span>, score)</div><div class="line">    <span class="keyword">for</span> param_name <span class="keyword">in</span> sorted(best_parameters.keys()):</div><div class="line">        print(<span class="string">'%s: %r'</span> % (param_name, best_parameters[param_name]))</div></pre></td></tr></table></figure></p>
<h3 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h3><p><img src="/images/process.png" alt="process"><br><img src="/images/best_score.png" alt="best para"></p>
]]></content>
    
    <summary type="html">
    
      &lt;blockquote class=&quot;blockquote-center&quot;&gt;&lt;h3 id=&quot;简介&quot;&gt;&lt;a href=&quot;#简介&quot; class=&quot;headerlink&quot; title=&quot;简介&quot;&gt;&lt;/a&gt;简介&lt;/h3&gt;&lt;p&gt;所谓超参，就是机器学习算法中，不能通过自身学习设定的参数，如SVM的惩罚因子C，核函数kernel，gamma参数等，参数间的组合很是繁琐，人工调节这些超参数时间成本太高，易出错。本文主要介绍sklearn模块的调参神器&lt;code&gt;GridSearchCV&lt;/code&gt;模块，它能够在指定范围内自动搜索具有不同超参数的不同模型组合，寻找最佳参数，大大提高调参效率。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="http://www.phoebepan.cn/categories/Machine-Learning/"/>
    
    
      <category term="ML" scheme="http://www.phoebepan.cn/tags/ML/"/>
    
      <category term="Tune" scheme="http://www.phoebepan.cn/tags/Tune/"/>
    
  </entry>
  
  <entry>
    <title>XGBoost调参指南</title>
    <link href="http://www.phoebepan.cn/2017/06/14/xgb_paras/"/>
    <id>http://www.phoebepan.cn/2017/06/14/xgb_paras/</id>
    <published>2017-06-14T07:30:16.000Z</published>
    <updated>2017-07-21T05:17:53.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote class="blockquote-center"><h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p>在预测分析建模时，如果结果不理想，那么不妨试试XGBoost，可以说，XGBoost已经成为许多数据科学家的秘密武器，，本文翻译自<a href="https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/" target="_blank" rel="external">Complete Guide to Parameter Tuning in XGBoost (with codes in Python)</a>，它详细介绍了XGBoost中参数的含义以及通过实例说明调参的技艺。在此，记下自己的学习笔记。</p>
</blockquote>
<a id="more"></a>
<h3 id="XGBoost"><a href="#XGBoost" class="headerlink" title="XGBoost"></a>XGBoost</h3><p>XGBoost是Gradient Boosting算法的一个优化版本，由于XGBoost算法的内部复杂性，涉及很多超参数，相比于建立一个XGBoost model，提高该model的性能需要花费很大的精力，但这又是必须做的。</p>
<h4 id="优势"><a href="#优势" class="headerlink" title="优势"></a>优势</h4><ul>
<li><strong>正则化</strong>，在GBM基础之上，增加了正则化，有效降低过拟合风险；</li>
<li><strong>并行处理</strong>，Boosting算法是顺序处理的，比起GBM，它快的惊人，另外，XGBoost也支持Hadoop实现；</li>
<li><strong>高度灵活性</strong>，允许自定义优化目标和评价标准；</li>
<li><strong>缺失值处理</strong>，内置处理缺失值的规则，提供一个特殊值作为参数传进去；</li>
<li><strong>剪枝</strong>，GBM实则是一贪心算法，遇到负损失，就停止分裂，XGBoost根据指定最大深度(max_depth)，回过头来剪枝。如果某节点之后没有正值，它会去除这一分裂；</li>
<li><strong>内置交叉验证</strong>，允许在每一轮boosting迭代中使用交叉验证，方便获得最优迭代次数；</li>
<li><strong>上一轮结果基础上继续训练</strong></li>
</ul>
<h3 id="参数"><a href="#参数" class="headerlink" title="参数"></a>参数</h3><p>XGBoost的作者把所有参数分成3类：通用参数、Booster参数、学习目标参数。</p>
<h4 id="通用参数"><a href="#通用参数" class="headerlink" title="通用参数"></a>通用参数</h4><p>控制XGBoost的宏观功能。有：</p>
<ul>
<li><code>booster</code>，选择每次迭代的模型，默认gbtree；</li>
<li><code>silent</code>，默认为0，设为1，则不会输出任何信息；</li>
<li><code>nthread</code>，最大可能的线程数，用来进行多线程控制。</li>
</ul>
<h4 id="Booster参数"><a href="#Booster参数" class="headerlink" title="Booster参数"></a>Booster参数</h4><p>这里只介绍tree booster参数，有：</p>
<ul>
<li><code>eta</code>，学习率，每一步减少权重，提高模型鲁棒性。通常设置在0.01-0.2范围内；</li>
<li><code>min_child_weight</code>，最小叶子节点样本权重和，避免过拟合，值过高会导致欠拟合，可以通过CV调节该参数；</li>
<li><code>max_depth</code>，树最大深度，避免过拟合，值越大，模型可学到更具体更局部样本，CV调节该参数，通常设在3-10区间内；</li>
<li><code>max_leaf_nodes</code>，树上叶子节点最大数量；</li>
<li><code>gamma</code>，默认为0，节点分裂时，只有分裂后损失函数值下降了，才会分裂该节点，该参数指定节点分裂损失函数最小下降值，值越大，越保守；</li>
<li><code>subsample</code>，控制每棵树随机采样比例，值越小越保守，避免过拟合，值过小，可能欠拟合，通常取值范围，0.5-1；</li>
<li><code>colsample_bytree</code>，默认1，对列数采样比例；</li>
<li><code>lambda</code>，默认1，L2正则化的权重，控制正则化部分，减少过拟合；</li>
<li><code>scale_pos_weight</code>，默认1，在类别十分不均衡时，该参数设置成正值，可以是model更快收敛。</li>
</ul>
<h4 id="学习目标参数"><a href="#学习目标参数" class="headerlink" title="学习目标参数"></a>学习目标参数</h4><p>控制理想的优化目标，和每一步结果的度量，有：</p>
<ul>
<li><code>objective</code>，最小化的损失函数，常用值有binary:logistic（二分类的逻辑回归） 、multi:softmax （softmax多分类器，返回预测的类别）、multi:softprob（softmax多分类器，返回属于各个类别的概率）</li>
<li><code>eval_metric</code>，对于回归默认rmse，分类默认error，常用值，rmse，mae，logloss，error，merror，mlogloss，auc</li>
<li><code>seed</code>，随机数种子，复现随机数据结果。</li>
</ul>
<p>最后，还有两个重要参数<code>num_boosting_rounds</code>，<code>early_stopping_rounds</code>，可以控制迭代次数。<br>更多参数参考：<br><a href="http://xgboost.readthedocs.io/en/latest/model.html" target="_blank" rel="external">XGBoost Guide – Introduction to Boosted Trees</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;blockquote class=&quot;blockquote-center&quot;&gt;&lt;h3 id=&quot;简介&quot;&gt;&lt;a href=&quot;#简介&quot; class=&quot;headerlink&quot; title=&quot;简介&quot;&gt;&lt;/a&gt;简介&lt;/h3&gt;&lt;p&gt;在预测分析建模时，如果结果不理想，那么不妨试试XGBoost，可以说，XGBoost已经成为许多数据科学家的秘密武器，，本文翻译自&lt;a href=&quot;https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/&quot;&gt;Complete Guide to Parameter Tuning in XGBoost (with codes in Python)&lt;/a&gt;，它详细介绍了XGBoost中参数的含义以及通过实例说明调参的技艺。在此，记下自己的学习笔记。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="http://www.phoebepan.cn/categories/Machine-Learning/"/>
    
    
      <category term="ML" scheme="http://www.phoebepan.cn/tags/ML/"/>
    
      <category term="Tune" scheme="http://www.phoebepan.cn/tags/Tune/"/>
    
      <category term="Python" scheme="http://www.phoebepan.cn/tags/Python/"/>
    
      <category term="XGBoost" scheme="http://www.phoebepan.cn/tags/XGBoost/"/>
    
  </entry>
  
  <entry>
    <title>单机安装Spark开发环境</title>
    <link href="http://www.phoebepan.cn/2017/06/07/spark_1/"/>
    <id>http://www.phoebepan.cn/2017/06/07/spark_1/</id>
    <published>2017-06-07T07:30:16.000Z</published>
    <updated>2017-08-08T15:43:26.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote class="blockquote-center"><h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p>MapReduce在迭代计算和交互计算的任务上表现得效率低下，Spark从一开始就是为交互式查询和迭代算法设计的，同时还支持内存式存储和高效的容错机制，Spark的核心是一个对由很多计算任务组成的、运行在多个工作机器或者是一个计算集群上的应用进行调度、分发以及监控的计算引擎。<br>因Spark支持java、python等语言，尝试安装了python语言环境下的spark开发环境。本篇笔记是win10下配置过程记录。</p>
</blockquote>
<a id="more"></a>
<p>机器：Win10 64bit</p>
<h3 id="jdk安装"><a href="#jdk安装" class="headerlink" title="jdk安装"></a>jdk安装</h3><p>从Oracle网站上下载JDK。我装的<a href="http://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html" target="_blank" rel="external">JDK 1.8版本</a>。安装完新建系统环境变量<strong>JAVA_HOME</strong>，值为<code>D:\program files\Java\jdk1.8.0_144</code>(根据自己安装路径来)，系统变量<strong>Path</strong>下添加<code>%JAVA_HOME%\bin</code>和<code>%JAVA_HOME%\jre\bin</code>。</p>
<h3 id="spark环境变量配置"><a href="#spark环境变量配置" class="headerlink" title="spark环境变量配置"></a>spark环境变量配置</h3><p>从<a href="https://spark.apache.org/downloads.html" target="_blank" rel="external">spark网站</a>上下载最新版本spark，我下载的了与Hadoop2.6匹配的spark，文件名spark-2.2.0-bin-hadoop2.6.tgz，将安装文件解压到本地文件夹中（如：D:\spark，<strong>路径中不能有空格</strong>）。将<code>D:\spark\spark-2.2.0-bin-hadoop2.6\bin</code>添加到系统<strong>Path</strong>变量，同时新建<strong>SPARK_HOME</strong>变量，变量值为：<code>D:\spark\spark-2.2.0-bin-hadoop2.6</code>。</p>
<h3 id="hadoop工具包安装"><a href="#hadoop工具包安装" class="headerlink" title="hadoop工具包安装"></a>hadoop工具包安装</h3><p>spark是基于hadoop之上的，运行过程中会调用相关hadoop库，下载hadoop 2.6编译好的包，<a href="https://www.barik.net/archive/2015/01/19/172716/" target="_blank" rel="external">hadoop-2.6.0.tar.gz</a>。将安装文件解压到本地文件夹中(如：D:\program files\hadoop)，系统变量<strong>Path</strong>中添加<code>D:\program files\hadoop\hadoop-2.6.0\bin</code>；新建<strong>HADOOP_HOME</strong>系统变量,值为：<code>D:\program files\hadoop\hadoop-2.6.0</code>。</p>
<h3 id="python下spark开发环境搭建"><a href="#python下spark开发环境搭建" class="headerlink" title="python下spark开发环境搭建"></a>python下spark开发环境搭建</h3><p>将spark目录下的pyspark文件夹(D:\spark\spark-2.2.0-bin-hadoop2.6\python\pyspark)复制到python安装目录(D:\Program Files (x86)\python3\Lib\site-packages)里。</p>
<h3 id="验证spark安装正确性"><a href="#验证spark安装正确性" class="headerlink" title="验证spark安装正确性"></a>验证spark安装正确性</h3><p>cmd中输入：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">d:</div><div class="line">cd D:\spark\spark-2.2.0-bin-hadoop2.6</div><div class="line">bin\spark-shell</div></pre></td></tr></table></figure></p>
<p>运行如下命令启动Spark python shell，<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">d:</div><div class="line">cd D:\spark\spark-2.2.0-bin-hadoop2.6</div><div class="line">bin\pyshark</div></pre></td></tr></table></figure></p>
<p><strong>Tip：</strong>将目录<code>D:\spark\spark-2.2.0-bin-hadoop2.6\bin\pyspark2.cmd</code>的<code>PYSPARK_DRIVER_PYTHON=ipython</code>更改成ipython，就可以用ipython开始交互啦~<br>如果正确，控制台会输出如下信息：<br><img src="/images/success_spark.png" alt="success_spark"></p>
]]></content>
    
    <summary type="html">
    
      &lt;blockquote class=&quot;blockquote-center&quot;&gt;&lt;h3 id=&quot;简介&quot;&gt;&lt;a href=&quot;#简介&quot; class=&quot;headerlink&quot; title=&quot;简介&quot;&gt;&lt;/a&gt;简介&lt;/h3&gt;&lt;p&gt;MapReduce在迭代计算和交互计算的任务上表现得效率低下，Spark从一开始就是为交互式查询和迭代算法设计的，同时还支持内存式存储和高效的容错机制，Spark的核心是一个对由很多计算任务组成的、运行在多个工作机器或者是一个计算集群上的应用进行调度、分发以及监控的计算引擎。&lt;br&gt;因Spark支持java、python等语言，尝试安装了python语言环境下的spark开发环境。本篇笔记是win10下配置过程记录。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="Spark" scheme="http://www.phoebepan.cn/categories/Spark/"/>
    
    
      <category term="Spark" scheme="http://www.phoebepan.cn/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>数据可视化——Seaborn</title>
    <link href="http://www.phoebepan.cn/2017/06/06/learn_seaborn/"/>
    <id>http://www.phoebepan.cn/2017/06/06/learn_seaborn/</id>
    <published>2017-06-06T07:30:16.000Z</published>
    <updated>2017-06-28T15:14:42.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote class="blockquote-center"><h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p>EDA过程中，想要更了解你的数据，选择一个合适的可视化工具，可以说会让你的工作事半功倍。<br>本文主要介绍一个以matplotlib作为底层，更易上手的作图库<code>seaborn</code>。</p>
</blockquote>
<a id="more"></a>
<h3 id="Seaborn"><a href="#Seaborn" class="headerlink" title="Seaborn"></a>Seaborn</h3><p>基于matplotlib的可视化库，旨在使默认的数据可视化更加悦目，简化复杂图表创建，可以与pandas很好的集成。</p>
<h3 id="简易用法"><a href="#简易用法" class="headerlink" title="简易用法"></a>简易用法</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</div><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns <span class="comment">#一旦导入了seaborn，matplotlib的默认作图风格就会被覆盖成seaborn的格式</span></div><div class="line">%matplotlib inline </div><div class="line"><span class="comment">#在jupyter notebook里作图，需要用到这个命令</span></div></pre></td></tr></table></figure>
<h4 id="读取原始数据（这是一份红酒成分与口感评分数据）"><a href="#读取原始数据（这是一份红酒成分与口感评分数据）" class="headerlink" title="读取原始数据（这是一份红酒成分与口感评分数据）"></a>读取原始数据（这是一份红酒成分与口感评分数据）</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">winedata=pd.read_csv(<span class="string">'winequality-red.csv'</span>)</div><div class="line">winedata.head()</div></pre></td></tr></table></figure>
<p><img src="/images/winedata.png" alt="png"></p>
<h4 id="直方图——seaborn-distplot"><a href="#直方图——seaborn-distplot" class="headerlink" title="直方图——seaborn.distplot()"></a><strong>直方图</strong>——seaborn.distplot()</h4><p>如对上面的quality列做直方图，保留概率密度曲线<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">sns.distplot(winedata[<span class="string">'quality'</span>])   <span class="comment"># 不需要概率密度曲线直接将 kde=False 即可</span></div><div class="line">sns.set_style(<span class="string">'dark'</span>)    <span class="comment">#设置背景色</span></div><div class="line">sns.utils.axlabel(<span class="string">'Quality'</span>, <span class="string">'Frequency'</span>) <span class="comment">#设置X,Y坐标名</span></div><div class="line">sns.plt.show()</div></pre></td></tr></table></figure></p>
<p><img src="/images/output_5_0.png" alt="png"></p>
<h4 id="折线图"><a href="#折线图" class="headerlink" title="折线图"></a>折线图</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">sns.factorplot(data=winedata, x=<span class="string">'quality'</span>, y=<span class="string">'total sulfur dioxide'</span>,size=<span class="number">3</span>)</div><div class="line">sns.plt.show()</div></pre></td></tr></table></figure>
<p><img src="/images/output_7_0.png" alt="png"></p>
<h4 id="柱状图——seaborn-barplot"><a href="#柱状图——seaborn-barplot" class="headerlink" title="柱状图——seaborn.barplot()"></a>柱状图——seaborn.barplot()</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">sns.factorplot(data=winedata, x=<span class="string">'quality'</span>, y=<span class="string">'total sulfur dioxide'</span>,kind=<span class="string">'bar'</span>,size=<span class="number">3</span>)</div><div class="line"><span class="comment">#ax = sns.barplot(data=winedata, x='quality', y='total sulfur dioxide',ci=0)</span></div><div class="line">sns.plt.show()</div></pre></td></tr></table></figure>
<p><img src="/images/output_9_0.png" alt="png"></p>
<h4 id="散点图——seaborn-stripplot"><a href="#散点图——seaborn-stripplot" class="headerlink" title="散点图——seaborn.stripplot()"></a>散点图——seaborn.stripplot()</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">temp=sns.FacetGrid(winedata, hue=<span class="string">'quality'</span>, size=<span class="number">3</span>)   <span class="comment">#hue参数设置区分色彩列</span></div><div class="line">temp.map(plt.scatter, <span class="string">'volatile acidity'</span>, <span class="string">'alcohol'</span>)</div><div class="line">temp.add_legend()</div><div class="line">sns.plt.show()</div><div class="line"><span class="comment">#ax = sns.stripplot(x='quality', y='alcohol', data=winedata) #普通散点图</span></div><div class="line"><span class="comment">#ax = sns.stripplot(x='quality', y='alcohol', data=winedata, jitter=True) #带抖动的散点图</span></div><div class="line"><span class="comment">#sns.plt.show()</span></div></pre></td></tr></table></figure>
<p><img src="/images/output_11_0.png" alt="png"></p>
<h4 id="箱型图——seaborn-boxplot"><a href="#箱型图——seaborn-boxplot" class="headerlink" title="箱型图——seaborn.boxplot()"></a>箱型图——seaborn.boxplot()</h4><p>以quality为X轴，alcohol为Y轴，做出箱线图，可以看出异常值<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">ax=sns.boxplot(x=<span class="string">'quality'</span>, y=<span class="string">'alcohol'</span>, data=winedata)</div><div class="line">ax=sns.stripplot(x=<span class="string">'quality'</span>, y=<span class="string">'alcohol'</span>, data=winedata, jitter=<span class="keyword">True</span>, color=<span class="string">'.3'</span>)  <span class="comment">#加上点，jitter=True 使各个散点分开，要不然会是一条直线</span></div><div class="line">sns.plt.show()</div></pre></td></tr></table></figure></p>
<p><img src="/images/output_13_0.png" alt="png"></p>
<h4 id="小提琴图——seaborn-violinplot"><a href="#小提琴图——seaborn-violinplot" class="headerlink" title="小提琴图——seaborn.violinplot()"></a>小提琴图——seaborn.violinplot()</h4><p>可以看出密度分布<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">ax = sns.violinplot(x=<span class="string">'quality'</span>, y=<span class="string">'alcohol'</span>, data=winedata, size=<span class="number">5</span>)</div><div class="line">ax = sns.swarmplot(x=<span class="string">'quality'</span>, y=<span class="string">'alcohol'</span>, data=winedata,color=<span class="string">'.9'</span>)</div><div class="line">sns.plt.show()</div></pre></td></tr></table></figure></p>
<p><img src="/images/output_15_0.png" alt="png"></p>
<h4 id="多变量作图——seaborn-pairplot"><a href="#多变量作图——seaborn-pairplot" class="headerlink" title="多变量作图——seaborn.pairplot()"></a>多变量作图——seaborn.pairplot()</h4><p>seaborn可以一次性两两组合多个变量做出多个对比图，有n个变量，就会做出一个n × n个格子的图，相同的两个变量之间以直方图展示，不同的变量则以散点图展示，<strong>要注意的是数据中不能有NaN（缺失的数据），否则会报错。</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">sns.pairplot(winedata, vars=[<span class="string">'quality'</span>, <span class="string">'residual sugar'</span>,<span class="string">'alcohol'</span>],hue=<span class="string">'quality'</span>)</div><div class="line">sns.plt.show()</div></pre></td></tr></table></figure></p>
<p><img src="/images/output_17_0.png" alt="png"></p>
<h4 id="回归图——seaborn-lmplot-、seaborn-regplot"><a href="#回归图——seaborn-lmplot-、seaborn-regplot" class="headerlink" title="回归图——seaborn.lmplot()、seaborn.regplot()"></a>回归图——seaborn.lmplot()、seaborn.regplot()</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">sns.lmplot(x=<span class="string">'volatile acidity'</span>, y=<span class="string">'alcohol'</span>, data=winedata)   <span class="comment"># hue参数进行分组拟合，markers=['o', 'x']，col参数不同组的子图</span></div><div class="line">sns.plt.show()</div></pre></td></tr></table></figure>
<p><img src="/images/output_19_0.png" alt="png"><br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">sns.regplot(x=<span class="string">'fixed acidity'</span>, y=<span class="string">'alcohol'</span>, data=winedata)</div><div class="line">sns.plt.show()</div></pre></td></tr></table></figure></p>
<p><img src="/images/output_20_0.png" alt="png"></p>
<p><a href="http://seaborn.pydata.org/tutorial.html" target="_blank" rel="external">更多用法参考官方手册</a><br>点这查看本文<a href="https://github.com/phoebepx/normally-accumulate/blob/master/learn_seaborn.ipynb" target="_blank" rel="external">.ipynb文件</a>，欢迎纠错~</p>
]]></content>
    
    <summary type="html">
    
      &lt;blockquote class=&quot;blockquote-center&quot;&gt;&lt;h3 id=&quot;简介&quot;&gt;&lt;a href=&quot;#简介&quot; class=&quot;headerlink&quot; title=&quot;简介&quot;&gt;&lt;/a&gt;简介&lt;/h3&gt;&lt;p&gt;EDA过程中，想要更了解你的数据，选择一个合适的可视化工具，可以说会让你的工作事半功倍。&lt;br&gt;本文主要介绍一个以matplotlib作为底层，更易上手的作图库&lt;code&gt;seaborn&lt;/code&gt;。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="Data Visualization" scheme="http://www.phoebepan.cn/categories/Data-Visualization/"/>
    
    
      <category term="ML" scheme="http://www.phoebepan.cn/tags/ML/"/>
    
      <category term="Visualization" scheme="http://www.phoebepan.cn/tags/Visualization/"/>
    
  </entry>
  
  <entry>
    <title>阅读笔记——7 Techniques to Handle Imbalanced Data</title>
    <link href="http://www.phoebepan.cn/2017/06/05/Imbalanced_data/"/>
    <id>http://www.phoebepan.cn/2017/06/05/Imbalanced_data/</id>
    <published>2017-06-05T07:30:16.000Z</published>
    <updated>2017-06-28T15:15:00.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote class="blockquote-center"><p>这篇阅读笔记，主要介绍处理不平衡数据的常见7种方法。所谓<strong>不平衡数据</strong>，指在网络入侵、癌症监测、银行信用卡检测等领域，出现如下图所示的数据集中，正负样本比例严重失调的情况。<br><img src="/images/imbalanced-data-1.png" alt="imbalanced-data-1" title="正负样本分布"></p>
</blockquote>
<a id="more"></a>
<p><a href="http://www.kdnuggets.com/2017/06/7-techniques-handle-imbalanced-data.html" target="_blank" rel="external">博客中</a>介绍了7种方法帮助我们训练一个分类器，来处理这些不平衡的数据。</p>
<h3 id="1-使用正确的评价指标"><a href="#1-使用正确的评价指标" class="headerlink" title="1.使用正确的评价指标"></a>1.使用正确的评价指标</h3><p>针对上图这样的数据集，如果我们还是采用准确度(accuracy)来评估模型训练结果，那么所有的分类器将所有的测试样本都分到“0”这一类，模型准确率无疑非常好，但显然，这样的model对我们来说，是没有价值的。<br>这种情况，其他适宜的评估指标有：<code>Precision/Specificity</code>、<code>Recall/Sensitivity</code>、<code>F1 score</code>、<code>MCC</code>、<code>AUC</code>、<code>G-Mean</code></p>
<h3 id="2-训练集重新采样-Resample"><a href="#2-训练集重新采样-Resample" class="headerlink" title="2.训练集重新采样(Resample)"></a>2.训练集重新采样(Resample)</h3><p>除了使用不同的评价指标，另外可以通过<strong>下采样</strong>和<strong>过采样</strong>在不平衡数据中得到平衡数据集。</p>
<h4 id="下采样-Under-sampling"><a href="#下采样-Under-sampling" class="headerlink" title="下采样(Under-sampling)"></a>下采样(Under-sampling)</h4><p>当数据量充足时，下采样通过减少负样本数量（即多数的类），即保留正样本和随机选择相同数量的负样本，得到新的平衡训练集。</p>
<h4 id="过采样-Over-sampling"><a href="#过采样-Over-sampling" class="headerlink" title="过采样(Over-sampling)"></a>过采样(Over-sampling)</h4><p>当数据量不够时，过采样通过增加正样本数来平衡数据集，可以采用<code>repetition</code>、<code>bootstrapping</code>、<code>SMOTE</code>得到新的正样本。<br><a href="https://github.com/scikit-learn-contrib/imbalanced-learn" target="_blank" rel="external">Python实现</a></p>
<p>下采样和过采样两者之间没有谁优谁劣，具体用哪种方式取决于数据集本身，有时两者结合使用可能效果更好。</p>
<h3 id="3-正确使用K折交叉验证"><a href="#3-正确使用K折交叉验证" class="headerlink" title="3.正确使用K折交叉验证"></a>3.正确使用K折交叉验证</h3><p>值得注意的是，当我们用过采样处理不平衡训练集时，通常需要在<strong>过采样之前应用交叉验证</strong>，这样做的好处就是避免模型过拟合。</p>
<h4 id="过拟合产生原因："><a href="#过拟合产生原因：" class="headerlink" title="过拟合产生原因："></a>过拟合产生原因：</h4><ul>
<li>模型的复杂度越高，越容易overfitting</li>
<li>数据的噪声越大，越容易overfitting</li>
<li>数据量越少，越容易overfitting</li>
</ul>
<h3 id="4-重采样训练集集成-Ensemble"><a href="#4-重采样训练集集成-Ensemble" class="headerlink" title="4.重采样训练集集成(Ensemble)"></a>4.重采样训练集集成(Ensemble)</h3><p><img src="/images/imbalanced-data-2.png" alt="imbalanced-data-2" title="Ensemble different resampled datasets"><br>如上面示例图所示，使用所有的正样本和 n 个不同的负样本建立 n 个models。比如你想得到10个models，如果正样本是1000个，那么你需要随机选择10000个负样本，然后将这10000个负样本分成10份，接下来训练这10个不同的models。<br>这种方法，简单方便，易扩展，更好的泛化能力。</p>
<h3 id="5-不同比例采样"><a href="#5-不同比例采样" class="headerlink" title="5.不同比例采样"></a>5.不同比例采样</h3><p>之前的方法，都是1:1调和样本，最佳的比例取决于数据和使用的模型。与其对所有models使用同样的比例进行ensemble，更值得尝试的是采用不同的比例进行ensemble。正如下图所示：<br><img src="/images/imbalanced-data-3.png" alt="imbalanced-data-3" title="Resample with different ratios"></p>
<h3 id="6-负样本进行聚类"><a href="#6-负样本进行聚类" class="headerlink" title="6.负样本进行聚类"></a>6.负样本进行聚类</h3><p>Sergey在Quora上提出一个更完美的<a href="www.quora.com/In-classification-how-do-you-handle-an-unbalanced-training-set/answers/1144228?srid=h3G6o">方法</a>，对负样本进行聚类，只用负样本聚类的簇中心和正样本组成训练集。</p>
<h3 id="7-自己设计模型"><a href="#7-自己设计模型" class="headerlink" title="7.自己设计模型"></a>7.自己设计模型</h3><p>事实上，已经有一些models本身就可以处理非平衡数据集，无需进行重新采样，如XGBoost。<br>重设损失函数，比起负样本误分，对正样本误分设置更大的惩罚系数。</p>
<h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><p>这些处理方法只是一个起点，没有一种方法可以解决所有问题，<code>多试才是王道</code>！</p>
]]></content>
    
    <summary type="html">
    
      &lt;blockquote class=&quot;blockquote-center&quot;&gt;&lt;p&gt;这篇阅读笔记，主要介绍处理不平衡数据的常见7种方法。所谓&lt;strong&gt;不平衡数据&lt;/strong&gt;，指在网络入侵、癌症监测、银行信用卡检测等领域，出现如下图所示的数据集中，正负样本比例严重失调的情况。&lt;br&gt;&lt;img src=&quot;/images/imbalanced-data-1.png&quot; alt=&quot;imbalanced-data-1&quot; title=&quot;正负样本分布&quot;&gt;&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="http://www.phoebepan.cn/categories/Machine-Learning/"/>
    
    
      <category term="ML" scheme="http://www.phoebepan.cn/tags/ML/"/>
    
      <category term="Reading" scheme="http://www.phoebepan.cn/tags/Reading/"/>
    
  </entry>
  
  <entry>
    <title>python——DFS+BFS</title>
    <link href="http://www.phoebepan.cn/2017/06/03/max_depth/"/>
    <id>http://www.phoebepan.cn/2017/06/03/max_depth/</id>
    <published>2017-06-03T07:30:16.000Z</published>
    <updated>2017-08-08T15:32:50.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote class="blockquote-center"><h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p>本篇笔记主要记录，运用DFS或BFS算法求解二叉树最大深度问题。</p>
</blockquote>
<a id="more"></a>
<h3 id="二叉树最大深度"><a href="#二叉树最大深度" class="headerlink" title="二叉树最大深度"></a>二叉树最大深度</h3><p> 题目来源：<a href="https://leetcode.com/problems/maximum-depth-of-binary-tree/description/" target="_blank" rel="external">LeeCode104: Maximum Depth of Binary Tree</a></p>
<h3 id="解法一"><a href="#解法一" class="headerlink" title="解法一"></a>解法一</h3><p>DFS(递归)<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Definition for a binary tree node.</span></div><div class="line"><span class="comment"># class TreeNode(object):</span></div><div class="line"><span class="comment">#     def __init__(self, x):</span></div><div class="line"><span class="comment">#         self.val = x</span></div><div class="line"><span class="comment">#         self.left = None</span></div><div class="line"><span class="comment">#         self.right = None</span></div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span><span class="params">(object)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">maxDepth</span><span class="params">(self, root)</span>:</span></div><div class="line">        <span class="string">"""</span></div><div class="line">        :type root: TreeNode</div><div class="line">        :rtype: int</div><div class="line">        """</div><div class="line">        <span class="keyword">return</span> <span class="number">1</span>+max(self.maxDepth(root.left),self.maxDepth(root.right)) <span class="keyword">if</span> root <span class="keyword">else</span> <span class="number">0</span></div></pre></td></tr></table></figure></p>
<h3 id="解法二"><a href="#解法二" class="headerlink" title="解法二"></a>解法二</h3><p>BFS、队列<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Definition for a binary tree node.</span></div><div class="line"><span class="comment"># class TreeNode(object):</span></div><div class="line"><span class="comment">#     def __init__(self, x):</span></div><div class="line"><span class="comment">#         self.val = x</span></div><div class="line"><span class="comment">#         self.left = None</span></div><div class="line"><span class="comment">#         self.right = None</span></div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span><span class="params">(object)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">maxDepth</span><span class="params">(self, root)</span>:</span></div><div class="line">        <span class="string">"""</span></div><div class="line">        :type root: TreeNode</div><div class="line">        :rtype: int</div><div class="line">        """</div><div class="line">        <span class="keyword">if</span> <span class="keyword">not</span> root:</div><div class="line">            <span class="keyword">return</span> <span class="number">0</span></div><div class="line">        queue,h=[root],<span class="number">0</span></div><div class="line">        <span class="keyword">while</span> queue:</div><div class="line">            h+=<span class="number">1</span></div><div class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>,len(queue)):</div><div class="line">                node=queue.pop(<span class="number">0</span>)</div><div class="line">                queue.extend(filter(<span class="keyword">None</span>,[node.left,node.right]))</div><div class="line">        <span class="keyword">return</span> h</div></pre></td></tr></table></figure></p>
]]></content>
    
    <summary type="html">
    
      &lt;blockquote class=&quot;blockquote-center&quot;&gt;&lt;h3 id=&quot;简介&quot;&gt;&lt;a href=&quot;#简介&quot; class=&quot;headerlink&quot; title=&quot;简介&quot;&gt;&lt;/a&gt;简介&lt;/h3&gt;&lt;p&gt;本篇笔记主要记录，运用DFS或BFS算法求解二叉树最大深度问题。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="Algorithms" scheme="http://www.phoebepan.cn/categories/Algorithms/"/>
    
    
      <category term="LeetCode" scheme="http://www.phoebepan.cn/tags/LeetCode/"/>
    
      <category term="Algorithms" scheme="http://www.phoebepan.cn/tags/Algorithms/"/>
    
      <category term="Python" scheme="http://www.phoebepan.cn/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>降维——PCA和LDA</title>
    <link href="http://www.phoebepan.cn/2017/06/02/pca_lda/"/>
    <id>http://www.phoebepan.cn/2017/06/02/pca_lda/</id>
    <published>2017-06-02T07:30:16.000Z</published>
    <updated>2017-08-08T15:08:30.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote class="blockquote-center"><h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p>所有机器学习方法共同面临的，在高维情况下出现的数据样本稀疏、距离计算困难等问题，被称为“维度灾难”，李航老师在他的博客《机器学习新动向：从人机交互中》中提到，学习精度越高，学习确信度越高，学习模型越复杂，所需要的样本也就越多。特征太多会造成模型复杂，缓解这一问题的一个重要途径是<strong>降维</strong>，将原高维空间中的数据点映射到低维的空间。</p>
</blockquote>
<a id="more"></a>
<h3 id="降维"><a href="#降维" class="headerlink" title="降维"></a>降维</h3><p>原始高维空间数据表示中包含冗余以及噪音信息，而降低准确率，通过降维减少冗余信息，提高精度。基于线性变换来进行降维的方法称为线性降维方法，通过对变换矩阵施加不同的约束，对低维子空间的性质的不同要求，可以得到多种线性降维方法。<br>对降维效果的<strong>评估</strong>，通常是比较降维前后学习器的性能，若性能有所提高，则认为降维起作用了。若维度降至二或三维，则可以通过可视化技术直观地判断降维效果。</p>
<h3 id="PCA"><a href="#PCA" class="headerlink" title="PCA"></a>PCA</h3><p>目标：逐个寻找超平面，将样本投射到这个超平面后使得各样本点间方差最大。</p>
<blockquote>
<p>若存在这样的超平面，那么它大概要具有的性质：<br><strong>最近重构性</strong>，样本点到这个超平面的距离都足够近；<br><strong>最大可分性</strong>，样本点在这个超平面上的投影能尽可能分开。</p>
</blockquote>
<p>PCA追求的是在降维后能够最大化保持数据内在信息，并通过衡量在投影方向上的数据方差的大小来衡量该方向的重要性。</p>
<p>算法：</p>
<hr>
<p>输入：样本数据集D={x<sub>1</sub>,x<sub>2</sub>,…,x<sub>m</sub>};<br>: 低维空间维度d’ </p>
<p>输出：变换矩阵W=(w<sub>1</sub>,w<sub>2</sub>,…,w<sub>d’</sub>)<br>过程：</p>
<ol>
<li>对所有样本进行中心化；</li>
<li>计算样本的协方差矩阵XX<sup>T</sup>；</li>
<li>对协方差矩阵XX<sup>T</sup>做特征值分解；</li>
<li>取最大的d’个特征值所对应的特征向量w<sub>1</sub>,w<sub>2</sub>,…,w<sub>d’</sub>.</li>
</ol>
<hr>
<blockquote>
<p><strong>决定降维维度/主成分个数</strong><br>用户指定；<br>重构角度设定阈值；<br>不同d’维空间+学习器(开销较小)进行交叉验证选取；</p>
</blockquote>
<p><code>不要盲目PCA，当在原数据有了一个比较好的结果，想进一步提升速度时可考虑PCA。</code></p>
<h3 id="LDA"><a href="#LDA" class="headerlink" title="LDA"></a>LDA</h3><p>目标：找到一个超平面，样本点映射在超平面使得相同类别点尽可能靠近，不同类别数据尽可能远。</p>
<p>PCA是无监督线性降维方法，LDA是有监督的线性降维方法。</p>
<p>区别：如下图中两堆点是两类的话，LDA会选择轴1，PCA会是轴2<br><img src="/images/pca.jpg" alt="pca"></p>
<h3 id="核化线性降维"><a href="#核化线性降维" class="headerlink" title="核化线性降维"></a>核化线性降维</h3><p>非线性降维常用的方法，是基于核技巧对线性降维方法进行“核化”，将原数据映射到高维特征空间，再在特征空间实施降维。</p>
<p>参考文献：</p>
<ol>
<li>李航，<a href="http://blog.sina.com.cn/s/blog_7ad48fee01016d25.html" target="_blank" rel="external">《机器学习新动向：从人机交互中》</a></li>
<li>周志华，《机器学习》</li>
</ol>
]]></content>
    
    <summary type="html">
    
      &lt;blockquote class=&quot;blockquote-center&quot;&gt;&lt;h3 id=&quot;简介&quot;&gt;&lt;a href=&quot;#简介&quot; class=&quot;headerlink&quot; title=&quot;简介&quot;&gt;&lt;/a&gt;简介&lt;/h3&gt;&lt;p&gt;所有机器学习方法共同面临的，在高维情况下出现的数据样本稀疏、距离计算困难等问题，被称为“维度灾难”，李航老师在他的博客《机器学习新动向：从人机交互中》中提到，学习精度越高，学习确信度越高，学习模型越复杂，所需要的样本也就越多。特征太多会造成模型复杂，缓解这一问题的一个重要途径是&lt;strong&gt;降维&lt;/strong&gt;，将原高维空间中的数据点映射到低维的空间。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="http://www.phoebepan.cn/categories/Machine-Learning/"/>
    
    
      <category term="ML" scheme="http://www.phoebepan.cn/tags/ML/"/>
    
  </entry>
  
  <entry>
    <title>稀疏表示与字典学习</title>
    <link href="http://www.phoebepan.cn/2017/06/01/SpaseL/"/>
    <id>http://www.phoebepan.cn/2017/06/01/SpaseL/</id>
    <published>2017-06-01T07:30:16.000Z</published>
    <updated>2017-08-03T23:07:13.000Z</updated>
    
    <content type="html"><![CDATA[<h3 id="稀疏表示"><a href="#稀疏表示" class="headerlink" title="稀疏表示"></a>稀疏表示</h3><p>文档分类时，每个文档当作一个样本，文档中的每个词作为一个特征，这往往会得到很高维的矩阵，而且矩阵中每一行都有大量的零元素，且每行零元素出现的列分布不同，具有这样的<strong>稀疏表达</strong>形式的矩阵，对学习任务来说是有好处的(可以当做线性可分问题处理)。<br><a id="more"></a></p>
<h3 id="字典学习"><a href="#字典学习" class="headerlink" title="字典学习"></a>字典学习</h3><p>如果将稠密的数据集转化成<strong>稀疏表示</strong>形式，使得数据集<strong>“恰当稀疏”</strong>，从而享受稀疏性的好处。那么问题来了，如何实现这种转化呢？<br>在上面的文本分类中提到的矩阵，常常是高维的，过度稀疏的，如果我们借鉴下“字典”的结构，将字的特征维度根据字典转化成合适的稀疏特征表示形式，就可以简化学习任务，模型复杂度大大降低，这样的过程称为<strong>“字典学习”</strong>。字典学习形式如下，<img src="/images/spase.png" alt="spase"><br>，样本x<sub>i</sub>通过字典矩阵B得到的稀疏表示$ \alpha $<sub>i</sub>。我们可以通过设置字典的维度，从而控制稀疏程度。<br>求解过程有很多，常用的有KSVD等</p>
<h3 id="压缩感知"><a href="#压缩感知" class="headerlink" title="压缩感知"></a>压缩感知</h3><p>我们常常希望根据部分信息来恢复全部信息，如，数据通信中要将数字信号还原成模拟信号，部分用户对电影的评价数据等，如何精确的重构出这样的信息呢？<br>针对这类问题，压缩感知提供了新的思路。<br>压缩感知关注的是如何利用数据本身的所具有的稀疏性，从部分观测样本中恢复原来缺失的信息。主要涉及两个过程，<strong>稀疏表示</strong>、<strong>矩阵补全</strong>。能够通过压缩感知技术恢复补全信息的前提条件之一是原始数据(部分信息)有稀疏表示。</p>
<h4 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h4><p>人脸识别的鲁棒主成分分析、基于矩阵补全的协同过滤……</p>
<p>[1]. 周志华.《机器学习》</p>
]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;稀疏表示&quot;&gt;&lt;a href=&quot;#稀疏表示&quot; class=&quot;headerlink&quot; title=&quot;稀疏表示&quot;&gt;&lt;/a&gt;稀疏表示&lt;/h3&gt;&lt;p&gt;文档分类时，每个文档当作一个样本，文档中的每个词作为一个特征，这往往会得到很高维的矩阵，而且矩阵中每一行都有大量的零元素，且每行零元素出现的列分布不同，具有这样的&lt;strong&gt;稀疏表达&lt;/strong&gt;形式的矩阵，对学习任务来说是有好处的(可以当做线性可分问题处理)。&lt;br&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="http://www.phoebepan.cn/categories/Machine-Learning/"/>
    
    
      <category term="ML" scheme="http://www.phoebepan.cn/tags/ML/"/>
    
  </entry>
  
  <entry>
    <title>神经网络</title>
    <link href="http://www.phoebepan.cn/2017/05/25/neural%20network/"/>
    <id>http://www.phoebepan.cn/2017/05/25/neural network/</id>
    <published>2017-05-25T07:30:16.000Z</published>
    <updated>2017-08-08T15:38:08.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote class="blockquote-center"><p>神经网络是由具有适应性的简单单元组成的广泛并行互连的网络，它的组织能够模拟生物神经系统对真实世界物体所作出的交互反应。<br>——Kohonen</p>
</blockquote>
<a id="more"></a>
<h3 id="基础知识"><a href="#基础知识" class="headerlink" title="基础知识"></a>基础知识</h3><h4 id="神经元模型"><a href="#神经元模型" class="headerlink" title="神经元模型"></a>神经元模型</h4><blockquote>
<p>如下图，<strong>M-P神经元模型</strong></p>
<ul>
<li>连接权重</li>
<li>阈值</li>
<li>激活函数</li>
</ul>
</blockquote>
<p><img src="/images/MP_model.jpg" alt="MP_model"></p>
<h4 id="学习过程"><a href="#学习过程" class="headerlink" title="学习过程"></a>学习过程</h4><p>根据训练集来调整神经元之间的”连接权”以及每个功能神经元的阈值。</p>
<h4 id="感知机与多层网络"><a href="#感知机与多层网络" class="headerlink" title="感知机与多层网络"></a>感知机与多层网络</h4><p><strong>感知机</strong>由两层神经元组成，输入层接收输入信号后传递给输出层，输出层是M-P神经元，可处理<strong>线性可分数据集</strong>。</p>
<p><strong>多层网络</strong>，在输入层与输出层增加隐含层(一个或多个)，隐含层和输出层神经元都是拥有激活函数的功能神经元。</p>
<h4 id="误差逆传播-BP-算法"><a href="#误差逆传播-BP-算法" class="headerlink" title="误差逆传播(BP)算法"></a>误差逆传播(BP)算法</h4><p>迭代学习算法，基于梯度下降策略，以目标的负梯度方向对参数进行调整。</p>
<blockquote>
<p><strong>标准BP和累积BP</strong><br>标准BP，每次更新只针对单个样例，参数更新频繁；<br>累积BP，读取整个数据集后才对参数进行一次更新。</p>
</blockquote>
<p><strong>缓解BP过拟合的策略</strong></p>
<ul>
<li>早停，训练误差降低，验证误差升高，停止训练；</li>
<li>正则化，增加描述网络复杂度项(连接权与阈值的平方和)，交叉验证来估计。</li>
</ul>
<h3 id="全局最小和局部极小"><a href="#全局最小和局部极小" class="headerlink" title="全局最小和局部极小"></a>全局最小和局部极小</h3><p><code>参数寻优</code></p>
<h4 id="跳出局部最小策略"><a href="#跳出局部最小策略" class="headerlink" title="跳出局部最小策略"></a>跳出局部最小策略</h4><ul>
<li>多组不同参数值初始化多个神经网路，训练，选择更接近全局最小的结果；</li>
<li>模拟退火，每一步以一定概率接受比当前解更差的结果；</li>
<li>随机梯度下降，计算梯度时加入随机因素。</li>
</ul>
<h3 id="常见的神经网路"><a href="#常见的神经网路" class="headerlink" title="常见的神经网路"></a>常见的神经网路</h3><h4 id="RBF"><a href="#RBF" class="headerlink" title="RBF"></a>RBF</h4><ul>
<li>单隐层前馈神经网路</li>
<li>径向基函数作为隐层神经元激活函数</li>
<li>输出层对隐层神经元输出进行线性组合</li>
</ul>
<h4 id="ART"><a href="#ART" class="headerlink" title="ART"></a>ART</h4><p><code>胜者通吃</code>、<code>竞争型学习</code>、<code>无监督学习</code>、<code>增量学习</code></p>
<blockquote>
<p>比较层(输入)、识别层(模式)、<strong>识别阈值</strong>、重置模块；</p>
<blockquote>
<p> 当识别阈值较高，学习更细；较低，产生粗略的模式类。</p>
</blockquote>
</blockquote>
<h4 id="SOM"><a href="#SOM" class="headerlink" title="SOM"></a>SOM</h4><p>高维空间相似的样本点映射到网络输出层中的邻近神经元。<br>在聚类、高维数据可视化、图像分割中广泛应用。</p>
<h4 id="级联相关网络"><a href="#级联相关网络" class="headerlink" title="级联相关网络"></a>级联相关网络</h4><p><code>结构自适应网络</code><br>数据较少时易过拟合。</p>
<h4 id="Elman网络"><a href="#Elman网络" class="headerlink" title="Elman网络"></a>Elman网络</h4><p><code>递归神经网络</code>、<code>环型结构</code></p>
<h4 id="Boltzmann机"><a href="#Boltzmann机" class="headerlink" title="Boltzmann机"></a>Boltzmann机</h4><p><code>完全图</code></p>
<h4 id="RBM"><a href="#RBM" class="headerlink" title="RBM"></a>RBM</h4><p>受限Boltzmann机，<code>二部图</code></p>
<h3 id="深度学习"><a href="#深度学习" class="headerlink" title="深度学习"></a>深度学习</h3><p><code>多隐层神经网络</code>(通常有八九层隐层)<br>训练手段，有以下两种：</p>
<h4 id="无监督逐层训练"><a href="#无监督逐层训练" class="headerlink" title="无监督逐层训练"></a>无监督逐层训练</h4><p>预训练+微调</p>
<blockquote>
<p>大量参数分组，对每组先找到局部较好设置，在基于局部较优结果联合起来进行全局寻优。</p>
</blockquote>
<h4 id="权共享"><a href="#权共享" class="headerlink" title="权共享"></a>权共享</h4><p>每一组神经元使用相同的连接权。</p>
<blockquote>
<p>卷积神经网络CNN</p>
</blockquote>
<p>参考文献</p>
<ol>
<li>周志华，《机器学习》</li>
</ol>
]]></content>
    
    <summary type="html">
    
      &lt;blockquote class=&quot;blockquote-center&quot;&gt;&lt;p&gt;神经网络是由具有适应性的简单单元组成的广泛并行互连的网络，它的组织能够模拟生物神经系统对真实世界物体所作出的交互反应。&lt;br&gt;——Kohonen&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="http://www.phoebepan.cn/categories/Machine-Learning/"/>
    
    
      <category term="ML" scheme="http://www.phoebepan.cn/tags/ML/"/>
    
  </entry>
  
  <entry>
    <title>机器学习经典算法优缺点总结</title>
    <link href="http://www.phoebepan.cn/2017/05/23/ML_good_bad/"/>
    <id>http://www.phoebepan.cn/2017/05/23/ML_good_bad/</id>
    <published>2017-05-23T07:30:16.000Z</published>
    <updated>2017-08-03T22:56:04.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote class="blockquote-center"><p>简介<br>机器学习大多数场景是搜索、广告、垃圾过滤、安全、推荐系统等等。本文是经典机器学习算法的优劣势比较，欢迎纠正。</p>
</blockquote>
<a id="more"></a>
<h3 id="朴素贝叶斯"><a href="#朴素贝叶斯" class="headerlink" title="朴素贝叶斯"></a>朴素贝叶斯</h3><p><code>生成模型</code>、<code>贝叶斯定理</code>、<code>特征条件独立</code>、<code>后验概率最大化</code></p>
<h4 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h4><blockquote>
<p>基于贝叶斯定理与特征条件独立假设的分类方法。对于给定的训练集首先基于特征条件独立假设学习输入/输出的联合概率分布，然后利用贝叶斯定理求出后验概率最大。</p>
</blockquote>
<h4 id="解决问题-适用场景"><a href="#解决问题-适用场景" class="headerlink" title="解决问题(适用场景)"></a>解决问题(适用场景)</h4><blockquote>
<p>分类问题<br>场景举例：文本主题分类、垃圾文本过滤</p>
</blockquote>
<h4 id="技术细节"><a href="#技术细节" class="headerlink" title="技术细节"></a>技术细节</h4><blockquote>
<p>概率估计方法：极大似然估计和贝叶斯估计(拉普拉斯平滑)</p>
</blockquote>
<h4 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h4><blockquote>
<p>实现简单；对小规模数据表现很好，适合多分类任务；适合增量式训练</p>
</blockquote>
<h4 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h4><blockquote>
<p>对输入数据表达形式敏感(离散、连续、值极大极小等)；需要条件独立假设，会牺牲准确率，分类性能不一定高</p>
</blockquote>
<h3 id="逻辑回归"><a href="#逻辑回归" class="headerlink" title="逻辑回归"></a>逻辑回归</h3><p><code>对数线性模型</code>、<code>似然函数为目标函数</code>、<code>最优化问题</code></p>
<h4 id="原理-1"><a href="#原理-1" class="headerlink" title="原理"></a>原理</h4><blockquote>
<p>主要计算在某个样本特征下事件发生的概率。用sigmoid函数将线性回归进行了归一化，输出值压缩到0-1之间，这个值代表的是发生的概率。</p>
</blockquote>
<h4 id="解决问题-适用场景-1"><a href="#解决问题-适用场景-1" class="headerlink" title="解决问题(适用场景)"></a>解决问题(适用场景)</h4><blockquote>
<p>分类问题(二分类推广到多分类)<br>场景举例：根据用户浏览情况预测是否购买商品</p>
</blockquote>
<h4 id="技术细节-1"><a href="#技术细节-1" class="headerlink" title="技术细节"></a>技术细节</h4><blockquote>
<ul>
<li>LogReg中，输出Y=1的对数几率是输入x的线性函数；</li>
<li>极大似然估计法估计模型参数；</li>
<li>softmax和k个LR的选择，<strong>类别之间互斥，softmax</strong>、类别之前有联系，K个LR</li>
<li>优化：梯度下降、拟牛顿法、BFGS、改进的迭代尺度法<br>梯度下降会陷入局部最优，改用随机梯度下降，收敛速度更快，且易并行。</li>
</ul>
</blockquote>
<h4 id="优点-1"><a href="#优点-1" class="headerlink" title="优点"></a>优点</h4><blockquote>
<p>简单高效；概率值输出；多重共线性可以通过L2正则化应对</p>
</blockquote>
<h4 id="缺点-1"><a href="#缺点-1" class="headerlink" title="缺点"></a>缺点</h4><blockquote>
<p>欠拟合，精度不高；<br>必须线性可分；<br>特征空间太大时表现不太好；<br>大量分类变量性能较差；</p>
</blockquote>
<h3 id="k-近邻"><a href="#k-近邻" class="headerlink" title="k-近邻"></a>k-近邻</h3><p><code>判别模型</code>、<code>多分类与回归</code>、<code>不具有显式学习过程</code></p>
<h4 id="原理-2"><a href="#原理-2" class="headerlink" title="原理"></a>原理</h4><blockquote>
<p>利用训练数据集对特征向量空间进行划分，根据k个最近邻的训练样本的类别，通过多数表决进行预测。</p>
</blockquote>
<h4 id="解决问题-适用场景-2"><a href="#解决问题-适用场景-2" class="headerlink" title="解决问题(适用场景)"></a>解决问题(适用场景)</h4><blockquote>
<p>分类与回归</p>
</blockquote>
<h4 id="技术细节-2"><a href="#技术细节-2" class="headerlink" title="技术细节"></a>技术细节</h4><blockquote>
<p>三要素：K的选择、距离度量、决策规则</p>
<blockquote>
<p><strong>交叉验证，取最优k值</strong><br>K小，模型变得复杂，过拟合；估计误差增大；<br>K大，模型变得简单，近似误差增大</p>
</blockquote>
<p><strong>kd 树</strong></p>
<blockquote>
<p>X的K个特征，一一个切分，使得每个数据最终都在切分点上(中位数)，对输入的数据搜索kd树，找到K近邻。</p>
</blockquote>
</blockquote>
<h4 id="优点-2"><a href="#优点-2" class="headerlink" title="优点"></a>优点</h4><blockquote>
<p>简单，分类与回归，可用于非线性，复杂度为O(n)，对噪声不敏感</p>
</blockquote>
<h4 id="缺点-2"><a href="#缺点-2" class="headerlink" title="缺点"></a>缺点</h4><blockquote>
<p>K需要人为设定，对大小不平衡数据易偏向大容量数据；计算量大，大量内存</p>
</blockquote>
<h3 id="决策树"><a href="#决策树" class="headerlink" title="决策树"></a>决策树</h3><p><code>判别模型</code>、<code>多分类与回归</code>、<code>正则化的极大似然估计</code></p>
<h4 id="原理-3"><a href="#原理-3" class="headerlink" title="原理"></a>原理</h4><blockquote>
<p>决策树实际上是将空间用超平面进行划分的一种方法，每次分割的时候，都将当前的空间一分为二，这样使得每一个叶子节点都是在空间中的一个不相交的区域，在进行决策的时候，会根据输入样本每一维feature的值，一步一步往下，最后使得样本落入N个区域中的一个。</p>
</blockquote>
<h4 id="解决问题-适用场景-3"><a href="#解决问题-适用场景-3" class="headerlink" title="解决问题(适用场景)"></a>解决问题(适用场景)</h4><blockquote>
<p>适用于小数据集<br>场景举例：基于规则的信用评估</p>
</blockquote>
<h4 id="技术细节-3"><a href="#技术细节-3" class="headerlink" title="技术细节"></a>技术细节</h4><blockquote>
<p><strong>特征选择</strong>准则：信息增益或信息增益比<br><strong>决策树生成</strong>，ID3、C4.5、CART(Gini index，平方误差)<br><strong>剪枝</strong>，减小模型复杂度，设定$ \alpha $ ，相当于正则化的极大似然估计；动态规划实现</p>
<blockquote>
<p><strong>CART</strong><br>通过递归方式建立决策二叉树，基尼指数最小化的特征(分类)，平方误差最小化(回归)作为划分特征</p>
</blockquote>
</blockquote>
<h4 id="优点-3"><a href="#优点-3" class="headerlink" title="优点"></a>优点</h4><blockquote>
<p>训练时间复杂度低，预测过程快速；可读性好；适合处理有缺失属性值得样本，能够处理不相关特征</p>
</blockquote>
<h4 id="缺点-3"><a href="#缺点-3" class="headerlink" title="缺点"></a>缺点</h4><blockquote>
<p>容易过拟合</p>
<blockquote>
<p>解决过拟合：剪枝、交叉验证、随机森林</p>
</blockquote>
<p>单棵树分类能力弱，对连续变量难以处理</p>
</blockquote>
<h3 id="随机森林"><a href="#随机森林" class="headerlink" title="随机森林"></a>随机森林</h3><p><code>判别模型</code>、<code>多分类与回归</code>、<code>正则化的极大似然估计</code>、<code>Random Future</code>、<code>Bagging</code></p>
<h4 id="原理-4"><a href="#原理-4" class="headerlink" title="原理"></a>原理</h4><blockquote>
<p>很多随机生成的决策树组成，预测时，每颗决策树进行判断，最后通过Bagging思想进行结果的输出。<br>RF被证明对大规模数据集和存在大量且有时不相关特征的项来说很有用。</p>
</blockquote>
<h4 id="解决问题-适用场景-4"><a href="#解决问题-适用场景-4" class="headerlink" title="解决问题(适用场景)"></a>解决问题(适用场景)</h4><blockquote>
<p>场景举例：用户流失分析、风险评估、检测离群点</p>
</blockquote>
<h4 id="技术细节-4"><a href="#技术细节-4" class="headerlink" title="技术细节"></a>技术细节</h4><blockquote>
<p>行(有放回)列(节点分裂时)采样，每个决策树使用相同参数<br>完全分裂，叶节点特征(样本特征相同、样本类别相同、样本数=1)<br>超参：树的数量、列采样特征数(通常取总特征的平方根)<br>泛化误差估计，oob(out-of-bag)</p>
</blockquote>
<h4 id="优点-4"><a href="#优点-4" class="headerlink" title="优点"></a>优点</h4><blockquote>
<ol>
<li>适合多分类问题，训练和预测速度快</li>
<li>对训练数据容错能力强，有效的估计缺失数据的方法</li>
<li>可以处理高维数据且不用特征选择</li>
<li>训练过程中检测到特征间相互影响及特征重要性</li>
<li>数据集上表现良好，避免过拟合</li>
<li>实现简单且容易并行化</li>
<li>在创建随机森林的时候，对generlization error使用的是无偏估计，所以在随机森林算法中不需要再进行交叉验证或者单独的测试集来获取测试集误差的无偏估计</li>
</ol>
</blockquote>
<h4 id="缺点-4"><a href="#缺点-4" class="headerlink" title="缺点"></a>缺点</h4><blockquote>
<p>噪声较大的分类和回归问题上会过拟合。对于类别特征的属性值较多时，RF产生的属性权重不可信。</p>
</blockquote>
<h3 id="支持向量机"><a href="#支持向量机" class="headerlink" title="支持向量机"></a>支持向量机</h3><p><code>间隔最大</code>、<code>凸二次规划</code>、<code>核函数</code></p>
<h4 id="原理-5"><a href="#原理-5" class="headerlink" title="原理"></a>原理</h4><blockquote>
<p>低维空间映射到高维空间，实现线性可分，求解正确划分训练集并(几何)间隔最大化的分离超平面。</p>
</blockquote>
<h4 id="技术细节-5"><a href="#技术细节-5" class="headerlink" title="技术细节"></a>技术细节</h4><blockquote>
<p>函数间隔，几何间隔<br>应用拉格朗日对偶性，求解对偶问题(更易求解、自然引用核函数)<br>正则化的合页损失函数最优化问题<br>核函数</p>
<blockquote>
<p>表示将输入从输入空间映射到特征空间得到的特征向量之间的内积。</p>
</blockquote>
<p>序列最小最优化算法(不断分解，对子问题求解，每个子问题中选择两个变量优化，其中一个变量是违反KKT条件的，直到所有变量都满足KKT条件为止)</p>
</blockquote>
<h4 id="优点-5"><a href="#优点-5" class="headerlink" title="优点"></a>优点</h4><blockquote>
<p>可以处理高维特征；使用核函数可以向高维空间进行映射，可以解决非线性分类；分类思想简单(间隔最大化)；分类效果好</p>
</blockquote>
<h4 id="缺点-5"><a href="#缺点-5" class="headerlink" title="缺点"></a>缺点</h4><blockquote>
<p>核函数及参数敏感；无法直接支持多分类；大量观测样本，效率低</p>
</blockquote>
<h3 id="集成学习"><a href="#集成学习" class="headerlink" title="集成学习"></a>集成学习</h3><h4 id="关键点"><a href="#关键点" class="headerlink" title="关键点"></a>关键点</h4><ol>
<li><code>差异性基分类器</code>:数据集(bagging、boosting)、数据特征、改变基分类器参数</li>
<li><code>基分类器整合</code>，回归(简单平均、加权平均)、分类(简单/加权投票、概率投票)</li>
</ol>
<p><code>Notes</code>：分类问题，每个分类器的分类精度大于0.5</p>
<h3 id="Boosting"><a href="#Boosting" class="headerlink" title="Boosting"></a>Boosting</h3><p><code>三个臭皮匠顶个诸葛亮</code>、<code>改变样本权重</code>、<code>分而治之</code>、<code>加法模型</code>、<code>前向分布算法</code>、<code>Shrinkage</code></p>
<h4 id="原理-6"><a href="#原理-6" class="headerlink" title="原理"></a>原理</h4><blockquote>
<p>在分类问题中，通过改变训练样本的权重，学习多个基分类器，最终进行线性组合。</p>
</blockquote>
<h4 id="技术细节-6"><a href="#技术细节-6" class="headerlink" title="技术细节"></a>技术细节</h4><blockquote>
<p>概念强可学习充要条件概念弱可学习</p>
<blockquote>
<ul>
<li>AdaBoost，提高前一轮弱分类器错分的<strong>样本权重</strong>，降低分对的样本权重；<strong>组合</strong>时，加大分类错误率低的基分类器权值，减小分类错误率大的权重。</li>
<li>AdaBoost二分类学习时是加法模型、损失函数为指数函数、学习算法为前向分布算法</li>
</ul>
</blockquote>
<p>boosting tree<br>基分类器，分类树或回归树，不同的问题的提升树主要区别在于<strong>损失函数</strong>不同。</p>
<blockquote>
<ul>
<li>梯度提升(gradient boosting)，每一轮计算为了减少上一次的残差，加入新的model是在之前model损失函数梯度下降方向。</li>
<li>GBDT精髓，在于训练都以上一棵树的残差为目标去提升。</li>
</ul>
</blockquote>
<p>调参：树的个数、树深度、学习速率、叶子上最大节点树、训练采样比例、训练特征采样比例……</p>
</blockquote>
<h4 id="优点-6"><a href="#优点-6" class="headerlink" title="优点"></a>优点</h4><blockquote>
<p>精度高；能处理非线性数据；能处理多特征类型；适合低维稠密数据</p>
</blockquote>
<h4 id="缺点-6"><a href="#缺点-6" class="headerlink" title="缺点"></a>缺点</h4><blockquote>
<p>并行麻烦；多分类时，复杂度大</p>
</blockquote>
<h3 id="EM"><a href="#EM" class="headerlink" title="EM"></a>EM</h3><p><code>含隐变量的概率模型</code>、<code>极大似然估计</code></p>
<h4 id="原理-7"><a href="#原理-7" class="headerlink" title="原理"></a>原理</h4><blockquote>
<p>从不完全数据或有数据丢失的数据集（存在隐含变量）中求解概率模型参数的最大似然估计方法。</p>
</blockquote>
<h4 id="解决问题-适用场景-5"><a href="#解决问题-适用场景-5" class="headerlink" title="解决问题(适用场景)"></a>解决问题(适用场景)</h4><blockquote>
<p>含有隐变量、非监督学习<br>高斯混合模型学习中的应用</p>
</blockquote>
<h4 id="技术细节-7"><a href="#技术细节-7" class="headerlink" title="技术细节"></a>技术细节</h4><blockquote>
<p>E步，求期望，根据上一步迭代的模型参数计算隐变量的后验概率；M步，求极大，似然函数最大化获得新的参数值</p>
</blockquote>
<h4 id="优点-7"><a href="#优点-7" class="headerlink" title="优点"></a>优点</h4><blockquote>
<p>思想简单，普适</p>
</blockquote>
<h4 id="缺点-7"><a href="#缺点-7" class="headerlink" title="缺点"></a>缺点</h4><blockquote>
<p>EM算法与初值的选择有关，不同初值，得到不同参数估计；</p>
</blockquote>
<h3 id="归一化使梯度下降收敛更好？"><a href="#归一化使梯度下降收敛更好？" class="headerlink" title="归一化使梯度下降收敛更好？"></a>归一化使梯度下降收敛更好？</h3><blockquote>
<p>如果不归一化，各维特征的跨度差距很大，梯度下降时，目标函数梯度方向就会偏离最小值的方向，走很多弯路，如下方左图。<br>如果归一化了，每一步梯度的方向都指向最小值，收敛更快，如下方右图。</p>
</blockquote>
<p><img src="/images/descend.png" alt="descend"></p>
<h3 id="算法横向比较"><a href="#算法横向比较" class="headerlink" title="算法横向比较"></a>算法横向比较</h3><h4 id="LR与Linear-SVM"><a href="#LR与Linear-SVM" class="headerlink" title="LR与Linear SVM"></a>LR与Linear SVM</h4><ul>
<li>都是线性分类器；</li>
<li>都会受到异常点影响；</li>
<li>本质不同——损失函数，一个hinge loss，一个logistical loss</li>
<li>Linear SVM只受支持向量影响(部分样本)，LR则受所有数据点影响；</li>
<li>Linear SVM需要归一化数据，LR不受影响；</li>
<li>Linear SVM自带正则，而LR需另加；</li>
<li>海量数据LR使用更广泛，Linear SVM内存消耗大</li>
</ul>
<h4 id="LR与朴素贝叶斯"><a href="#LR与朴素贝叶斯" class="headerlink" title="LR与朴素贝叶斯"></a>LR与朴素贝叶斯</h4><ul>
<li>判别/生成model；</li>
<li>朴素贝叶斯条件独立假设；</li>
<li>当数据集小时，首选朴素贝叶斯，数据集大时，考虑LR</li>
</ul>
<h3 id="附两份cheat-sheet"><a href="#附两份cheat-sheet" class="headerlink" title="附两份cheat sheet"></a>附两份cheat sheet</h3><p><img src="/images/sk-learn.png" alt="sk-learn"><br><img src="/images/cheat sheet.png" alt="cheat sheet"></p>
]]></content>
    
    <summary type="html">
    
      &lt;blockquote class=&quot;blockquote-center&quot;&gt;&lt;p&gt;简介&lt;br&gt;机器学习大多数场景是搜索、广告、垃圾过滤、安全、推荐系统等等。本文是经典机器学习算法的优劣势比较，欢迎纠正。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="http://www.phoebepan.cn/categories/Machine-Learning/"/>
    
    
      <category term="ML" scheme="http://www.phoebepan.cn/tags/ML/"/>
    
  </entry>
  
  <entry>
    <title>推荐系统实战——架构</title>
    <link href="http://www.phoebepan.cn/2017/05/22/recsys/"/>
    <id>http://www.phoebepan.cn/2017/05/22/recsys/</id>
    <published>2017-05-22T07:30:16.000Z</published>
    <updated>2017-07-20T07:40:50.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote class="blockquote-center"><h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p>推荐系统的核心任务可拆解成两部分，一个是如何为给定用户生成特征，另一个是如何根据特征找到物品。用户特征包括，<code>人口统计学特征</code>，<code>用户行为特征</code>，<code>用户话题特征</code>。本文是具体的基于物品的推荐算法系统的架构学习笔记。</p>
</blockquote>
<a id="more"></a>
<h3 id="推荐系统架构图"><a href="#推荐系统架构图" class="headerlink" title="推荐系统架构图"></a>推荐系统架构图</h3><p><img src="/images/recsys_arich.png" alt="recsys_arich" title="基于物品的推荐算法架构图"></p>
<h3 id="生成用户特征向量"><a href="#生成用户特征向量" class="headerlink" title="生成用户特征向量"></a>生成用户特征向量</h3><ul>
<li><strong>用户行为的种类</strong>，一般的标准就是用户付出代价越大的行为权重越高；</li>
<li><strong>用户行为产生的时间</strong>，一般来说，用户近期的行为比较重要，而用户很久之前的行为相对比较次要；</li>
<li><strong>用户行为的次数</strong>，用户对同一个物品的同一种行为发生的次数也反映了用户对物品的兴趣，行为次数多的物品对应的特征权重越高；</li>
<li><strong>物品的热门程度</strong>，加重不热门物品对应的特征的权重。</li>
</ul>
<h3 id="特征—物品相关推荐"><a href="#特征—物品相关推荐" class="headerlink" title="特征—物品相关推荐"></a>特征—物品相关推荐</h3><p>可以离线计算很多相关表，在线服务启动时会将这些相关表按照配置的权<br>重相加，得到最终的相关表保存在内存中，而在给用户进行推荐时，用的是加权后的相关表。</p>
<h3 id="过滤模块"><a href="#过滤模块" class="headerlink" title="过滤模块"></a>过滤模块</h3><ul>
<li>用户已经产生过行为物品</li>
<li>候选物品以外的物品</li>
<li>某些质量很差的物品</li>
</ul>
<h3 id="排名模块"><a href="#排名模块" class="headerlink" title="排名模块"></a>排名模块</h3><ul>
<li>新颖性排名，尽量推荐他们不知道的、长尾中的物品，热门的物品进行降权；</li>
<li>多样性；</li>
<li>时间多样性；</li>
<li>用户反馈，点击率预测。</li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;blockquote class=&quot;blockquote-center&quot;&gt;&lt;h3 id=&quot;简介&quot;&gt;&lt;a href=&quot;#简介&quot; class=&quot;headerlink&quot; title=&quot;简介&quot;&gt;&lt;/a&gt;简介&lt;/h3&gt;&lt;p&gt;推荐系统的核心任务可拆解成两部分，一个是如何为给定用户生成特征，另一个是如何根据特征找到物品。用户特征包括，&lt;code&gt;人口统计学特征&lt;/code&gt;，&lt;code&gt;用户行为特征&lt;/code&gt;，&lt;code&gt;用户话题特征&lt;/code&gt;。本文是具体的基于物品的推荐算法系统的架构学习笔记。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="Recommendation" scheme="http://www.phoebepan.cn/categories/Recommendation/"/>
    
    
      <category term="Recommendation" scheme="http://www.phoebepan.cn/tags/Recommendation/"/>
    
  </entry>
  
  <entry>
    <title>推荐系统实战——社交网络数据</title>
    <link href="http://www.phoebepan.cn/2017/05/21/social_data/"/>
    <id>http://www.phoebepan.cn/2017/05/21/social_data/</id>
    <published>2017-05-21T07:30:16.000Z</published>
    <updated>2017-07-20T06:13:49.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote class="blockquote-center"><h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p><strong>六度原理</strong>讲的是社会中任意两个人都可以通过不超过6个人的路径相互认识。在现实社会中，很多时候我们都是通过朋友获得推荐，基于社交网络的推荐可以很好地模拟现实社会。本文是利用社交网络数据给用户进行个性化推荐相关内容的学习。</p>
</blockquote>
<a id="more"></a>
<h3 id="思维导图"><a href="#思维导图" class="headerlink" title="思维导图"></a>思维导图</h3><p><img src="/images/social_data.png" alt="social_data"></p>
<p> [1]项亮，推荐系统实战.</p>
]]></content>
    
    <summary type="html">
    
      &lt;blockquote class=&quot;blockquote-center&quot;&gt;&lt;h3 id=&quot;简介&quot;&gt;&lt;a href=&quot;#简介&quot; class=&quot;headerlink&quot; title=&quot;简介&quot;&gt;&lt;/a&gt;简介&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;六度原理&lt;/strong&gt;讲的是社会中任意两个人都可以通过不超过6个人的路径相互认识。在现实社会中，很多时候我们都是通过朋友获得推荐，基于社交网络的推荐可以很好地模拟现实社会。本文是利用社交网络数据给用户进行个性化推荐相关内容的学习。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="Recommendation" scheme="http://www.phoebepan.cn/categories/Recommendation/"/>
    
    
      <category term="Recommendation" scheme="http://www.phoebepan.cn/tags/Recommendation/"/>
    
  </entry>
  
  <entry>
    <title>推荐系统实战——上下文信息</title>
    <link href="http://www.phoebepan.cn/2017/05/20/context/"/>
    <id>http://www.phoebepan.cn/2017/05/20/context/</id>
    <published>2017-05-20T07:30:16.000Z</published>
    <updated>2017-07-20T06:13:19.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote class="blockquote-center"><h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p>将最符合用户兴趣的物品推荐给用户时，用户此时、此刻、心情等等，是提高推荐系统算法不可忽视的一环，这些信息在推荐系统中对应的专业名词是上下文(context)，本文主要说说<strong>上下文信息</strong>的事儿~</p>
</blockquote>
<a id="more"></a>
<h3 id="思维导图"><a href="#思维导图" class="headerlink" title="思维导图"></a>思维导图</h3><p><img src="/images/context.png" alt="context"></p>
<p> [1]项亮，推荐系统实战.</p>
]]></content>
    
    <summary type="html">
    
      &lt;blockquote class=&quot;blockquote-center&quot;&gt;&lt;h3 id=&quot;简介&quot;&gt;&lt;a href=&quot;#简介&quot; class=&quot;headerlink&quot; title=&quot;简介&quot;&gt;&lt;/a&gt;简介&lt;/h3&gt;&lt;p&gt;将最符合用户兴趣的物品推荐给用户时，用户此时、此刻、心情等等，是提高推荐系统算法不可忽视的一环，这些信息在推荐系统中对应的专业名词是上下文(context)，本文主要说说&lt;strong&gt;上下文信息&lt;/strong&gt;的事儿~&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="Recommendation" scheme="http://www.phoebepan.cn/categories/Recommendation/"/>
    
    
      <category term="Recommendation" scheme="http://www.phoebepan.cn/tags/Recommendation/"/>
    
  </entry>
  
  <entry>
    <title>推荐系统实战——用户标签数据</title>
    <link href="http://www.phoebepan.cn/2017/05/19/user_tags/"/>
    <id>http://www.phoebepan.cn/2017/05/19/user_tags/</id>
    <published>2017-05-19T07:30:16.000Z</published>
    <updated>2017-07-19T08:25:13.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote class="blockquote-center"><h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p>推荐系统的目的是联系用户的兴趣和物品，倘若通过一些特征(feature)联系用户和物品，给用户推荐那些具有用户喜欢的特征的物品，标签作为一种无层次化结构的、用来描述信息的关键词。本文主要介绍有关<strong>标签</strong>在推荐系统中的应用。</p>
</blockquote>
<a id="more"></a>
<h3 id="思维导图"><a href="#思维导图" class="headerlink" title="思维导图"></a>思维导图</h3><p><img src="/images/user_tag.png" alt="user_tag"></p>
<p> [1]项亮，推荐系统实战.</p>
]]></content>
    
    <summary type="html">
    
      &lt;blockquote class=&quot;blockquote-center&quot;&gt;&lt;h3 id=&quot;简介&quot;&gt;&lt;a href=&quot;#简介&quot; class=&quot;headerlink&quot; title=&quot;简介&quot;&gt;&lt;/a&gt;简介&lt;/h3&gt;&lt;p&gt;推荐系统的目的是联系用户的兴趣和物品，倘若通过一些特征(feature)联系用户和物品，给用户推荐那些具有用户喜欢的特征的物品，标签作为一种无层次化结构的、用来描述信息的关键词。本文主要介绍有关&lt;strong&gt;标签&lt;/strong&gt;在推荐系统中的应用。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="Recommendation" scheme="http://www.phoebepan.cn/categories/Recommendation/"/>
    
    
      <category term="Recommendation" scheme="http://www.phoebepan.cn/tags/Recommendation/"/>
    
  </entry>
  
  <entry>
    <title>推荐系统实战——冷启动</title>
    <link href="http://www.phoebepan.cn/2017/05/18/cold_start/"/>
    <id>http://www.phoebepan.cn/2017/05/18/cold_start/</id>
    <published>2017-05-18T07:30:16.000Z</published>
    <updated>2017-07-19T08:24:24.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote class="blockquote-center"><h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p>前一篇博文的介绍，可见大量用户行为数据是推荐系统的先决条件。那么，如何在没有大量用户数据的情况下设计个性化推荐系统并让用户对推荐结果满意而增加用户粘性，这就是本文所介绍的冷启动问题。</p>
</blockquote>
<a id="more"></a>
<h3 id="思维导图"><a href="#思维导图" class="headerlink" title="思维导图"></a>思维导图</h3><p><img src="/images/coldstart.png" alt="coldstart"></p>
<h3 id="用户注册信息"><a href="#用户注册信息" class="headerlink" title="用户注册信息"></a>用户注册信息</h3><p>基于用户注册信息的推荐算法其核心问题是计算每种特征的用户喜<br>欢的物品，将 p(f,i)定义为喜欢物品i的用户中具有特征f的比例，$$ p(f,i)=\frac{\left | N(i)\bigcap U(f) \right |}{\left | N(i) \right | + \alpha} $$，参数$\alpha$（较大的数）的目的是解决数据稀疏问题。</p>
<h4 id="如何选择合适的物品启动用户的兴趣"><a href="#如何选择合适的物品启动用户的兴趣" class="headerlink" title="如何选择合适的物品启动用户的兴趣"></a>如何选择合适的物品启动用户的兴趣</h4><p>Nadav Golbandi的算法通过建立物品区分度的决策树，来启动用户的兴趣，下图是Nadav Golbandi算法的举例，<br><img src="/images/Nadav_Golbandi.png" alt="Nadav Golbandi"></p>
<h3 id="物品内容信息"><a href="#物品内容信息" class="headerlink" title="物品内容信息"></a>物品内容信息</h3><p>内容相似度计算简单，能频繁更新，而且能够解决物品冷启动问题，那么为什么还需要协同过滤的算法?</p>
<ul>
<li>内容过滤算法忽视了用户行为，从而也忽视了物品的流行度以及用户行为中所包含的规律，所以它的精度比较低，但结果的新颖度却比较高。</li>
<li>如果用户的行为强烈受某一内容属性的影响，那么内容过滤的算法还是可以在精度上超过协同过滤算法的。</li>
</ul>
<p>所以，通常将这两种算法融合，可获得比单独使用这两种算法更好的效果。</p>
<h4 id="话题模型（topic-model）"><a href="#话题模型（topic-model）" class="headerlink" title="话题模型（topic model）"></a>话题模型（topic model）</h4><p>“推荐系统的动态特性”和“基于时间的协同过滤算法研究”，这两篇文章title关键词不同，但关键词所属话题相同。这种情况下，先知道文章的话题分布，然后才能准确地计算文章的相似度。</p>
<h4 id="KL散度"><a href="#KL散度" class="headerlink" title="KL散度"></a>KL散度</h4><p><strong>定义：</strong><br>对于<code>离散空间</code>两个概率分布P和Q，从Q到P的KL散度为<img src="/images/lisan_KL.gif" alt="lisan_KL"><br>对于<code>连续空间</code>的两个概率分布P和Q，从Q到P的KL散度为：<img src="/images/lianxu_KL.gif" alt="lianxu_KL">，p和q是概率分布P和Q的概率密度。<br><strong>简单例子计算：</strong><br>比如有四个类别，一个方法P得到四个类别的概率分别是0.1，0.2，0.3，0.4。另一种方法Q（或者说是事实情况）是得到四个类别的概率分别是0.4，0.3，0.2，0.1,那么这两个分布的 KL 散度就是：<br><img src="/images/KL_eg.gif" alt="KL_eg"><br><strong>实际案例</strong><br>参考附录2<br> [1]项亮，推荐系统实战.<br> <a href="http://chuansong.me/n/2759305" target="_blank" rel="external">[2] KL散度（从动力系统到推荐系统）</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;blockquote class=&quot;blockquote-center&quot;&gt;&lt;h3 id=&quot;简介&quot;&gt;&lt;a href=&quot;#简介&quot; class=&quot;headerlink&quot; title=&quot;简介&quot;&gt;&lt;/a&gt;简介&lt;/h3&gt;&lt;p&gt;前一篇博文的介绍，可见大量用户行为数据是推荐系统的先决条件。那么，如何在没有大量用户数据的情况下设计个性化推荐系统并让用户对推荐结果满意而增加用户粘性，这就是本文所介绍的冷启动问题。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="Recommendation" scheme="http://www.phoebepan.cn/categories/Recommendation/"/>
    
    
      <category term="Recommendation" scheme="http://www.phoebepan.cn/tags/Recommendation/"/>
    
  </entry>
  
  <entry>
    <title>推荐系统实战——用户行为分析</title>
    <link href="http://www.phoebepan.cn/2017/05/17/user_action_analysis/"/>
    <id>http://www.phoebepan.cn/2017/05/17/user_action_analysis/</id>
    <published>2017-05-17T07:30:16.000Z</published>
    <updated>2017-07-19T08:24:59.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote class="blockquote-center"><h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p>古语，听其言，观其行，用户的行为不是随机的，而是蕴含着很多模式，通过算法自动挖掘用户行为数据，从而推测出用户的兴趣和需求，做出个性化推荐。本文是自己对用户行为分析的学习笔记。</p>
</blockquote>
<a id="more"></a>
<h3 id="思维导图"><a href="#思维导图" class="headerlink" title="思维导图"></a>思维导图</h3><p><img src="/images/user_action_analysis.png" alt="user_action_analysis"></p>
<h3 id="基于邻域算法"><a href="#基于邻域算法" class="headerlink" title="基于邻域算法"></a>基于邻域算法</h3><h4 id="基于用户的协同过滤-UserCF"><a href="#基于用户的协同过滤-UserCF" class="headerlink" title="基于用户的协同过滤(UserCF)"></a>基于用户的协同过滤(UserCF)</h4><p>推荐原理：给用户推荐那些和他有共同兴趣爱好的用户喜欢的物品，着重于和用户兴趣相似的小群体的热点，更社会化。<br>主要包括两步：</p>
<ul>
<li>找到和目标用户兴趣相似的用户集合</li>
<li>找到集合中用户喜欢且目标用户没有行为的物品做出推荐</li>
</ul>
<p><img src="/images/user_item_reversort.png" alt="reserve_sort"><br>上面这张图是，用户行为数据导出用户间共现矩阵的过程，基于该共现矩阵，利用余弦相似度（或Jaccard）得到用户间兴趣相似度后，UserCF算法会给用户推荐和他兴趣最相似的K个用户喜欢的物品。用户u对物品i的感兴趣程度：<br><img src="/images/pui.png" alt="pui"><br>,S(u,K)包含和用户u兴趣最接近的K个用户，N(i)是对物品i有过行为的用户集合，w<sub>uv</sub>是用户u,v的兴趣相似度，r<sub>vi</sub>用户v对物品i的兴趣。<br><strong>改进</strong><br>计算用户间兴趣相似度时，增加用户u,v共同兴趣列表中热门物品的惩罚。原理类似TF-IDF。</p>
<h4 id="基于物品的协同过滤-ItemCF"><a href="#基于物品的协同过滤-ItemCF" class="headerlink" title="基于物品的协同过滤(ItemCF)"></a>基于物品的协同过滤(ItemCF)</h4><p>推荐原理：给用户推荐那些和他之前喜欢的物品类似的物品，着重于维系用户历史兴趣，更个性化。<br>主要包括两步：</p>
<ul>
<li>计算物品间相似度</li>
<li>根据物品相似度和用户历史行为做出推荐</li>
</ul>
<p>ItemCF结果可解释性更好。<br><strong>改进</strong></p>
<ul>
<li>计算物品相似度时，增加活跃用户贡献的惩罚；</li>
<li>物品相似度的归一化，可提高推荐的多样性。</li>
</ul>
<h4 id="UserCF和ItemCF比较："><a href="#UserCF和ItemCF比较：" class="headerlink" title="UserCF和ItemCF比较："></a>UserCF和ItemCF比较：</h4><p><img src="/images/UserCF_ItemCF.png" alt="UserCF_ItemCF"></p>
<h3 id="隐语义模型-LFM"><a href="#隐语义模型-LFM" class="headerlink" title="隐语义模型(LFM)"></a>隐语义模型(LFM)</h3><p>LFM和基于邻域的方法的比较：</p>
<ul>
<li>理论基础：LFM较好的理论依据，邻域主要是基于统计方法；</li>
<li>离线计算的空间复杂度：LFM更节省空间；</li>
<li>离线计算的时间复杂度：一般情况，LFM高于CF；</li>
<li>在线实时推荐：传统LFM难以实现实时；</li>
<li>推荐解释：ItemCF好于LFM。</li>
</ul>
<p>注：当数据集非常稀疏时，LFM的性能会明显下降，甚至不如UserCF和ItemCF的性能。</p>
<h3 id="基于图的模型"><a href="#基于图的模型" class="headerlink" title="基于图的模型"></a>基于图的模型</h3><p>图中顶点的相关性高的一对顶点一般具有如下特征：</p>
<ul>
<li>两个顶点之间有很多路径相连；</li>
<li>连接两个顶点之间的路径长度都比较短；</li>
<li>连接两个顶点之间的路径不会经过出度比较大的顶点。</li>
</ul>
<p><strong>随机游走</strong>算法思路：<br>假如要给用户A进行个性化推荐，可以从用户A对应的节点开始在用户物品<strong>二分图</strong>（如下图）上随机游走，游走到任何一个节点时，首先按照概率a决定是否继续游走，若继续，就从当前节点指向的节点中按照均匀分布随机选择一个节点作为下一次游走经过的节点；否则，停止该次游走，从用户A对应节点重新开始游走。多次迭代，直到每个物品节点被访问到的概率收敛到一个数。<br><img src="/images/part2graph.png" alt="part2graph"></p>
<p> [1]项亮，推荐系统实战.</p>
]]></content>
    
    <summary type="html">
    
      &lt;blockquote class=&quot;blockquote-center&quot;&gt;&lt;h3 id=&quot;简介&quot;&gt;&lt;a href=&quot;#简介&quot; class=&quot;headerlink&quot; title=&quot;简介&quot;&gt;&lt;/a&gt;简介&lt;/h3&gt;&lt;p&gt;古语，听其言，观其行，用户的行为不是随机的，而是蕴含着很多模式，通过算法自动挖掘用户行为数据，从而推测出用户的兴趣和需求，做出个性化推荐。本文是自己对用户行为分析的学习笔记。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="Recommendation" scheme="http://www.phoebepan.cn/categories/Recommendation/"/>
    
    
      <category term="Recommendation" scheme="http://www.phoebepan.cn/tags/Recommendation/"/>
    
  </entry>
  
  <entry>
    <title>推荐系统实战——概述</title>
    <link href="http://www.phoebepan.cn/2017/05/16/Recommendation_action1/"/>
    <id>http://www.phoebepan.cn/2017/05/16/Recommendation_action1/</id>
    <published>2017-05-16T07:30:16.000Z</published>
    <updated>2017-07-19T08:24:32.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote class="blockquote-center"><h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p>信息过载的现在，催生出了个性化推荐系统，“千人千面”，分析你的历史兴趣，发现对用户有价值的信息，同时，让信息能够展现在对它感兴趣的用户面前，可谓是双赢的酷事！选择《推荐系统实战》这本书作为自己的入门读物，一步一步深入学习个性化推荐，本文是推荐系统的概述，作为系列开篇之记。</p>
</blockquote>
<a id="more"></a>
<h3 id="思维导图"><a href="#思维导图" class="headerlink" title="思维导图"></a>思维导图</h3><p>下面这张思维导图，可以粗略了解推荐系统是什么，解决什么问题，有什么用（经典三问，是什么？为什么？怎么样？），以及如何评价推荐系统优劣？<br><img src="/images/recommend.png" alt="recommend"></p>
<h3 id="附1：覆盖率——基尼系数计算原理："><a href="#附1：覆盖率——基尼系数计算原理：" class="headerlink" title="附1：覆盖率——基尼系数计算原理："></a>附1：覆盖率——基尼系数计算原理：</h3><p>结合下图，gini系数的形象化解释为（黑色曲线表示最不热门的x%物品的总流行度占系统的比例y%）,$$Gini=\frac{A的面积}{(A+B)的面积} $$<br><img src="/images/gini.png" alt="gini"><br>由此可见，如果系统物品流行度分配很不均匀，那么分子就会很大，从而基尼系数也会很大。</p>
<h3 id="附2：一个推荐算法最终上线，需完成3个实验。"><a href="#附2：一个推荐算法最终上线，需完成3个实验。" class="headerlink" title="附2：一个推荐算法最终上线，需完成3个实验。"></a>附2：一个推荐算法最终上线，需完成3个实验。</h3><ul>
<li>离线实验证明他在很多离线指标上优于现有算法；</li>
<li>通过用户调查确定它的用户满意度不低于现有算法；</li>
<li>在线AB测试确定它在我们关心的指标上优于现有算法。</li>
</ul>
<h3 id="附3：离线实验的优化目标"><a href="#附3：离线实验的优化目标" class="headerlink" title="附3：离线实验的优化目标"></a>附3：离线实验的优化目标</h3><p>用一个数学公式表达，如下：<br>最大化预测准确度<br>使得 覆盖率 &gt; A，多样性 &gt; B，新颖性 &gt; C</p>
<p> [1]项亮，推荐系统实战.</p>
]]></content>
    
    <summary type="html">
    
      &lt;blockquote class=&quot;blockquote-center&quot;&gt;&lt;h3 id=&quot;简介&quot;&gt;&lt;a href=&quot;#简介&quot; class=&quot;headerlink&quot; title=&quot;简介&quot;&gt;&lt;/a&gt;简介&lt;/h3&gt;&lt;p&gt;信息过载的现在，催生出了个性化推荐系统，“千人千面”，分析你的历史兴趣，发现对用户有价值的信息，同时，让信息能够展现在对它感兴趣的用户面前，可谓是双赢的酷事！选择《推荐系统实战》这本书作为自己的入门读物，一步一步深入学习个性化推荐，本文是推荐系统的概述，作为系列开篇之记。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="Recommendation" scheme="http://www.phoebepan.cn/categories/Recommendation/"/>
    
    
      <category term="Recommendation" scheme="http://www.phoebepan.cn/tags/Recommendation/"/>
    
  </entry>
  
</feed>
